{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll first be loading the FC matrices and explore their structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using read_dataset from Datasets/FC/create_dataset.py to read the dataset\n",
    "from Datasets.FC.create_dataset import read_dataset_MDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_MDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[116, 116], edge_index=[2, 1294], edge_attr=[1294, 1], y=[1])\n",
      "['edge_attr', 'y', 'edge_index', 'x']\n",
      "ValuesView({'x': tensor([[ 0.0000,  0.2857,  0.0804,  ...,  0.2032,  0.1674,  0.0906],\n",
      "        [ 0.2857,  0.0000, -0.3860,  ...,  0.1637, -0.0359,  0.1674],\n",
      "        [ 0.0804, -0.3860,  0.0000,  ..., -0.0175, -0.0309,  0.0296],\n",
      "        ...,\n",
      "        [ 0.2032,  0.1637, -0.0175,  ...,  0.0000,  0.2329, -0.1452],\n",
      "        [ 0.1674, -0.0359, -0.0309,  ...,  0.2329,  0.0000,  0.0183],\n",
      "        [ 0.0906,  0.1674,  0.0296,  ..., -0.1452,  0.0183,  0.0000]]), 'edge_index': tensor([[  0,   0,   0,  ..., 113, 113, 114],\n",
      "        [ 10,  12,  14,  ..., 111, 112, 108]]), 'edge_attr': tensor([[0.7785],\n",
      "        [0.6966],\n",
      "        [0.5463],\n",
      "        ...,\n",
      "        [0.6415],\n",
      "        [0.4745],\n",
      "        [0.5401]]), 'y': tensor([0])})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1604"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok, let's explore the data a bit more\n",
    "#dataset is a list object of torch_geometric.data objects\n",
    "\n",
    "#let's see the first element\n",
    "print(dataset[0])\n",
    "\n",
    "#it's a dictionary object, let's see the keys\n",
    "print(dataset[0].keys())\n",
    "\n",
    "#ok, let's see the values\n",
    "print(dataset[0].values())\n",
    "\n",
    "#it has 4 keys, 'x', 'edge_index', 'edge_attr' and 'y' where y=0 menas the patient is healthy and y=1 means the patient has Autism Spectrum Disorder (ASD)\n",
    "\"\"\"graph = Data(x=ROI.reshape(-1,116).float(),\n",
    "                     edge_index=G.indices().reshape(2,-1).long(),\n",
    "                     edge_attr=G.values().reshape(-1,1).float(),\n",
    "                     y=y.long())\"\"\"\n",
    "\n",
    "#how much data do we have?\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (gin_layers): ModuleList(\n",
      "    (0): GINConv(nn=Sequential(\n",
      "      (0): Linear(in_features=116, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): BatchNorm(128)\n",
      "    ))\n",
      "    (1): GINConv(nn=Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): BatchNorm(128)\n",
      "    ))\n",
      "    (2): GINConv(nn=Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): BatchNorm(128)\n",
      "    ))\n",
      "    (3): GINConv(nn=Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): BatchNorm(128)\n",
      "    ))\n",
      "    (4): GINConv(nn=Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): BatchNorm(128)\n",
      "    ))\n",
      "  )\n",
      "  (lin1): Linear(128, 128, bias=True)\n",
      "  (lin2): Linear(128, 2, bias=True)\n",
      "  (bn1): BatchNorm(128)\n",
      ")\n",
      "tensor([1365,  984, 1017,  ...,  260,  678,   83])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#set the seed\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "MODEL = \"GIN\"\n",
    "DATASET = \"FC_MDD\"\n",
    "\n",
    "from models.models_FC import GIN_framework as framework # import the model\n",
    "\n",
    "gnn = framework(dataset)\n",
    "\n",
    "print(gnn.model)\n",
    "print(gnn.train_idx)\n",
    "\n",
    "MODELtri = \"GINtri\"\n",
    "from models.models_FC import GIN_framework_tri as framework # import the model\n",
    "gnntri = framework(dataset)\n",
    "\n",
    "MODEL2 = \"GIN2\"\n",
    "from models.models_FC import GIN_framework2 as framework2 # import the model\n",
    "gnn2 = framework2(dataset)\n",
    "\n",
    "MODEL3 = \"GIN3\"\n",
    "from models.models_FC import GIN_framework3 as framework3 # import the model\n",
    "gnn3 = framework3(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.380, Test Loss: 1.026, Train Acc: 0.827 Test Acc: 0.506\n",
      "Epoch: 040, Loss: 0.028, Test Loss: 1.371, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 060, Loss: 0.008, Test Loss: 1.690, Train Acc: 0.986 Test Acc: 0.654\n",
      "Epoch: 080, Loss: 0.004, Test Loss: 1.835, Train Acc: 0.985 Test Acc: 0.642\n",
      "Epoch: 100, Loss: 0.003, Test Loss: 1.950, Train Acc: 0.986 Test Acc: 0.630\n",
      "Epoch: 120, Loss: 0.002, Test Loss: 2.048, Train Acc: 0.985 Test Acc: 0.617\n",
      "Epoch: 140, Loss: 0.002, Test Loss: 2.131, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 160, Loss: 0.001, Test Loss: 2.202, Train Acc: 0.983 Test Acc: 0.605\n",
      "Epoch: 180, Loss: 0.001, Test Loss: 2.257, Train Acc: 0.983 Test Acc: 0.593\n",
      "Epoch: 200, Loss: 0.001, Test Loss: 2.309, Train Acc: 0.983 Test Acc: 0.593\n",
      "Epoch: 220, Loss: 0.001, Test Loss: 2.358, Train Acc: 0.984 Test Acc: 0.593\n",
      "Epoch: 240, Loss: 0.001, Test Loss: 2.394, Train Acc: 0.984 Test Acc: 0.593\n",
      "Epoch: 260, Loss: 0.001, Test Loss: 2.430, Train Acc: 0.984 Test Acc: 0.593\n",
      "Epoch: 280, Loss: 0.001, Test Loss: 2.460, Train Acc: 0.984 Test Acc: 0.593\n",
      "Epoch: 300, Loss: 0.001, Test Loss: 2.487, Train Acc: 0.984 Test Acc: 0.605\n",
      "Epoch: 320, Loss: 0.000, Test Loss: 2.512, Train Acc: 0.984 Test Acc: 0.605\n",
      "Epoch: 340, Loss: 0.000, Test Loss: 2.533, Train Acc: 0.984 Test Acc: 0.605\n",
      "Epoch: 360, Loss: 0.000, Test Loss: 2.551, Train Acc: 0.984 Test Acc: 0.605\n",
      "Epoch: 380, Loss: 0.000, Test Loss: 2.567, Train Acc: 0.984 Test Acc: 0.605\n",
      "Epoch: 400, Loss: 0.000, Test Loss: 2.583, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 420, Loss: 0.000, Test Loss: 2.597, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 440, Loss: 0.000, Test Loss: 2.611, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 460, Loss: 0.000, Test Loss: 2.622, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 480, Loss: 0.000, Test Loss: 2.632, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 500, Loss: 0.000, Test Loss: 2.640, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 520, Loss: 0.000, Test Loss: 2.650, Train Acc: 0.985 Test Acc: 0.617\n",
      "Epoch: 540, Loss: 0.000, Test Loss: 2.656, Train Acc: 0.985 Test Acc: 0.617\n",
      "Epoch: 560, Loss: 0.000, Test Loss: 2.660, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 580, Loss: 0.000, Test Loss: 2.668, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 600, Loss: 0.000, Test Loss: 2.678, Train Acc: 0.984 Test Acc: 0.605\n",
      "Epoch: 620, Loss: 0.000, Test Loss: 2.687, Train Acc: 0.984 Test Acc: 0.605\n",
      "Epoch: 640, Loss: 0.000, Test Loss: 2.692, Train Acc: 0.984 Test Acc: 0.617\n",
      "Epoch: 660, Loss: 0.074, Test Loss: 1.098, Train Acc: 0.912 Test Acc: 0.605\n",
      "Epoch: 680, Loss: 0.002, Test Loss: 1.492, Train Acc: 0.972 Test Acc: 0.667\n",
      "Epoch: 700, Loss: 0.001, Test Loss: 1.598, Train Acc: 0.969 Test Acc: 0.654\n",
      "Epoch: 720, Loss: 0.001, Test Loss: 1.660, Train Acc: 0.970 Test Acc: 0.654\n",
      "Epoch: 740, Loss: 0.001, Test Loss: 1.708, Train Acc: 0.970 Test Acc: 0.642\n"
     ]
    }
   ],
   "source": [
    "gnn.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.621, Test Loss: 0.788, Train Acc: 0.687 Test Acc: 0.506\n",
      "Epoch: 040, Loss: 0.381, Test Loss: 0.940, Train Acc: 0.891 Test Acc: 0.556\n",
      "Epoch: 060, Loss: 0.186, Test Loss: 1.308, Train Acc: 0.974 Test Acc: 0.543\n",
      "Epoch: 080, Loss: 0.094, Test Loss: 1.283, Train Acc: 0.996 Test Acc: 0.568\n",
      "Epoch: 100, Loss: 0.052, Test Loss: 1.363, Train Acc: 0.999 Test Acc: 0.580\n",
      "Epoch: 120, Loss: 0.038, Test Loss: 1.512, Train Acc: 0.998 Test Acc: 0.593\n",
      "Epoch: 140, Loss: 0.031, Test Loss: 1.496, Train Acc: 0.999 Test Acc: 0.605\n",
      "Epoch: 160, Loss: 0.019, Test Loss: 1.561, Train Acc: 1.000 Test Acc: 0.605\n",
      "Epoch: 180, Loss: 0.016, Test Loss: 1.593, Train Acc: 0.999 Test Acc: 0.605\n",
      "Epoch: 200, Loss: 0.021, Test Loss: 1.630, Train Acc: 0.999 Test Acc: 0.667\n",
      "Epoch: 220, Loss: 0.020, Test Loss: 1.723, Train Acc: 0.999 Test Acc: 0.593\n",
      "Epoch: 240, Loss: 0.013, Test Loss: 1.719, Train Acc: 1.000 Test Acc: 0.593\n",
      "Epoch: 260, Loss: 0.016, Test Loss: 1.746, Train Acc: 0.999 Test Acc: 0.630\n",
      "Epoch: 280, Loss: 0.009, Test Loss: 1.748, Train Acc: 0.999 Test Acc: 0.630\n",
      "Epoch: 300, Loss: 0.008, Test Loss: 1.702, Train Acc: 0.999 Test Acc: 0.667\n",
      "Epoch: 320, Loss: 0.004, Test Loss: 1.603, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 340, Loss: 0.005, Test Loss: 2.010, Train Acc: 0.999 Test Acc: 0.580\n",
      "Epoch: 360, Loss: 0.007, Test Loss: 1.775, Train Acc: 0.999 Test Acc: 0.630\n",
      "Epoch: 380, Loss: 0.004, Test Loss: 2.075, Train Acc: 0.999 Test Acc: 0.593\n",
      "Epoch: 400, Loss: 0.006, Test Loss: 1.884, Train Acc: 0.999 Test Acc: 0.617\n"
     ]
    }
   ],
   "source": [
    "gnntri.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn2.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn3.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Epoch: 50, Loss: 0.120, Train Acc: 0.961, Test Acc: 0.600\n",
      "Fold: 1, Epoch: 100, Loss: 0.008, Train Acc: 1.000, Test Acc: 0.618\n"
     ]
    }
   ],
   "source": [
    "gnn3.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/FC_MDD_GINserver.pt\n"
     ]
    }
   ],
   "source": [
    "gnntri.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/FC_MDD_GINserver.pt\n"
     ]
    }
   ],
   "source": [
    "#save the model \n",
    "gnn.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")\n",
    "\n",
    "# gnn2.save_model(path=\"models/\"+DATASET+\"_\"+MODEL2+\"server.pt\")\n",
    "\n",
    "# gnn3.save_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")\n",
    "\n",
    "gnn2.load_model(path=\"models/\"+DATASET+\"_\"+MODEL2+\"server.pt\")\n",
    "\n",
    "gnn3.load_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server.pt\")\n",
    "#gnn3.save_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server_iterate.pt\")\n",
    "#gnn3.load_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server_iterate.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.502, Train Acc: 0.480 Test Acc: 0.482\n"
     ]
    }
   ],
   "source": [
    "gnn2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn3.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\tomdu\\AppData\\Local\\Temp\\ipykernel_15308\\2147926338.py\", line 2, in <module>\n",
      "    test_loader = DataLoader(dataset[gnn.test_idx], batch_size=1, shuffle=False)\n",
      "TypeError: only integer tensors of a single element can be converted to an index\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1062, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1114, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\pygments\\style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\pygments\\style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(dataset[gnn.test_idx], batch_size=1, shuffle=False)\n",
    "\n",
    "gnn3.evaluate2(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_features[0]))\n",
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Alternative metrics for disconnected graphs\n",
    "        # Option 1: Use the average path length of the largest connected component\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        num_edges = G.number_of_edges()\n",
    "        density = nx.density(G)\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        number_of_node_in_the_largest_fully_connected_component = len(max(nx.connected_components(G), key=len))\n",
    "        #small_world = nx.algorithms.smallworld.sigma(G)\n",
    "\n",
    "        properties.append((num_nodes, num_edges, density, avg_path_len, num_cliques, num_triangles, num_squares, number_of_node_in_the_largest_fully_connected_component)) #, small_world))\n",
    "    return properties\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties = compute_graph_properties(selected_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_properties))\n",
    "train_properties[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties, f)\n",
    "\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"rb\") as f:\n",
    "    train_properties = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"rb\") as f:\n",
    "    test_properties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embeddings of GIN are like this:\n",
    "return F.log_softmax(x7, dim=-1), (x1, x2, x3, x4, x5, x_global, x6, x7, x8)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "output_size = 1  # Predicting one property at a time\n",
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Assume we have already evaluated to get features\n",
    "train_features, test_features = gnn.evaluate_with_features2()\n",
    "\n",
    "# Extract x embeddings\n",
    "train_x = np.array([feat[0] for feat in train_features])\n",
    "test_x = np.array([feat[0] for feat in test_features])\n",
    "\n",
    "# Extract 2, 3, 4, global, 5, 6, 7 embeddings\n",
    "train_x2 = np.array([feat[1] for feat in train_features])\n",
    "test_x2 = np.array([feat[1] for feat in test_features])\n",
    "\n",
    "train_x3 = np.array([feat[2] for feat in train_features])\n",
    "test_x3 = np.array([feat[2] for feat in test_features])\n",
    "\n",
    "train_x4 = np.array([feat[3] for feat in train_features])\n",
    "test_x4 = np.array([feat[3] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[4] for feat in train_features])\n",
    "test_x5 = np.array([feat[4] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[5] for feat in train_features])\n",
    "test_x5 = np.array([feat[5] for feat in test_features])\n",
    "\n",
    "train_x_global = np.array([feat[6] for feat in train_features])\n",
    "test_x_global = np.array([feat[6] for feat in test_features])\n",
    "\n",
    "train_x6 = np.array([feat[7] for feat in train_features])\n",
    "test_x6 = np.array([feat[7] for feat in test_features])\n",
    "\n",
    "train_x7 = np.array([feat[8] for feat in train_features])\n",
    "test_x7 = np.array([feat[8] for feat in test_features])\n",
    "\n",
    "train_x8 = np.array([feat[9] for feat in train_features])\n",
    "test_x8 = np.array([feat[9] for feat in test_features])\n",
    "\n",
    "\n",
    "# Compute graph properties\n",
    "# train_properties = compute_graph_properties(gnn.dataset[gnn.train_idx])\n",
    "# test_properties = compute_graph_properties(gnn.dataset[gnn.test_idx])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_x2 = torch.tensor(train_x2, dtype=torch.float32)\n",
    "train_x3 = torch.tensor(train_x3, dtype=torch.float32)\n",
    "train_x4 = torch.tensor(train_x4, dtype=torch.float32)\n",
    "train_x5 = torch.tensor(train_x5, dtype=torch.float32)\n",
    "train_x_global = torch.tensor(train_x_global, dtype=torch.float32)\n",
    "train_x6 = torch.tensor(train_x6, dtype=torch.float32)\n",
    "train_x7 = torch.tensor(train_x7, dtype=torch.float32)\n",
    "train_x8 = torch.tensor(train_x8, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x2 = torch.tensor(test_x2, dtype=torch.float32)\n",
    "test_x3 = torch.tensor(test_x3, dtype=torch.float32)\n",
    "test_x4 = torch.tensor(test_x4, dtype=torch.float32)\n",
    "test_x_global = torch.tensor(test_x_global, dtype=torch.float32)\n",
    "test_x5 = torch.tensor(test_x5, dtype=torch.float32)\n",
    "test_x6 = torch.tensor(test_x6, dtype=torch.float32)\n",
    "test_x7 = torch.tensor(test_x7, dtype=torch.float32)\n",
    "test_x8 = torch.tensor(test_x8, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_properties, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_properties, dtype=torch.float32)\n",
    "\n",
    "# Train and evaluate a model for each graph property and each embedding\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component']\n",
    "embeddings = [(train_x, test_x), (train_x2, test_x2), (train_x3, test_x3), (train_x4, test_x4), (train_x5, test_x5), (train_x_global, test_x_global), (train_x6, test_x6), (train_x7, test_x7), (train_x8, test_x8)]\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global',  'x6', 'x7', 'x8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary where we will sotre the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    input_size = train_embedding.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 100000  # Adjust this as needed\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding).squeeze()\n",
    "            target = train_y[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Adjust this for more frequent/lower print frequency\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y[:, i].cpu().numpy()\n",
    "            test_target = test_y[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embdedding = embeddings_names[ii]\n",
    "            results[(name_of_embdedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component']\n",
    "embeddings_names = ['x', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'orange']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_name)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R² values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig(\"results/\"+DATASET+\"_\"+MODEL+\"test_R2_plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component']\n",
    "embeddings_names = ['x', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'orange']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_name)][2]\n",
    "        if train_r2 < -0.05:\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "\n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig(\"results/\"+DATASET+\"_\"+MODEL+\"train_R2_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with more properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Use the average path length of the largest connected component for disconnected graphs\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "def betweenness_centralization(G):\n",
    "    n = len(G)\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    max_betweenness = max(betweenness.values())\n",
    "    centralization = sum(max_betweenness - bet for bet in betweenness.values())\n",
    "    if n > 2:\n",
    "        centralization /= (n - 1) * (n - 2) / 2\n",
    "    return centralization\n",
    "\n",
    "def pagerank_centralization(G, alpha=0.85):\n",
    "    n = len(G)\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    max_pagerank = max(pagerank.values())\n",
    "    centralization = sum(max_pagerank - pr for pr in pagerank.values())\n",
    "    if n > 1:\n",
    "        centralization /= (n - 1)\n",
    "    return centralization\n",
    "\n",
    "def clustering_properties(G):\n",
    "    average_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "    return average_clustering, transitivity\n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        \n",
    "        # Number of nodes\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        \n",
    "        # Number of edges\n",
    "        num_edges = G.number_of_edges()\n",
    "        \n",
    "        # Density\n",
    "        density = nx.density(G)\n",
    "        \n",
    "        # Average Path Length\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        \n",
    "        # Diameter\n",
    "        if nx.is_connected(G):\n",
    "            diameter = nx.diameter(G)\n",
    "        else:\n",
    "            # Use the diameter of the largest connected component for disconnected graphs\n",
    "            components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "            largest_component = max(components, key=len)\n",
    "            diameter = nx.diameter(largest_component)\n",
    "        \n",
    "        # Radius\n",
    "        if nx.is_connected(G):\n",
    "            radius = nx.radius(G)\n",
    "        else:\n",
    "            radius = nx.radius(largest_component)\n",
    "        \n",
    "        # Clustering Coefficient\n",
    "        clustering_coeff = nx.average_clustering(G)\n",
    "        \n",
    "        # Transitivity\n",
    "        transitivity = nx.transitivity(G)\n",
    "        \n",
    "        # Assortativity\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "        \n",
    "        # Number of Cliques\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        \n",
    "        # Number of Triangles\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        \n",
    "        # Number of Squares (4-cycles)\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        \n",
    "        # Size of the Largest Connected Component\n",
    "        largest_component_size = len(max(nx.connected_components(G), key=len))\n",
    "        \n",
    "        # Average Degree\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        avg_degree = np.mean(degrees)\n",
    "        \n",
    "        # Betweenness Centrality\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        avg_betweenness_centrality = np.mean(list(betweenness_centrality.values()))\n",
    "        \n",
    "        # Eigenvalues of the Adjacency Matrix (for spectral properties)\n",
    "        eigenvalues = np.linalg.eigvals(nx.adjacency_matrix(G).todense())\n",
    "        spectral_radius = max(eigenvalues)\n",
    "        algebraic_connectivity = sorted(eigenvalues)[1]  # second smallest eigenvalue\n",
    "        \n",
    "        # Graph Laplacian Eigenvalues\n",
    "        laplacian_eigenvalues = np.linalg.eigvals(nx.laplacian_matrix(G).todense())\n",
    "        graph_energy = sum(abs(laplacian_eigenvalues))\n",
    "        \n",
    "        # Small-World-ness\n",
    "        # Compare clustering coefficient and average path length with those of a random graph\n",
    "        random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "        random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "        random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "        small_world_coefficient = (clustering_coeff / random_clustering_coeff) / (avg_path_len / random_avg_path_len)\n",
    "\n",
    "        # Calculate Betweenness Centralization\n",
    "        betweenness_cent = betweenness_centralization(G)\n",
    "        print(f\"Betweenness Centralization: {betweenness_cent}\")\n",
    "\n",
    "        # Calculate PageRank Centralization\n",
    "        pagerank_cent = pagerank_centralization(G)\n",
    "        print(f\"PageRank Centralization: {pagerank_cent}\")\n",
    "\n",
    "        # Calculate Clustering properties\n",
    "        avg_clustering, transitivity = clustering_properties(G)\n",
    "        print(f\"Average Clustering Coefficient: {avg_clustering}\")\n",
    "        print(f\"Transitivity: {transitivity}\")\n",
    "        \n",
    "        properties.append((\n",
    "            num_nodes,\n",
    "            num_edges,\n",
    "            density,\n",
    "            avg_path_len,\n",
    "            diameter,\n",
    "            radius,\n",
    "            clustering_coeff,\n",
    "            transitivity,\n",
    "            assortativity,\n",
    "            num_cliques,\n",
    "            num_triangles,\n",
    "            num_squares,\n",
    "            largest_component_size,\n",
    "            avg_degree,\n",
    "            avg_betweenness_centrality,\n",
    "            spectral_radius,\n",
    "            algebraic_connectivity,\n",
    "            graph_energy,\n",
    "            small_world_coefficient\n",
    "        ))\n",
    "    return properties\n",
    "\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties_long = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties_long = compute_graph_properties(selected_dataset)\n",
    "\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties_long, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties_long, f)\n",
    "\n",
    "#load the properties\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"rb\") as f:\n",
    "#     train_properties_long = pkl.load(f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"rb\") as f:\n",
    "#     test_properties_long = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient']\n",
    "train_y_long = torch.tensor(train_properties_long, dtype=torch.float32)\n",
    "test_y_long = torch.tensor(test_properties_long, dtype=torch.float32)\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    input_size = train_embedding.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names_long):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 800000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding).squeeze()\n",
    "            target = train_y_long[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y_long[:, i].cpu().numpy()\n",
    "            test_target = test_y_long[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors_long = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_names_long)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R² values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'test_R2_plot_long.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_names_long)][2]\n",
    "        if train_r2 < -0.05:  # Handle negative R² values\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "\n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'train_R2_plot_long.png')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
