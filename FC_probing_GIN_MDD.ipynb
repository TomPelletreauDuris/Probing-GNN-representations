{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing GIN MDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll first be loading the FC matrices and explore their structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using read_dataset from Datasets/FC/create_dataset.py to read the dataset\n",
    "from Datasets.FC.create_dataset import read_dataset_MDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_MDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[116, 116], edge_index=[2, 1294], edge_attr=[1294, 1], y=[1])\n",
      "['edge_index', 'x', 'edge_attr', 'y']\n",
      "ValuesView({'x': tensor([[ 0.0000,  0.2857,  0.0804,  ...,  0.2032,  0.1674,  0.0906],\n",
      "        [ 0.2857,  0.0000, -0.3860,  ...,  0.1637, -0.0359,  0.1674],\n",
      "        [ 0.0804, -0.3860,  0.0000,  ..., -0.0175, -0.0309,  0.0296],\n",
      "        ...,\n",
      "        [ 0.2032,  0.1637, -0.0175,  ...,  0.0000,  0.2329, -0.1452],\n",
      "        [ 0.1674, -0.0359, -0.0309,  ...,  0.2329,  0.0000,  0.0183],\n",
      "        [ 0.0906,  0.1674,  0.0296,  ..., -0.1452,  0.0183,  0.0000]]), 'edge_index': tensor([[  0,   0,   0,  ..., 113, 113, 114],\n",
      "        [ 10,  12,  14,  ..., 111, 112, 108]]), 'edge_attr': tensor([[0.7785],\n",
      "        [0.6966],\n",
      "        [0.5463],\n",
      "        ...,\n",
      "        [0.6415],\n",
      "        [0.4745],\n",
      "        [0.5401]]), 'y': tensor([0])})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1604"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok, let's explore the data a bit more\n",
    "#dataset is a list object of torch_geometric.data objects\n",
    "\n",
    "#let's see the first element\n",
    "print(dataset[0])\n",
    "\n",
    "#it's a dictionary object, let's see the keys\n",
    "print(dataset[0].keys())\n",
    "\n",
    "#ok, let's see the values\n",
    "print(dataset[0].values())\n",
    "\n",
    "#it has 4 keys, 'x', 'edge_index', 'edge_attr' and 'y' where y=0 menas the patient is healthy and y=1 means the patient has Autism Spectrum Disorder (ASD)\n",
    "\"\"\"graph = Data(x=ROI.reshape(-1,116).float(),\n",
    "                     edge_index=G.indices().reshape(2,-1).long(),\n",
    "                     edge_attr=G.values().reshape(-1,1).float(),\n",
    "                     y=y.long())\"\"\"\n",
    "\n",
    "#how much data do we have?\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#set the seed\n",
    "import torch\n",
    "torch.manual_seed(37)\n",
    "\n",
    "DATASET = \"FC_MDD\"\n",
    "\n",
    "MODEL = \"GIN\"\n",
    "from models.models_FC import GIN_framework as framework # import the model\n",
    "gnn = framework(dataset)\n",
    "\n",
    "MODELbis = \"GINbis\"\n",
    "from models.models_FC import GIN_framework_bis as framework # import the model\n",
    "gnnbis = framework(dataset)\n",
    "\n",
    "MODELtri = \"GINtri\"\n",
    "from models.models_FC import GIN_framework_tri as framework # import the model\n",
    "gnntri = framework(dataset)\n",
    "\n",
    "MODEL2 = \"GIN2\"\n",
    "from models.models_FC import GIN_framework2 as framework2 # import the model\n",
    "gnn2 = framework2(dataset)\n",
    "\n",
    "MODEL3 = \"GIN3\"\n",
    "from models.models_FC import GIN_framework3 as framework3 # import the model\n",
    "gnn3 = framework3(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnnbis.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnntri.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn2.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn3.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn3.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnntri.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "# gnn.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")\n",
    "\n",
    "# gnn2.save_model(path=\"models/\"+DATASET+\"_\"+MODEL2+\"server.pt\")\n",
    "\n",
    "# gnn3.save_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\", map_location='cpu')\n",
    "\n",
    "# gnn2.load_model(path=\"models/\"+DATASET+\"_\"+MODEL2+\"server.pt\")\n",
    "\n",
    "# gnn3.load_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server.pt\")\n",
    "\n",
    "# gnntri.load_model(path=\"models/\"+DATASET+\"_\"+MODELtri+\"server.pt\")\n",
    "#gnn3.save_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server_iterate.pt\")\n",
    "#gnn3.load_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server_iterate.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.728, Train Acc: 0.970 Test Acc: 0.642\n"
     ]
    }
   ],
   "source": [
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnntri.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn3.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import DataLoader\n",
    "# test_loader = DataLoader(dataset[gnn.test_idx], batch_size=1, shuffle=False)\n",
    "\n",
    "# gnn3.evaluate2(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Alternative metrics for disconnected graphs\n",
    "        # Option 1: Use the average path length of the largest connected component\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "def calculate_small_world(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.algorithms.smallworld.sigma(G)\n",
    "    else:\n",
    "        # Alternative metrics for disconnected graphs\n",
    "        # Option 1: Use the small world coefficient of the largest connected component\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.algorithms.smallworld.sigma(largest_component)\n",
    "    \n",
    "def compute_swi(graph):\n",
    "    # Calculate clustering coefficient and average path length for the given graph\n",
    "    clustering_coeff = nx.average_clustering(graph)\n",
    "    avg_path_len = calculate_avg_path_length(graph)\n",
    "    \n",
    "    # Generate a random graph with the same number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    \n",
    "    # Generate a lattice graph with the same number of nodes and edges\n",
    "    lattice_graph = nx.watts_strogatz_graph(num_nodes, k=4, p=0)  # Adjust k as needed\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the random graph\n",
    "    random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "    random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the lattice graph\n",
    "    lattice_clustering_coeff = nx.average_clustering(lattice_graph)\n",
    "    lattice_avg_path_len = calculate_avg_path_length(lattice_graph)\n",
    "    \n",
    "    # Compute the Small-World Index (SWI)\n",
    "    swi = ((avg_path_len - lattice_avg_path_len) / (random_avg_path_len - lattice_avg_path_len)) * \\\n",
    "          ((clustering_coeff - random_clustering_coeff) / (lattice_clustering_coeff - random_clustering_coeff))\n",
    "    \n",
    "    return swi\n",
    "    \n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        num_edges = G.number_of_edges()\n",
    "        density = nx.density(G)\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        number_of_node_in_the_largest_fully_connected_component = len(max(nx.connected_components(G), key=len))\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "        small_world = compute_swi(G)\n",
    "            \n",
    "        properties.append((num_nodes, num_edges, density, avg_path_len, num_cliques, num_triangles, num_squares, number_of_node_in_the_largest_fully_connected_component, assortativity, small_world))\n",
    "    return properties\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties = compute_graph_properties(selected_dataset)\n",
    "\n",
    "# Save the properties to files\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_with_sm.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_with_sm.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_properties))\n",
    "# train_properties[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#save the properties in a file\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(train_properties, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(test_properties, f)\n",
    "\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_with_sm.pkl\", \"rb\") as f:\n",
    "    train_properties = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_with_sm.pkl\", \"rb\") as f:\n",
    "    test_properties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1523, 81)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_features[0]))\n",
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embeddings of GIN are like this:\n",
    "return F.log_softmax(x7, dim=-1), (x1, x2, x3, x4, x5, x_global, x6, x7, x8)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "output_size = 1  # Predicting one property at a time\n",
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Assume we have already evaluated to get features\n",
    "# train_features, test_features = gnn.evaluate_with_features2()\n",
    "\n",
    "# Extract x embeddings\n",
    "train_x = np.array([feat[0] for feat in train_features])\n",
    "test_x = np.array([feat[0] for feat in test_features])\n",
    "\n",
    "# Extract 2, 3, 4, global, 5, 6, 7 embeddings\n",
    "train_x2 = np.array([feat[1] for feat in train_features])\n",
    "test_x2 = np.array([feat[1] for feat in test_features])\n",
    "\n",
    "train_x3 = np.array([feat[2] for feat in train_features])\n",
    "test_x3 = np.array([feat[2] for feat in test_features])\n",
    "\n",
    "train_x4 = np.array([feat[3] for feat in train_features])\n",
    "test_x4 = np.array([feat[3] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[4] for feat in train_features])\n",
    "test_x5 = np.array([feat[4] for feat in test_features])\n",
    "\n",
    "train_x_global = np.array([feat[5] for feat in train_features])\n",
    "test_x_global = np.array([feat[5] for feat in test_features])\n",
    "\n",
    "train_x6 = np.array([feat[6] for feat in train_features])\n",
    "test_x6 = np.array([feat[6] for feat in test_features])\n",
    "\n",
    "train_x7 = np.array([feat[7] for feat in train_features])\n",
    "test_x7 = np.array([feat[7] for feat in test_features])\n",
    "\n",
    "train_x8 = np.array([feat[8] for feat in train_features])\n",
    "test_x8 = np.array([feat[8] for feat in test_features])\n",
    "\n",
    "\n",
    "# Compute graph properties\n",
    "# train_properties = compute_graph_properties(gnn.dataset[gnn.train_idx])\n",
    "# test_properties = compute_graph_properties(gnn.dataset[gnn.test_idx])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_x2 = torch.tensor(train_x2, dtype=torch.float32)\n",
    "train_x3 = torch.tensor(train_x3, dtype=torch.float32)\n",
    "train_x4 = torch.tensor(train_x4, dtype=torch.float32)\n",
    "train_x5 = torch.tensor(train_x5, dtype=torch.float32)\n",
    "train_x_global = torch.tensor(train_x_global, dtype=torch.float32)\n",
    "train_x6 = torch.tensor(train_x6, dtype=torch.float32)\n",
    "train_x7 = torch.tensor(train_x7, dtype=torch.float32)\n",
    "train_x8 = torch.tensor(train_x8, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x2 = torch.tensor(test_x2, dtype=torch.float32)\n",
    "test_x3 = torch.tensor(test_x3, dtype=torch.float32)\n",
    "test_x4 = torch.tensor(test_x4, dtype=torch.float32)\n",
    "test_x_global = torch.tensor(test_x_global, dtype=torch.float32)\n",
    "test_x5 = torch.tensor(test_x5, dtype=torch.float32)\n",
    "test_x6 = torch.tensor(test_x6, dtype=torch.float32)\n",
    "test_x7 = torch.tensor(test_x7, dtype=torch.float32)\n",
    "test_x8 = torch.tensor(test_x8, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_properties, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_properties, dtype=torch.float32)\n",
    "\n",
    "# Train and evaluate a model for each graph property and each embedding\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings = [(train_x, test_x), (train_x2, test_x2), (train_x3, test_x3), (train_x4, test_x4), (train_x5, test_x5), (train_x_global, test_x_global), (train_x6, test_x6), (train_x7, test_x7), (train_x8, test_x8)]\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global',  'x6', 'x7', 'x8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1523, 14848])\n",
      "torch.Size([81, 14848])\n",
      "Epoch [1000/2000000], Property: num_nodes, Loss: 13.1149\n",
      "Epoch [2000/2000000], Property: num_nodes, Loss: 4.6198\n",
      "Epoch [3000/2000000], Property: num_nodes, Loss: 1.7315\n",
      "Epoch [4000/2000000], Property: num_nodes, Loss: 0.5770\n",
      "Epoch [5000/2000000], Property: num_nodes, Loss: 0.1578\n",
      "Epoch [6000/2000000], Property: num_nodes, Loss: 0.0415\n",
      "Epoch [7000/2000000], Property: num_nodes, Loss: 0.0177\n",
      "Epoch [8000/2000000], Property: num_nodes, Loss: 0.0109\n",
      "Epoch [9000/2000000], Property: num_nodes, Loss: 0.0065\n",
      "Epoch [10000/2000000], Property: num_nodes, Loss: 0.3116\n",
      "Epoch [11000/2000000], Property: num_nodes, Loss: 0.0028\n",
      "Epoch [12000/2000000], Property: num_nodes, Loss: 0.0018\n",
      "Epoch [13000/2000000], Property: num_nodes, Loss: 0.0012\n",
      "Epoch [14000/2000000], Property: num_nodes, Loss: 0.0010\n",
      "Epoch [15000/2000000], Property: num_nodes, Loss: 0.0006\n",
      "Epoch [16000/2000000], Property: num_nodes, Loss: 0.0005\n",
      "Epoch [17000/2000000], Property: num_nodes, Loss: 0.0010\n",
      "Epoch [18000/2000000], Property: num_nodes, Loss: 0.0003\n",
      "Epoch [19000/2000000], Property: num_nodes, Loss: 0.0002\n",
      "Epoch [20000/2000000], Property: num_nodes, Loss: 0.0001\n",
      "Epoch [21000/2000000], Property: num_nodes, Loss: 0.0006\n",
      "Epoch [22000/2000000], Property: num_nodes, Loss: 0.0003\n",
      "Epoch [23000/2000000], Property: num_nodes, Loss: 0.0001\n",
      "Epoch [24000/2000000], Property: num_nodes, Loss: 0.0001\n",
      "Epoch [25000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [26000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [27000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [28000/2000000], Property: num_nodes, Loss: 0.0106\n",
      "Epoch [29000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [30000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [31000/2000000], Property: num_nodes, Loss: 0.0035\n",
      "Epoch [32000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [33000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [34000/2000000], Property: num_nodes, Loss: 0.0088\n",
      "Epoch [35000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [36000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [37000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [38000/2000000], Property: num_nodes, Loss: 0.3814\n",
      "Epoch [39000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [40000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Epoch [41000/2000000], Property: num_nodes, Loss: 0.0001\n",
      "Epoch [42000/2000000], Property: num_nodes, Loss: 0.0000\n",
      "Early stopping at epoch 42648\n",
      "Embedding: torch.Size([1523, 116, 128])\n",
      "Property: num_nodes\n",
      "  Train MSE: 0.0000, Test MSE: 70.6397\n",
      "  Train R²: 1.0000, Test R²: -10.1997\n",
      "Epoch [1000/2000000], Property: num_edges, Loss: 1298.6735\n",
      "Epoch [2000/2000000], Property: num_edges, Loss: 742.9111\n",
      "Epoch [3000/2000000], Property: num_edges, Loss: 431.9999\n",
      "Epoch [4000/2000000], Property: num_edges, Loss: 253.1178\n",
      "Epoch [5000/2000000], Property: num_edges, Loss: 145.6349\n",
      "Epoch [6000/2000000], Property: num_edges, Loss: 79.1147\n",
      "Epoch [7000/2000000], Property: num_edges, Loss: 38.6277\n",
      "Epoch [8000/2000000], Property: num_edges, Loss: 15.8482\n",
      "Epoch [9000/2000000], Property: num_edges, Loss: 5.0467\n",
      "Epoch [10000/2000000], Property: num_edges, Loss: 1.2408\n",
      "Epoch [11000/2000000], Property: num_edges, Loss: 0.3215\n",
      "Epoch [12000/2000000], Property: num_edges, Loss: 0.1413\n",
      "Epoch [13000/2000000], Property: num_edges, Loss: 0.0786\n",
      "Epoch [14000/2000000], Property: num_edges, Loss: 0.0428\n",
      "Epoch [15000/2000000], Property: num_edges, Loss: 0.0253\n",
      "Epoch [16000/2000000], Property: num_edges, Loss: 0.0155\n",
      "Epoch [17000/2000000], Property: num_edges, Loss: 0.7475\n",
      "Epoch [18000/2000000], Property: num_edges, Loss: 0.0063\n",
      "Epoch [19000/2000000], Property: num_edges, Loss: 0.0041\n",
      "Epoch [20000/2000000], Property: num_edges, Loss: 0.0027\n",
      "Epoch [21000/2000000], Property: num_edges, Loss: 0.0018\n",
      "Epoch [22000/2000000], Property: num_edges, Loss: 0.0013\n",
      "Epoch [23000/2000000], Property: num_edges, Loss: 0.1836\n",
      "Epoch [24000/2000000], Property: num_edges, Loss: 0.0006\n",
      "Epoch [25000/2000000], Property: num_edges, Loss: 0.0005\n",
      "Epoch [26000/2000000], Property: num_edges, Loss: 0.0003\n",
      "Epoch [27000/2000000], Property: num_edges, Loss: 0.0002\n",
      "Epoch [28000/2000000], Property: num_edges, Loss: 0.0002\n",
      "Epoch [29000/2000000], Property: num_edges, Loss: 0.0003\n",
      "Epoch [30000/2000000], Property: num_edges, Loss: 0.0001\n",
      "Epoch [31000/2000000], Property: num_edges, Loss: 0.0005\n",
      "Epoch [32000/2000000], Property: num_edges, Loss: 0.0068\n",
      "Epoch [33000/2000000], Property: num_edges, Loss: 0.0001\n",
      "Epoch [34000/2000000], Property: num_edges, Loss: 0.0007\n",
      "Epoch [35000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [36000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [37000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [38000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [39000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [40000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [41000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [42000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [43000/2000000], Property: num_edges, Loss: 0.0085\n",
      "Epoch [44000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [45000/2000000], Property: num_edges, Loss: 0.1088\n",
      "Epoch [46000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [47000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [48000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Epoch [49000/2000000], Property: num_edges, Loss: 0.2592\n",
      "Epoch [50000/2000000], Property: num_edges, Loss: 0.0076\n",
      "Epoch [51000/2000000], Property: num_edges, Loss: 0.0004\n",
      "Epoch [52000/2000000], Property: num_edges, Loss: 0.0000\n",
      "Early stopping at epoch 52092\n",
      "Embedding: torch.Size([1523, 116, 128])\n",
      "Property: num_edges\n",
      "  Train MSE: 0.0275, Test MSE: 2930.9871\n",
      "  Train R²: 1.0000, Test R²: 0.8514\n",
      "Epoch [1000/2000000], Property: density, Loss: 0.0001\n",
      "Epoch [2000/2000000], Property: density, Loss: 0.0001\n",
      "Epoch [3000/2000000], Property: density, Loss: 0.0000\n",
      "Epoch [4000/2000000], Property: density, Loss: 0.0000\n",
      "Epoch [5000/2000000], Property: density, Loss: 0.0000\n",
      "Epoch [6000/2000000], Property: density, Loss: 0.0000\n",
      "Epoch [7000/2000000], Property: density, Loss: 0.0009\n",
      "Early stopping at epoch 7932\n",
      "Embedding: torch.Size([1523, 116, 128])\n",
      "Property: density\n",
      "  Train MSE: 0.0000, Test MSE: 0.0265\n",
      "  Train R²: 0.9994, Test R²: -64.0947\n",
      "Epoch [1000/2000000], Property: avg_path_len, Loss: 0.0004\n",
      "Epoch [2000/2000000], Property: avg_path_len, Loss: 0.0001\n",
      "Epoch [3000/2000000], Property: avg_path_len, Loss: 0.0001\n",
      "Epoch [4000/2000000], Property: avg_path_len, Loss: 0.0063\n",
      "Epoch [5000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [6000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [7000/2000000], Property: avg_path_len, Loss: 0.0013\n",
      "Epoch [8000/2000000], Property: avg_path_len, Loss: 0.0003\n",
      "Epoch [9000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [10000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [11000/2000000], Property: avg_path_len, Loss: 0.0048\n",
      "Epoch [12000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [13000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [14000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [15000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [16000/2000000], Property: avg_path_len, Loss: 0.4456\n",
      "Epoch [17000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Epoch [18000/2000000], Property: avg_path_len, Loss: 0.0000\n",
      "Early stopping at epoch 18704\n",
      "Embedding: torch.Size([1523, 116, 128])\n",
      "Property: avg_path_len\n",
      "  Train MSE: 0.0000, Test MSE: 0.1083\n",
      "  Train R²: 1.0000, Test R²: 0.2850\n",
      "Epoch [1000/2000000], Property: num_cliques, Loss: 186.2971\n",
      "Epoch [2000/2000000], Property: num_cliques, Loss: 87.7669\n",
      "Epoch [3000/2000000], Property: num_cliques, Loss: 42.0641\n",
      "Epoch [4000/2000000], Property: num_cliques, Loss: 18.4817\n",
      "Epoch [5000/2000000], Property: num_cliques, Loss: 7.0099\n",
      "Epoch [6000/2000000], Property: num_cliques, Loss: 2.3744\n",
      "Epoch [7000/2000000], Property: num_cliques, Loss: 0.8784\n",
      "Epoch [8000/2000000], Property: num_cliques, Loss: 0.4083\n",
      "Epoch [9000/2000000], Property: num_cliques, Loss: 0.2051\n",
      "Epoch [10000/2000000], Property: num_cliques, Loss: 0.0997\n",
      "Epoch [11000/2000000], Property: num_cliques, Loss: 0.0538\n",
      "Epoch [12000/2000000], Property: num_cliques, Loss: 0.0320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13000/2000000], Property: num_cliques, Loss: 0.0191\n",
      "Epoch [14000/2000000], Property: num_cliques, Loss: 0.0119\n",
      "Epoch [15000/2000000], Property: num_cliques, Loss: 0.0076\n",
      "Epoch [16000/2000000], Property: num_cliques, Loss: 0.0048\n",
      "Epoch [17000/2000000], Property: num_cliques, Loss: 0.0327\n",
      "Epoch [18000/2000000], Property: num_cliques, Loss: 0.0020\n",
      "Epoch [19000/2000000], Property: num_cliques, Loss: 0.0018\n",
      "Epoch [20000/2000000], Property: num_cliques, Loss: 0.0009\n",
      "Epoch [21000/2000000], Property: num_cliques, Loss: 0.0006\n",
      "Epoch [22000/2000000], Property: num_cliques, Loss: 0.0004\n",
      "Epoch [23000/2000000], Property: num_cliques, Loss: 0.0004\n",
      "Epoch [24000/2000000], Property: num_cliques, Loss: 0.0002\n",
      "Epoch [25000/2000000], Property: num_cliques, Loss: 0.0008\n",
      "Epoch [26000/2000000], Property: num_cliques, Loss: 0.0001\n",
      "Epoch [27000/2000000], Property: num_cliques, Loss: 0.0002\n",
      "Epoch [28000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Epoch [29000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Epoch [30000/2000000], Property: num_cliques, Loss: 0.0063\n",
      "Epoch [31000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Epoch [32000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Epoch [33000/2000000], Property: num_cliques, Loss: 0.0322\n",
      "Epoch [34000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Epoch [35000/2000000], Property: num_cliques, Loss: 0.3162\n",
      "Epoch [36000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Epoch [37000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Epoch [38000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Epoch [39000/2000000], Property: num_cliques, Loss: 0.0000\n",
      "Early stopping at epoch 39254\n",
      "Embedding: torch.Size([1523, 116, 128])\n",
      "Property: num_cliques\n",
      "  Train MSE: 0.0000, Test MSE: 857.6617\n",
      "  Train R²: 1.0000, Test R²: 0.0916\n",
      "Epoch [1000/2000000], Property: num_triangles, Loss: 38491.6445\n",
      "Epoch [2000/2000000], Property: num_triangles, Loss: 14108.1748\n",
      "Epoch [3000/2000000], Property: num_triangles, Loss: 8894.1953\n",
      "Epoch [4000/2000000], Property: num_triangles, Loss: 6241.1963\n",
      "Epoch [5000/2000000], Property: num_triangles, Loss: 4305.1387\n",
      "Epoch [6000/2000000], Property: num_triangles, Loss: 2861.9697\n",
      "Epoch [7000/2000000], Property: num_triangles, Loss: 1799.8684\n",
      "Epoch [8000/2000000], Property: num_triangles, Loss: 1049.0576\n",
      "Epoch [9000/2000000], Property: num_triangles, Loss: 552.3955\n",
      "Epoch [10000/2000000], Property: num_triangles, Loss: 252.9244\n",
      "Epoch [11000/2000000], Property: num_triangles, Loss: 95.8318\n",
      "Epoch [12000/2000000], Property: num_triangles, Loss: 28.9478\n",
      "Epoch [13000/2000000], Property: num_triangles, Loss: 7.9853\n",
      "Epoch [14000/2000000], Property: num_triangles, Loss: 3.0730\n",
      "Epoch [15000/2000000], Property: num_triangles, Loss: 1.5522\n",
      "Epoch [16000/2000000], Property: num_triangles, Loss: 0.7160\n",
      "Epoch [17000/2000000], Property: num_triangles, Loss: 0.4233\n",
      "Epoch [18000/2000000], Property: num_triangles, Loss: 0.1723\n",
      "Epoch [19000/2000000], Property: num_triangles, Loss: 0.0948\n",
      "Epoch [20000/2000000], Property: num_triangles, Loss: 0.0554\n",
      "Epoch [21000/2000000], Property: num_triangles, Loss: 0.0344\n",
      "Epoch [22000/2000000], Property: num_triangles, Loss: 0.0224\n",
      "Epoch [23000/2000000], Property: num_triangles, Loss: 0.0152\n",
      "Epoch [24000/2000000], Property: num_triangles, Loss: 0.0106\n",
      "Epoch [25000/2000000], Property: num_triangles, Loss: 0.0082\n",
      "Epoch [26000/2000000], Property: num_triangles, Loss: 0.0057\n",
      "Epoch [27000/2000000], Property: num_triangles, Loss: 0.0043\n",
      "Epoch [28000/2000000], Property: num_triangles, Loss: 0.0305\n",
      "Epoch [29000/2000000], Property: num_triangles, Loss: 0.0044\n",
      "Epoch [30000/2000000], Property: num_triangles, Loss: 0.5829\n",
      "Epoch [31000/2000000], Property: num_triangles, Loss: 0.0021\n",
      "Epoch [32000/2000000], Property: num_triangles, Loss: 0.0012\n",
      "Epoch [33000/2000000], Property: num_triangles, Loss: 0.0010\n",
      "Epoch [34000/2000000], Property: num_triangles, Loss: 0.0008\n",
      "Epoch [35000/2000000], Property: num_triangles, Loss: 0.4086\n",
      "Epoch [36000/2000000], Property: num_triangles, Loss: 0.0005\n",
      "Epoch [37000/2000000], Property: num_triangles, Loss: 0.4428\n",
      "Epoch [38000/2000000], Property: num_triangles, Loss: 0.0006\n",
      "Epoch [39000/2000000], Property: num_triangles, Loss: 0.0003\n",
      "Epoch [40000/2000000], Property: num_triangles, Loss: 0.0002\n",
      "Epoch [41000/2000000], Property: num_triangles, Loss: 0.0008\n",
      "Epoch [42000/2000000], Property: num_triangles, Loss: 0.0002\n",
      "Epoch [43000/2000000], Property: num_triangles, Loss: 0.0010\n",
      "Epoch [44000/2000000], Property: num_triangles, Loss: 0.0001\n",
      "Epoch [45000/2000000], Property: num_triangles, Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    # Flatten the embeddings before determining the input size\n",
    "    train_embedding_flat = train_embedding.view(train_embedding.size(0), -1)\n",
    "    test_embedding_flat = test_embedding.view(test_embedding.size(0), -1)\n",
    "    #print the shapes\n",
    "    print(train_embedding_flat.shape)\n",
    "    print(test_embedding_flat.shape)\n",
    "    input_size = train_embedding_flat.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 2000000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding_flat).squeeze()\n",
    "            target = train_y[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding_flat).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding_flat).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y[:, i].cpu().numpy()\n",
    "            test_target = test_y[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_full_embedding.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\tomdu\\AppData\\Local\\Temp\\ipykernel_8044\\1929962153.py\", line 2, in <module>\n",
      "    with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_full_embedding.pkl\", \"rb\") as f:\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 284, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'results/FC_MDD_GIN_results_full_embedding.pkl'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1062, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1114, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\pygments\\style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\pygments\\style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "#load results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_full_embedding.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'grey', 'orange', 'purple']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_name)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R² values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig(\"results/\"+DATASET+\"_\"+MODEL+\"test_R2_plot_full_embedding.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'grey', 'orange', 'purple']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_name)][2]\n",
    "        if train_r2 < -0.05:\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "\n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig(\"results/\"+DATASET+\"_\"+MODEL+\"train_R2_plot_full_embedding.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with more properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Use the average path length of the largest connected component for disconnected graphs\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "   \n",
    "def compute_swi(graph):\n",
    "    # Calculate clustering coefficient and average path length for the given graph\n",
    "    clustering_coeff = nx.average_clustering(graph)\n",
    "    avg_path_len = calculate_avg_path_length(graph)\n",
    "    \n",
    "    # Generate a random graph with the same number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    \n",
    "    # Generate a lattice graph with the same number of nodes and edges\n",
    "    lattice_graph = nx.watts_strogatz_graph(num_nodes, k=4, p=0)  # Adjust k as needed\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the random graph\n",
    "    random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "    random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the lattice graph\n",
    "    lattice_clustering_coeff = nx.average_clustering(lattice_graph)\n",
    "    lattice_avg_path_len = calculate_avg_path_length(lattice_graph)\n",
    "    \n",
    "    # Compute the Small-World Index (SWI)\n",
    "    swi = ((avg_path_len - lattice_avg_path_len) / (random_avg_path_len - lattice_avg_path_len)) * \\\n",
    "          ((clustering_coeff - random_clustering_coeff) / (lattice_clustering_coeff - random_clustering_coeff))\n",
    "    \n",
    "    return swi\n",
    "\n",
    "\n",
    "def betweenness_centralization(G):\n",
    "    n = len(G)\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    max_betweenness = max(betweenness.values())\n",
    "    centralization = sum(max_betweenness - bet for bet in betweenness.values())\n",
    "    if n > 2:\n",
    "        centralization /= (n - 1) * (n - 2) / 2\n",
    "    return centralization\n",
    "\n",
    "def pagerank_centralization(G, alpha=0.85):\n",
    "    n = len(G)\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    max_pagerank = max(pagerank.values())\n",
    "    centralization = sum(max_pagerank - pr for pr in pagerank.values())\n",
    "    if n > 1:\n",
    "        centralization /= (n - 1)\n",
    "    return centralization\n",
    "\n",
    "def clustering_properties(G):\n",
    "    average_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "    return average_clustering, transitivity\n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        \n",
    "        # Number of nodes\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        \n",
    "        # Number of edges\n",
    "        num_edges = G.number_of_edges()\n",
    "        \n",
    "        # Density\n",
    "        density = nx.density(G)\n",
    "        \n",
    "        # Average Path Length\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        \n",
    "        # Diameter\n",
    "        if nx.is_connected(G):\n",
    "            diameter = nx.diameter(G)\n",
    "        else:\n",
    "            # Use the diameter of the largest connected component for disconnected graphs\n",
    "            components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "            largest_component = max(components, key=len)\n",
    "            diameter = nx.diameter(largest_component)\n",
    "        \n",
    "        # Radius\n",
    "        if nx.is_connected(G):\n",
    "            radius = nx.radius(G)\n",
    "        else:\n",
    "            radius = nx.radius(largest_component)\n",
    "        \n",
    "        # Clustering Coefficient\n",
    "        clustering_coeff = nx.average_clustering(G)\n",
    "        \n",
    "        # Transitivity\n",
    "        transitivity = nx.transitivity(G)\n",
    "        \n",
    "        # Assortativity\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "        \n",
    "        # Number of Cliques\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        \n",
    "        # Number of Triangles\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        \n",
    "        # Number of Squares (4-cycles)\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        \n",
    "        # Size of the Largest Connected Component\n",
    "        largest_component_size = len(max(nx.connected_components(G), key=len))\n",
    "        \n",
    "        # Average Degree\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        avg_degree = np.mean(degrees)\n",
    "        \n",
    "        # Betweenness Centrality\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        avg_betweenness_centrality = np.mean(list(betweenness_centrality.values()))\n",
    "        \n",
    "        # Eigenvalues of the Adjacency Matrix (for spectral properties)\n",
    "        eigenvalues = np.linalg.eigvals(nx.adjacency_matrix(G).todense())\n",
    "        spectral_radius = max(eigenvalues)\n",
    "        algebraic_connectivity = sorted(eigenvalues)[1]  # second smallest eigenvalue\n",
    "        \n",
    "        # Graph Laplacian Eigenvalues\n",
    "        laplacian_eigenvalues = np.linalg.eigvals(nx.laplacian_matrix(G).todense())\n",
    "        graph_energy = sum(abs(laplacian_eigenvalues))\n",
    "        \n",
    "        # Small-World-ness\n",
    "        # Compare clustering coefficient and average path length with those of a random graph\n",
    "        random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "        random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "        random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "        small_world_coefficient = (clustering_coeff / random_clustering_coeff) / (avg_path_len / random_avg_path_len)\n",
    "\n",
    "        small_world_index = compute_swi(G)\n",
    "\n",
    "        # Calculate Betweenness Centralization\n",
    "        betweenness_cent = betweenness_centralization(G)\n",
    "        print(f\"Betweenness Centralization: {betweenness_cent}\")\n",
    "\n",
    "        # Calculate PageRank Centralization\n",
    "        pagerank_cent = pagerank_centralization(G)\n",
    "        print(f\"PageRank Centralization: {pagerank_cent}\")\n",
    "\n",
    "        # Calculate Clustering properties\n",
    "        avg_clustering, transitivity = clustering_properties(G)\n",
    "        print(f\"Average Clustering Coefficient: {avg_clustering}\")\n",
    "        print(f\"Transitivity: {transitivity}\")\n",
    "        \n",
    "        properties.append((\n",
    "            num_nodes,\n",
    "            num_edges,\n",
    "            density,\n",
    "            avg_path_len,\n",
    "            diameter,\n",
    "            radius,\n",
    "            clustering_coeff,\n",
    "            transitivity,\n",
    "            assortativity,\n",
    "            num_cliques,\n",
    "            num_triangles,\n",
    "            num_squares,\n",
    "            largest_component_size,\n",
    "            avg_degree,\n",
    "            avg_betweenness_centrality,\n",
    "            spectral_radius,\n",
    "            algebraic_connectivity,\n",
    "            graph_energy,\n",
    "            small_world_coefficient, \n",
    "            betweenness_cent,\n",
    "            pagerank_cent,\n",
    "            avg_clustering,\n",
    "            small_world_index           \n",
    "\n",
    "        ))\n",
    "    return properties\n",
    "\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties_long = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties_long = compute_graph_properties(selected_dataset)\n",
    "\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties_long, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties_long, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"rb\") as f:\n",
    "    train_properties_long = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"rb\") as f:\n",
    "    test_properties_long = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "train_y_long = torch.tensor(train_properties_long, dtype=torch.float32)\n",
    "test_y_long = torch.tensor(test_properties_long, dtype=torch.float32)\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    # Flatten the embeddings before determining the input size\n",
    "    train_embedding_flat = train_embedding.view(train_embedding.size(0), -1)\n",
    "    test_embedding_flat = test_embedding.view(test_embedding.size(0), -1)\n",
    "    #print the shapes\n",
    "    print(train_embedding_flat.shape)\n",
    "    print(test_embedding_flat.shape)\n",
    "    input_size = train_embedding_flat.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names_long):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 2000000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding_flat).squeeze()\n",
    "            target = train_y_long[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding_flat).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding_flat).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y_long[:, i].cpu().numpy()\n",
    "            test_target = test_y_long[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long_full_embedding.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long_full_embedding.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "#make a color list for the properties names but with different colors\n",
    "colors_long = [\n",
    "    (0.0, 0.45, 0.70),  # Blue\n",
    "    (0.85, 0.37, 0.01),  # Orange\n",
    "    (0.8, 0.47, 0.74),   # Magenta\n",
    "    (0.0, 0.62, 0.45),   # Green\n",
    "    (0.95, 0.90, 0.25),  # Yellow\n",
    "    (0.9, 0.6, 0.0),     # Brown\n",
    "    (0.35, 0.7, 0.9),    # Sky Blue\n",
    "    (0.8, 0.6, 0.7),     # Light Pink\n",
    "    (0.3, 0.3, 0.3),     # Dark Gray\n",
    "    (0.5, 0.5, 0.0),     # Olive\n",
    "    (0.0, 0.75, 0.75),   # Cyan\n",
    "    (0.6, 0.6, 0.6),     # Light Gray\n",
    "    (0.7, 0.3, 0.1),     # Dark Orange\n",
    "    (0.6, 0.2, 0.5),     # Purple\n",
    "    (0.9, 0.4, 0.3),     # Salmon\n",
    "    (0.4, 0.4, 0.8),     # Light Blue\n",
    "    (0.2, 0.8, 0.2),     # Light Green\n",
    "    (0.6, 0.6, 0.3),     # Mustard\n",
    "    (0.3, 0.55, 0.55),    # Teal\n",
    "    (0.8, 0.5, 0.2),     # Dark Salmon\n",
    "    (0.5, 0.5, 0.5),     # Gray\n",
    "    (0.2, 0.2, 0.2),      # Black-Gray\n",
    "    (0.0, 0.0, 0.0)      # Black\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_names_long)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R² values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'test_R2_plot_long.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6090954542160034, 0.5785549283027649, 0.5785547494888306, 0.5710922479629517, 0.5627355575561523, 0.5171620845794678, 0.48370879888534546, 0.481626033782959, 0.47943955659866333, 0.47933870553970337, 0.4641278386116028, 0.4609038829803467, 0.45253467559814453, 0.4325084686279297, 0.4175136089324951, 0.3686663508415222, 0.3647555112838745, 0.18807393312454224, 0.13714563846588135, 0.04869192838668823, -0.05669963359832764, -0.09409618377685547, -1.3222777843475342]\n",
      "['num_squares', 'graph_energy', 'num_edges', 'avg_degree', 'density', 'transitivity', 'small_world_index', 'avg_path_len', 'avg_clustering', 'clustering_coeff', 'spectral_radius', 'num_cliques', 'avg_betweenness_centrality', 'algebraic_connectivity', 'num_triangles', 'radius', 'diameter', 'largest_component_size', 'pagerank_cent', 'assortativity', 'num_nodes', 'small_world_coefficient', 'betweenness_cent']\n"
     ]
    }
   ],
   "source": [
    "#print the R2 values in order (bigger to smaller) for x_global\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']\n",
    "\n",
    "#sort the R2 values for x_global\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x_global', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.522092878818512, 0.522091269493103, 0.5128549337387085, 0.5025855898857117, 0.4524869918823242, 0.4169384837150574, 0.4147563576698303, 0.4137481451034546, 0.4122782349586487, 0.39186936616897583, 0.3865439295768738, 0.37806111574172974, 0.35091328620910645, 0.33920663595199585, 0.3346071243286133, 0.3290066123008728, 0.3087393045425415, 0.29865479469299316, 0.16403818130493164, 0.060342609882354736, -0.09877514839172363, -0.14251136779785156, -0.22295606136322021]\n",
      "['graph_energy', 'num_edges', 'avg_degree', 'density', 'avg_path_len', 'num_cliques', 'algebraic_connectivity', 'avg_betweenness_centrality', 'radius', 'small_world_index', 'diameter', 'spectral_radius', 'num_squares', 'num_triangles', 'avg_clustering', 'clustering_coeff', 'largest_component_size', 'transitivity', 'betweenness_cent', 'pagerank_cent', 'num_nodes', 'small_world_coefficient', 'assortativity']\n"
     ]
    }
   ],
   "source": [
    "#same for x6\n",
    "\n",
    "#sort the R2 values for x6\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x6', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0011767745018005371, 1.1265277862548828e-05, -0.006779909133911133, -0.006829738616943359, -0.007808685302734375, -0.007820963859558105, -0.009294867515563965, -0.010425329208374023, -0.01376485824584961, -0.016520977020263672, -0.016980409622192383, -0.01721322536468506, -0.017390012741088867, -0.017391204833984375, -0.024333953857421875, -0.025952935218811035, -0.026882529258728027, -0.02892780303955078, -0.02993631362915039, -0.03466188907623291, -0.051340341567993164, -0.06316637992858887, -0.08530426025390625]\n",
      "['spectral_radius', 'num_triangles', 'avg_clustering', 'clustering_coeff', 'small_world_index', 'radius', 'largest_component_size', 'diameter', 'avg_path_len', 'avg_betweenness_centrality', 'density', 'avg_degree', 'num_edges', 'graph_energy', 'transitivity', 'num_squares', 'pagerank_cent', 'assortativity', 'algebraic_connectivity', 'betweenness_cent', 'num_cliques', 'num_nodes', 'small_world_coefficient']\n"
     ]
    }
   ],
   "source": [
    "#same for x7\n",
    "\n",
    "#sort the R2 values for x7\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x7', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_names_long)][2]\n",
    "        if train_r2 < -0.05:  # Handle negative R² values\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "\n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'train_R2_plot_long_full_embedding.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with a gnn train on random (the y are shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_MDD()\n",
    "MODEL = \"GIN3\"\n",
    "DATASET = \"FC_MDD_suffled\"\n",
    "\n",
    "from models.models_FC import GIN_framework3 as framework3 # import the model\n",
    "\n",
    "print(gnn.model)\n",
    "print(gnn.train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the y values of the dataset\n",
    "# y = np.array([data.y for data in dataset])\n",
    "# print(y)\n",
    "# y = np.array(y, dtype=np.int64)  # Ensure y is a numeric array of type int64\n",
    "\n",
    "# np.random.shuffle(y)\n",
    "\n",
    "# #make y torch.int64, tensor([0]) instead of [0]\n",
    "# y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# for i in range(len(dataset)):\n",
    "#     dataset[i].y = y[i]\n",
    "\n",
    "# # check if the y values are shuffled\n",
    "# y = np.array([data.y for data in dataset])\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataset\n",
    "# import pickle as pkl\n",
    "# with open(\"Datasets/FC/\"+DATASET+\"_\"+MODEL+\".pkl\", \"wb\") as f:\n",
    "#     pkl.dump(dataset, f)\n",
    "\n",
    "#load the dataset\n",
    "import pickle as pkl\n",
    "with open(\"Datasets/FC/\"+DATASET+\"_\"+MODEL+\".pkl\", \"rb\") as f:\n",
    "    dataset = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = framework3(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "# gnn.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\".pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties_long = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties_long = compute_graph_properties(selected_dataset)\n",
    "\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties_long, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties_long, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#save the properties in a file\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(train_properties, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(test_properties, f)\n",
    "\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"rb\") as f:\n",
    "    train_properties = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"rb\") as f:\n",
    "    test_properties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embeddings of GIN are like this:\n",
    "return F.log_softmax(x7, dim=-1), (x1, x2, x3, x4, x5, x_global, x6, x7, x8)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "output_size = 1  # Predicting one property at a time\n",
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Assume we have already evaluated to get features\n",
    "#train_features, test_features = gnn.evaluate_with_features2()\n",
    "\n",
    "# Extract x embeddings\n",
    "train_x = np.array([feat[0] for feat in train_features])\n",
    "test_x = np.array([feat[0] for feat in test_features])\n",
    "\n",
    "# Extract 2, 3, 4, global, 5, 6, 7 embeddings\n",
    "train_x2 = np.array([feat[1] for feat in train_features])\n",
    "test_x2 = np.array([feat[1] for feat in test_features])\n",
    "\n",
    "train_x3 = np.array([feat[2] for feat in train_features])\n",
    "test_x3 = np.array([feat[2] for feat in test_features])\n",
    "\n",
    "train_x4 = np.array([feat[3] for feat in train_features])\n",
    "test_x4 = np.array([feat[3] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[4] for feat in train_features])\n",
    "test_x5 = np.array([feat[4] for feat in test_features])\n",
    "\n",
    "train_x_global = np.array([feat[5] for feat in train_features])\n",
    "test_x_global = np.array([feat[5] for feat in test_features])\n",
    "\n",
    "train_x6 = np.array([feat[6] for feat in train_features])\n",
    "test_x6 = np.array([feat[6] for feat in test_features])\n",
    "\n",
    "train_x7 = np.array([feat[7] for feat in train_features])\n",
    "test_x7 = np.array([feat[7] for feat in test_features])\n",
    "\n",
    "train_x8 = np.array([feat[8] for feat in train_features])\n",
    "test_x8 = np.array([feat[8] for feat in test_features])\n",
    "\n",
    "# Compute graph properties\n",
    "# train_properties = compute_graph_properties(gnn.dataset[gnn.train_idx])\n",
    "# test_properties = compute_graph_properties(gnn.dataset[gnn.test_idx])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_x2 = torch.tensor(train_x2, dtype=torch.float32)\n",
    "train_x3 = torch.tensor(train_x3, dtype=torch.float32)\n",
    "train_x4 = torch.tensor(train_x4, dtype=torch.float32)\n",
    "train_x5 = torch.tensor(train_x5, dtype=torch.float32)\n",
    "train_x_global = torch.tensor(train_x_global, dtype=torch.float32)\n",
    "train_x6 = torch.tensor(train_x6, dtype=torch.float32)\n",
    "train_x7 = torch.tensor(train_x7, dtype=torch.float32)\n",
    "train_x8 = torch.tensor(train_x8, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x2 = torch.tensor(test_x2, dtype=torch.float32)\n",
    "test_x3 = torch.tensor(test_x3, dtype=torch.float32)\n",
    "test_x4 = torch.tensor(test_x4, dtype=torch.float32)\n",
    "test_x5 = torch.tensor(test_x5, dtype=torch.float32)\n",
    "test_x_global = torch.tensor(test_x_global, dtype=torch.float32)\n",
    "test_x6 = torch.tensor(test_x6, dtype=torch.float32)\n",
    "test_x7 = torch.tensor(test_x7, dtype=torch.float32)\n",
    "test_x8 = torch.tensor(test_x8, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_properties, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_properties, dtype=torch.float32)\n",
    "\n",
    "# Train and evaluate a model for each graph property and each embedding\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "embeddings = [(train_x, test_x), (train_x2, test_x2), (train_x3, test_x3), (train_x4, test_x4), (train_x5, test_x5), (train_x_global, test_x_global), (train_x6, test_x6), (train_x7, test_x7), (train_x8, test_x8)]\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "train_y_long = torch.tensor(train_properties_long, dtype=torch.float32)\n",
    "test_y_long = torch.tensor(test_properties_long, dtype=torch.float32)\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    # Flatten the embeddings before determining the input size\n",
    "    train_embedding_flat = train_embedding.view(train_embedding.size(0), -1)\n",
    "    test_embedding_flat = test_embedding.view(test_embedding.size(0), -1)\n",
    "    #print the shapes\n",
    "    print(train_embedding_flat.shape)\n",
    "    print(test_embedding_flat.shape)\n",
    "    input_size = train_embedding_flat.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names_long):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 2000000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding_flat).squeeze()\n",
    "            target = train_y_long[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding_flat).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding_flat).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y_long[:, i].cpu().numpy()\n",
    "            test_target = test_y_long[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long_full_embedding.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results \n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long_full_embedding.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors_long = [\n",
    "    (0.0, 0.45, 0.70),  # Blue\n",
    "    (0.85, 0.37, 0.01),  # Orange\n",
    "    (0.8, 0.47, 0.74),   # Magenta\n",
    "    (0.0, 0.62, 0.45),   # Green\n",
    "    (0.95, 0.90, 0.25),  # Yellow\n",
    "    (0.9, 0.6, 0.0),     # Brown\n",
    "    (0.35, 0.7, 0.9),    # Sky Blue\n",
    "    (0.8, 0.6, 0.7),     # Light Pink\n",
    "    (0.3, 0.3, 0.3),     # Dark Gray\n",
    "    (0.5, 0.5, 0.0),     # Olive\n",
    "    (0.0, 0.75, 0.75),   # Cyan\n",
    "    (0.6, 0.6, 0.6),     # Light Gray\n",
    "    (0.7, 0.3, 0.1),     # Dark Orange\n",
    "    (0.6, 0.2, 0.5),     # Purple\n",
    "    (0.9, 0.4, 0.3),     # Salmon\n",
    "    (0.4, 0.4, 0.8),     # Light Blue\n",
    "    (0.2, 0.8, 0.2),     # Light Green\n",
    "    (0.6, 0.6, 0.3),     # Mustard\n",
    "    (0.3, 0.55, 0.55),    # Teal\n",
    "    (0.8, 0.5, 0.2),     # Dark Salmon\n",
    "    (0.5, 0.5, 0.5),     # Gray\n",
    "    (0.2, 0.2, 0.2),      # Black-Gray\n",
    "    (0.0, 0.0, 0.0)      # Black\n",
    "]\n",
    "\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_names_long)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R² values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'test_R2_plot_long_random_full_embedding.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node embedding probing\n",
    "\n",
    "/!\\ Try to not forget that we need to change the batch_size to 1 if we want to probe for node properties as we need the forward pass to be made 1 graph by 1 graph at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_MDD()\n",
    "import torch\n",
    "torch.manual_seed(37)\n",
    "MODEL = \"GIN3\"\n",
    "DATASET = \"FC_MDD\"\n",
    "from models.models_FC import GIN_framework3 as framework3 # import the model\n",
    "gnn = framework(dataset)\n",
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\".pt\")\n",
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Node properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Define function to compute node-level properties\n",
    "def compute_node_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        node_degrees = list(dict(G.degree()).values())\n",
    "        clustering_coeffs = list(nx.clustering(G).values())\n",
    "        betweenness_centralities = list(nx.betweenness_centrality(G).values())\n",
    "        eigenvector_centralities = list(nx.eigenvector_centrality(G, max_iter=10000).values())\n",
    "        Local_clustering_coefficients = list(nx.clustering(G).values())\n",
    "\n",
    "        properties.append((node_degrees, clustering_coeffs, betweenness_centralities, eigenvector_centralities, Local_clustering_coefficients))\n",
    "    return properties\n",
    "\n",
    "# Compute node-level properties for train and test sets\n",
    "# Ensure gnn.train_idx and gnn.test_idx are lists of integers\n",
    "train_idx = gnn.train_idx.tolist() if isinstance(gnn.train_idx, torch.Tensor) else gnn.train_idx\n",
    "test_idx = gnn.test_idx.tolist() if isinstance(gnn.test_idx, torch.Tensor) else gnn.test_idx\n",
    "\n",
    "# Compute node-level properties for train and test sets\n",
    "train_node_properties = compute_node_properties([gnn.dataset[i] for i in train_idx])\n",
    "test_node_properties = compute_node_properties([gnn.dataset[i] for i in test_idx])\n",
    "\n",
    "#train_node_properties is a list of tuples, where each tuple contains 5 lists, each list contains the node-level property for each node in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_node_properties), len(test_node_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the first three betweenness centralities of the three first graphs in the train set\n",
    "[len(train_node_properties[i][2]) for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Ensure gnn.test_idx is a list of integers\n",
    "test_idx = gnn.test_idx.tolist() if isinstance(gnn.test_idx, torch.Tensor) else gnn.test_idx\n",
    "\n",
    "# Visualize the first graph of the test set to see if the properties are correct\n",
    "G = nx.from_edgelist(gnn.dataset[test_idx[0]].edge_index.t().tolist())\n",
    "nx.draw(G, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#represent the node degrees of the first graph in the test set\n",
    "node_degrees = test_node_properties[0][0]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_size=[v * 100 for v in node_degrees], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#represent the properties of betweenness centrality on the graph by chaging the size of the nodes\n",
    "betweenness_centrality = test_node_properties[0][2]\n",
    "node_degrees = test_node_properties[0][0]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_size=[v * 10000 for v in betweenness_centrality], node_color=node_degrees, cmap='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for eigenvector centrality\n",
    "eigenvector_centralities = test_node_properties[0][3]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_size=[v * 10000 for v in eigenvector_centralities], node_color=node_degrees, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for local clustering coefficients\n",
    "Local_clustering_coefficients = test_node_properties[0][4]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_size=[v * 1000 for v in Local_clustering_coefficients], node_color=node_degrees, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each graph, the length of the betweenness centralities, and in general the length of properties, are equal to the number of nodes in the graph and thus\n",
    "is equal to the length of the x matrix in the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2(return_node_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the first graph's features\n",
    "first_graph_features = train_features[1]\n",
    "for i, feature in enumerate(first_graph_features):\n",
    "    print(f\"Feature {i+1} shape:\", feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[1][0][115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probing for the top n nodes on train_features only and averaging the results of the different diagnostic classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Function to get the top 3 nodes based on a specific property\n",
    "def get_top_nodes(property_list, top_n=37):\n",
    "    sorted_indices = sorted(range(len(property_list)), key=lambda k: property_list[k], reverse=True)\n",
    "    return sorted_indices[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the top 3 nodes for local clustering coefficient and eigenvector centrality\n",
    "top_nodes_degrees = [get_top_nodes(graph_props[0], 37) for graph_props in train_node_properties]  # Assuming 0th index is for node degrees\n",
    "top_nodes_clustering = [get_top_nodes(graph_props[1], 37) for graph_props in train_node_properties]  # Assuming 1st index is for clustering coefficient\n",
    "top_nodes_betweenness = [get_top_nodes(graph_props[2], 37) for graph_props in train_node_properties]  # Assuming 2nd index is for betweenness centrality\n",
    "top_nodes_local_clustering = [get_top_nodes(graph_props[3], 37) for graph_props in train_node_properties]  # Assuming 3rd index is for local clustering coefficient\n",
    "top_nodes_eigenvector = [get_top_nodes(graph_props[4], 37) for graph_props in train_node_properties]  # Assuming 4th index is for eigenvector centrality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for linear regression model training\n",
    "def prepare_regression_data(features, properties, top_nodes_indices):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, graph_features in enumerate(features):\n",
    "        for layer in range(len(graph_features)):\n",
    "            for node_index in top_nodes_indices[i]:\n",
    "                X.append(graph_features[layer][node_index])\n",
    "                y.append(properties[i][node_index])\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "# Training and evaluating linear regression models\n",
    "def train_and_evaluate_regression(X, y):\n",
    "    model = LinearModel(X.shape[1], 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X.float())\n",
    "        loss = criterion(outputs, y.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X.float()).view(-1)\n",
    "        r2 = r2_score(y.float(), predictions)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare data for node degree regression\n",
    "X_node_degree, y_node_degree = prepare_regression_data(train_features, [props[0] for props in train_node_properties], top_nodes_degrees)\n",
    "\n",
    "# Train and evaluate model for node degree\n",
    "r2_node_degree = train_and_evaluate_regression(X_node_degree, y_node_degree)\n",
    "print(f'R² for node degree prediction: {r2_node_degree}')\n",
    "\n",
    "# Prepare data for betweenness centrality regression\n",
    "X_betweenness, y_betweenness = prepare_regression_data(train_features, [props[2] for props in train_node_properties], top_nodes_betweenness)\n",
    "\n",
    "# Train and evaluate model for betweenness centrality\n",
    "r2_betweenness = train_and_evaluate_regression(X_betweenness, y_betweenness)\n",
    "print(f'R² for betweenness centrality prediction: {r2_betweenness}')\n",
    "\n",
    "# Prepare data for local clustering coefficient regression\n",
    "X_local_clustering, y_local_clustering = prepare_regression_data(train_features, [props[3] for props in train_node_properties], top_nodes_local_clustering)\n",
    "\n",
    "# Train and evaluate model for local clustering coefficient\n",
    "r2_local_clustering = train_and_evaluate_regression(X_local_clustering, y_local_clustering)\n",
    "print(f'R² for local clustering coefficient prediction: {r2_local_clustering}')\n",
    "\n",
    "# Prepare data for eigenvector centrality regression\n",
    "X_eigenvector, y_eigenvector = prepare_regression_data(train_features, [props[4] for props in train_node_properties], top_nodes_eigenvector)\n",
    "\n",
    "# Train and evaluate model for eigenvector centrality\n",
    "r2_eigenvector = train_and_evaluate_regression(X_eigenvector, y_eigenvector)\n",
    "print(f'R² for eigenvector centrality prediction: {r2_eigenvector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probing for the top n nodes with diagnostic classifier trained on the train set and test on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define Linear Model for probing (diagnostic classifier)\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Prepare the data for probing classifier\n",
    "def prepare_regression_data(features, properties, property_index, top_n_nodes=37):\n",
    "    X_layers = [[] for _ in range(len(features[0]))]\n",
    "    y_layers = [[] for _ in range(len(features[0]))]\n",
    "\n",
    "    for i, graph_features in enumerate(features):\n",
    "        top_nodes_indices = get_top_nodes(properties[i][property_index], top_n=top_n_nodes)\n",
    "        for layer in range(len(graph_features)):\n",
    "            for node_index in top_nodes_indices:\n",
    "                X_layers[layer].append(graph_features[layer][node_index])\n",
    "                y_layers[layer].append(properties[i][property_index][node_index])\n",
    "\n",
    "    X_layers = [torch.tensor(X) for X in X_layers]\n",
    "    y_layers = [torch.tensor(y) for y in y_layers]\n",
    "    \n",
    "    return X_layers, y_layers\n",
    "\n",
    "# Train and evaluate the probing classifier for each layer\n",
    "def train_and_evaluate_regression(X_train_layers, y_train_layers, X_test_layers, y_test_layers):\n",
    "    r2_scores_train = []\n",
    "    r2_scores_test = []\n",
    "    \n",
    "    for layer in range(len(X_train_layers)):\n",
    "        X_train = X_train_layers[layer]\n",
    "        y_train = y_train_layers[layer]\n",
    "        X_test = X_test_layers[layer]\n",
    "        y_test = y_test_layers[layer]\n",
    "        \n",
    "        model = LinearModel(X_train.shape[1], 1)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(10000):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train.float())\n",
    "            loss = criterion(outputs, y_train.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Layer {layer+1}, Epoch {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions_train = model(X_train.float()).view(-1)\n",
    "            predictions_test = model(X_test.float()).view(-1)\n",
    "            r2_train = r2_score(y_train.float(), predictions_train)\n",
    "            r2_test = r2_score(y_test.float(), predictions_test)\n",
    "        r2_scores_train.append(r2_train)\n",
    "        r2_scores_test.append(r2_test)\n",
    "    \n",
    "    return r2_scores_train, r2_scores_test\n",
    "\n",
    "# Aggregate R² scores across all graphs\n",
    "def aggregate_r2_scores(features_train, properties_train, features_test, properties_test, property_index):\n",
    "    X_train_layers, y_train_layers = prepare_regression_data(features_train, properties_train, property_index)\n",
    "    X_test_layers, y_test_layers = prepare_regression_data(features_test, properties_test, property_index)\n",
    "\n",
    "    #save the results in a file\n",
    "    with open(\"results/\"+DATASET+\"_\"+MODEL+\"_X_train_layers.pkl\", \"wb\") as f:\n",
    "        pkl.dump(X_train_layers, f)\n",
    "\n",
    "    with open(\"results/\"+DATASET+\"_\"+MODEL+\"_y_train_layers.pkl\", \"wb\") as f:\n",
    "        pkl.dump(y_train_layers, f)\n",
    "    \n",
    "    r2_scores_train, r2_scores_test = train_and_evaluate_regression(X_train_layers, y_train_layers, X_test_layers, y_test_layers)\n",
    "    \n",
    "    return r2_scores_train, r2_scores_test\n",
    "\n",
    "# Properties indices: 0 - node_degrees, 1 - clustering_coeffs, 2 - betweenness_centralities, 3 - eigenvector_centralities, 4 - Local_clustering_coefficients\n",
    "properties_indices = [0, 1, 2, 3, 4]\n",
    "property_names = ['Node Degrees', 'Clustering Coefficients', 'Betweenness Centralities', 'Eigenvector Centralities', 'Local Clustering Coefficients']\n",
    "\n",
    "# Initialize dictionaries to store average R² scores across all layers\n",
    "avg_r2_train_dict = {name: [] for name in property_names}\n",
    "avg_r2_test_dict = {name: [] for name in property_names}\n",
    "\n",
    "# Train and evaluate the probing classifier for each property\n",
    "for prop_idx, prop_name in zip(properties_indices, property_names):\n",
    "    print(f\"Processing property: {prop_name}\")\n",
    "    avg_r2_train_dict[prop_name], avg_r2_test_dict[prop_name] = aggregate_r2_scores(train_features, train_node_properties, test_features, test_node_properties, prop_idx)\n",
    "\n",
    "# Plotting the average R² scores across layers for each property\n",
    "layers = np.arange(len(avg_r2_train_dict[property_names[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\tomdu\\AppData\\Local\\Temp\\ipykernel_17916\\2978085347.py\", line 17, in <module>\n",
      "    with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_train_dict_long.pkl\", \"rb\") as f:\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 284, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'results/FC_suffled_GCN_w_edge_weight_avg_r2_train_dict_long.pkl'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1062, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1114, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\pygments\\style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"c:\\Users\\tomdu\\miniconda3\\lib\\site-packages\\pygments\\style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#save all the variables necessary for the plot\n",
    "# import pickle as pkl\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_train_dict_long.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(avg_r2_train_dict, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_test_dict_long.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(avg_r2_test_dict, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_layers_long.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(layers, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_property_names_long.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(property_names, f)\n",
    "\n",
    "#load all the variables necessary for the plot\n",
    "import pickle as pkl\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_train_dict_long.pkl\", \"rb\") as f:\n",
    "    avg_r2_train_dict = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_test_dict_long.pkl\", \"rb\") as f:\n",
    "    avg_r2_test_dict = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_layers_long.pkl\", \"rb\") as f:\n",
    "    layers = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_property_names_long.pkl\", \"rb\") as f:\n",
    "    property_names = pkl.load(f)\n",
    "\n",
    "    #load the layer results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_X_train_layers.pkl\", \"rb\") as f:\n",
    "    X_train_layers = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_y_train_layers.pkl\", \"rb\") as f:\n",
    "    y_train_layers = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for average R² scores\n",
    "for prop_name in property_names:\n",
    "    plt.plot(layers, avg_r2_train_dict[prop_name], label=f'{prop_name} (Train)', marker='o')\n",
    "    plt.plot(layers, avg_r2_test_dict[prop_name], label=f'{prop_name} (Test)', linestyle='--', marker='x')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average R² Score')\n",
    "plt.title('Average R² Score for Node Properties Prediction Across Layers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot only test results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for average R² scores\n",
    "for prop_name in property_names:\n",
    "    plt.plot(layers, avg_r2_test_dict[prop_name], label=f'{prop_name} (Test)', linestyle='--', marker='x')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average R² Score')\n",
    "plt.title('Average R² Score for Node Properties Prediction Across Layers')\n",
    "plt.legend()\n",
    "#x axis called layer 1, layer 2, etc\n",
    "plt.xticks(range(len(layers)), [f'Layer {i+1}' for i in layers])           \n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y=1 and y=0 as two different plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels from the dataset using train_idx_list and test_idx_list\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "\n",
    "train_labels = [gnn.dataset[i].y.item() for i in train_idx_list]\n",
    "test_labels = [gnn.dataset[i].y.item() for i in test_idx_list]\n",
    "\n",
    "# Split the dataset by label y=0 and y=1\n",
    "def split_by_label(features, properties, labels):\n",
    "    features_0, properties_0, features_1, properties_1 = [], [], [], []\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 0:\n",
    "            features_0.append(features[i])\n",
    "            properties_0.append(properties[i])\n",
    "        else:\n",
    "            features_1.append(features[i])\n",
    "            properties_1.append(properties[i])\n",
    "    \n",
    "    return features_0, properties_0, features_1, properties_1\n",
    "\n",
    "# Assuming you have train_features, train_node_properties, test_features, test_node_properties from your GNN\n",
    "train_features_0, train_node_properties_0, train_features_1, train_node_properties_1 = split_by_label(train_features, train_node_properties, train_labels)\n",
    "test_features_0, test_node_properties_0, test_features_1, test_node_properties_1 = split_by_label(test_features, test_node_properties, test_labels)\n",
    "\n",
    "# Properties indices: 0 - node_degrees, 1 - clustering_coeffs, 2 - betweenness_centralities, 3 - eigenvector_centralities, 4 - Local_clustering_coefficients\n",
    "properties_indices = [0, 1, 2, 3, 4]\n",
    "property_names = ['Node Degrees', 'Clustering Coefficients', 'Betweenness Centralities', 'Eigenvector Centralities', 'Local Clustering Coefficients']\n",
    "\n",
    "# Initialize dictionaries to store average R² scores across all layers for y=0 and y=1\n",
    "avg_r2_train_dict_0 = {name: [] for name in property_names}\n",
    "avg_r2_test_dict_0 = {name: [] for name in property_names}\n",
    "avg_r2_train_dict_1 = {name: [] for name in property_names}\n",
    "avg_r2_test_dict_1 = {name: [] for name in property_names}\n",
    "\n",
    "# Train and evaluate the probing classifier for each property for y=0\n",
    "for prop_idx, prop_name in zip(properties_indices, property_names):\n",
    "    print(f\"Processing property for y=0: {prop_name}\")\n",
    "    avg_r2_train_dict_0[prop_name], avg_r2_test_dict_0[prop_name] = aggregate_r2_scores(train_features_0, train_node_properties_0, test_features_0, test_node_properties_0, prop_idx)\n",
    "\n",
    "# Train and evaluate the probing classifier for each property for y=1\n",
    "for prop_idx, prop_name in zip(properties_indices, property_names):\n",
    "    print(f\"Processing property for y=1: {prop_name}\")\n",
    "    avg_r2_train_dict_1[prop_name], avg_r2_test_dict_1[prop_name] = aggregate_r2_scores(train_features_1, train_node_properties_1, test_features_1, test_node_properties_1, prop_idx)\n",
    "\n",
    "# Plotting the average R² scores across layers for each property, separately for y=0 and y=1\n",
    "layers = np.arange(len(avg_r2_train_dict_0[property_names[0]]))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Plot for y=0\n",
    "plt.subplot(2, 1, 1)\n",
    "for prop_name in property_names:\n",
    "    plt.plot(layers, avg_r2_train_dict_0[prop_name], label=f'{prop_name} (Train)', marker='o')\n",
    "    plt.plot(layers, avg_r2_test_dict_0[prop_name], label=f'{prop_name} (Test)', linestyle='--', marker='x')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average R² Score')\n",
    "plt.title('Average R² Score for Node Properties Prediction Across Layers (y=0)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for y=1\n",
    "plt.subplot(2, 1, 2)\n",
    "for prop_name in property_names:\n",
    "    plt.plot(layers, avg_r2_train_dict_1[prop_name], label=f'{prop_name} (Train)', marker='o')\n",
    "    plt.plot(layers, avg_r2_test_dict_1[prop_name], label=f'{prop_name} (Test)', linestyle='--', marker='x')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average R² Score')\n",
    "plt.title('Average R² Score for Node Properties Prediction Across Layers (y=1)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node embedding probing\n",
    "\n",
    "/!\\ Try to not forget that we need to change the batch_size to 1 if we want to probe for node properties as we need the forward pass to be made 1 graph by 1 graph at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_MDD()\n",
    "import torch\n",
    "torch.manual_seed(37)\n",
    "MODEL = \"GIN\"\n",
    "DATASET = \"FC_MDD\"\n",
    "from models.models_FC import GIN_framework as framework # import the model\n",
    "gnn = framework(dataset)\n",
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\", map_location='cpu')\n",
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "def compute_node_properties(data, indices):\n",
    "    properties = []\n",
    "    for idx in indices:\n",
    "        graph_data = data[idx]\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        \n",
    "        # Add all nodes to the graph to handle disconnected nodes\n",
    "        all_nodes = set(range(len(graph_data.x)))\n",
    "        connected_nodes = set(G.nodes())\n",
    "        disconnected_nodes = all_nodes - connected_nodes\n",
    "        \n",
    "        # Calculate node properties using NetworkX for connected nodes\n",
    "        degree = dict(G.degree())\n",
    "        clustering = nx.clustering(G)\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        closeness = nx.closeness_centrality(G)\n",
    "        eigenvector = nx.eigenvector_centrality(G, max_iter=10000)\n",
    "        pagerank = nx.pagerank(G)\n",
    "\n",
    "        # Initialize properties with zeros for all nodes\n",
    "        node_properties = [{'degree': 0, 'clustering': 0, 'betweenness': 0, 'closeness': 0, 'eigenvector': 0, 'pagerank': 0} for _ in all_nodes]\n",
    "        \n",
    "        # Store properties for each connected node in the graph\n",
    "        for node in connected_nodes:\n",
    "            node_properties[node] = {\n",
    "                'degree': degree[node],\n",
    "                'clustering': clustering[node],\n",
    "                'betweenness': betweenness[node],\n",
    "                'closeness': closeness[node],\n",
    "                'eigenvector': eigenvector[node],\n",
    "                'pagerank': pagerank[node]\n",
    "            }\n",
    "\n",
    "        #if there a disconnected nodes : print \n",
    "        # if disconnected_nodes:\n",
    "        #     print(f\"Graph {idx}: Disconnected nodes: {disconnected_nodes}\")\n",
    "        #     print(node_properties)\n",
    "\n",
    "        properties.append(node_properties)\n",
    "    return properties\n",
    "\n",
    "# Ensure gnn.train_idx and gnn.test_idx are lists of integers\n",
    "train_idx = gnn.train_idx.tolist() if isinstance(gnn.train_idx, torch.Tensor) else gnn.train_idx\n",
    "test_idx = gnn.test_idx.tolist() if isinstance(gnn.test_idx, torch.Tensor) else gnn.test_idx\n",
    "\n",
    "# Compute node-level properties for train and test sets\n",
    "train_node_properties = compute_node_properties(gnn.dataset, train_idx)\n",
    "test_node_properties = compute_node_properties(gnn.dataset, test_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2(return_node_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the first graph's features\n",
    "first_graph_features = train_features[1]\n",
    "for i, feature in enumerate(first_graph_features):\n",
    "    print(f\"Feature {i+1} shape:\", feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[1][0][115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_features))\n",
    "print(len(train_features[0]))\n",
    "print(train_features[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_node_properties))\n",
    "print(len(train_node_properties[0]))\n",
    "print(train_node_properties[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier\n",
    "\n",
    "**Probe**\n",
    "\n",
    "Example data structure for multiple graphs\n",
    "\n",
    "train_features: List of graphs, each with multiple layers of features\n",
    "\n",
    "```plaintext\n",
    "train_features = [\n",
    "    [np.array([...]), np.array([...]), ...],  # Graph 1: features for each layer\n",
    "    [np.array([...]), np.array([...]), ...],  # Graph 2: features for each layer\n",
    "    ...\n",
    "]\n",
    "```\n",
    "train_node_properties: List of graphs, each with a list of node properties\n",
    "\n",
    "```plaintext\n",
    "train_node_properties = [\n",
    "    [{'degree': ..., 'clustering': ..., ...}, {'degree': ..., ...}, ...],  # Graph 1: properties for each node\n",
    "    [{'degree': ..., 'clustering': ..., ...}, {'degree': ..., ...}, ...],  # Graph 2: properties for each node\n",
    "    ...\n",
    "]\n",
    "```\n",
    "test_features and test_node_properties would be similarly structured for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probe_for_layer(features, property_values, test_features, test_property_values, num_epochs=10000, learning_rate=0.01):\n",
    "    # Convert features and property values to PyTorch tensors if they are NumPy arrays\n",
    "    if isinstance(features, np.ndarray):\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "    if isinstance(property_values, np.ndarray):\n",
    "        property_values = torch.tensor(property_values, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(test_features, np.ndarray):\n",
    "        test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "    if isinstance(test_property_values, np.ndarray):\n",
    "        test_property_values = torch.tensor(test_property_values, dtype=torch.float32)\n",
    "\n",
    "    print(f\"Training on features with shape: {features.shape} for property values shape: {property_values.shape}\")\n",
    "\n",
    "    model = LinearModel(features.shape[1])  # Features should be 2D: (num_nodes, feature_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features).squeeze()  # Remove single-dimensional entries\n",
    "        loss = criterion(output, property_values)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(test_features).squeeze()\n",
    "        mse = criterion(pred, test_property_values).item()\n",
    "        # Flatten the tensors for proper use of r2_score\n",
    "        r2 = r2_score(test_property_values.cpu().numpy(), pred.cpu().numpy())\n",
    "\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Option 2: Train a Single Classifier Across All Graphs\n",
    "\n",
    "This approach involves combining data from all graphs to train a single probe for each property across the graphs. This assumes that the properties across different graphs share some common structure that can be captured by a single model. We modify the `evaluate_layer_probes` function to aggregate features and properties across all graphs before training.\n",
    "\n",
    "This approach results in a single probe being trained for each property at each layer, but the probe is trained on data aggregated from all graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_layer_probes_across_graphs(train_features_list, test_features_list, train_properties_list, test_properties_list):\n",
    "    num_layers = len(train_features_list[0])  # Assuming all graphs have the same number of layers\n",
    "    results = []\n",
    "\n",
    "    num_test_graphs = len(test_features_list)  # Number of graphs in the test set\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        combined_train_features = []\n",
    "        combined_test_features = []\n",
    "        combined_train_properties = []\n",
    "        combined_test_properties = []\n",
    "\n",
    "        # Aggregate features and properties across all graphs\n",
    "        for graph_idx in range(len(train_features_list)):\n",
    "            combined_train_features.append(np.vstack(train_features_list[graph_idx][layer_idx]))\n",
    "\n",
    "            # Use modulo to cycle through the test graphs\n",
    "            test_idx = graph_idx % num_test_graphs\n",
    "            combined_test_features.append(np.vstack(test_features_list[test_idx][layer_idx]))\n",
    "\n",
    "            combined_train_properties.extend(train_properties_list[graph_idx])\n",
    "            combined_test_properties.extend(test_properties_list[test_idx])\n",
    "\n",
    "        combined_train_features = np.vstack(combined_train_features)  # Combine features across graphs\n",
    "        combined_test_features = np.vstack(combined_test_features)\n",
    "\n",
    "        # Train and evaluate probe for each property across all graphs\n",
    "        for property_name in combined_train_properties[0].keys():  # Assuming all nodes have the same properties\n",
    "            train_property_values = np.array([node[property_name] for node in combined_train_properties])\n",
    "            test_property_values = np.array([node[property_name] for node in combined_test_properties])\n",
    "\n",
    "            mse, r2 = train_probe_for_layer(combined_train_features, train_property_values, combined_test_features, test_property_values)\n",
    "            \n",
    "            results.append({\n",
    "                'layer': layer_idx,\n",
    "                'property': property_name,\n",
    "                'mse': mse,\n",
    "                'r2': r2\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_across_graphs = evaluate_layer_probes_across_graphs(train_features, test_features, train_node_properties, test_node_properties)\n",
    "\n",
    "#save the results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_node_results_across_graphs.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results_across_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_node_results_across_graphs.pkl\", \"rb\") as f:\n",
    "    results_across_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Between Options:\n",
    "\n",
    "- If our graphs are very similar in nature and you expect the relationships between node embeddings and their properties to be consistent across all graphs, **Option 2** (Single Classifier Across Graphs) will be the better choice.\n",
    "  \n",
    "- If our graphs are diverse, or we expect the relationships to vary significantly between graphs, *Option 1* (Separate Classifiers) might be more appropriate.\n",
    "\n",
    "We experiment with both approaches and compare the performance to see which one gives us the best results : we decided to keep option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "- *Layers and Properties*: The function iterate over the layers and properties to aggregate and visualize the R² scores.\n",
    "  \n",
    "- *Mean R² Calculation*: The mean R² scores are calculated for each layer and property. If any R² value is below `-0.05`, it is set to `-0.05` to prevent extreme values from skewing the visualization.\n",
    "\n",
    "- *Plotting*:\n",
    "  - *Option 1 (`plot_results_per_graph`)*: Plots a separate line for each graph, allowing you to see how the R² scores vary across layers and graphs.\n",
    "  - *Option 2 (`plot_results_across_graphs`)*: Plots a single line for each property, aggregating the results across all graphs. This provides a high-level view of how each property behaves across layers when considering all graphs together.\n",
    "  \n",
    "We decided to keep option 2 as it makes more sense. This function is designed to visualize the results from the `evaluate_layer_probes_across_graphs` function. It will plot the R² scores for each property across layers, combining the results from all graphs into single lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_results_across_graphs(results):\n",
    "    layers = sorted(set(result['layer'] for result in results))\n",
    "    properties = sorted(set(result['property'] for result in results))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))  # Increase the figure size for better readability\n",
    "    \n",
    "    for property_name in properties:\n",
    "        r2_scores = []\n",
    "        for layer in layers:\n",
    "            layer_results = [r for r in results if r['layer'] == layer and r['property'] == property_name]\n",
    "            # Calculate mean R² score for the layer\n",
    "            mean_r2 = np.mean([r['r2'] for r in layer_results])\n",
    "            # Set any R² value below -0.05 to -0.05\n",
    "            if mean_r2 < -0.05:\n",
    "                mean_r2 = -0.05\n",
    "            r2_scores.append(mean_r2)\n",
    "        \n",
    "        # Plot the R² scores with crosses and lines, one line per property across all graphs\n",
    "        plt.plot(layers, r2_scores, marker='x', linestyle='-', label=property_name)\n",
    "\n",
    "    plt.title('R² Scores Across Layers for Different Properties (Aggregated Across Graphs)')\n",
    "    plt.xticks(layers, [f'Layer {i+1}' for i in layers])  # Set the x-axis labels to layer numbers\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)  # Add grid for better visibility of points and lines\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    with open(\"results/\"+DATASET+\"_\"+MODEL+\"_node_results_across_graphs.pkl\", \"wb\") as f:\n",
    "        pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results across all graphs\n",
    "plot_results_across_graphs(results_across_graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
