{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing GIN MDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll first be loading the FC matrices and explore their structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using read_dataset from Datasets/FC/create_dataset.py to read the dataset\n",
    "from Datasets.FC.create_dataset import read_dataset_MDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_MDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[116, 116], edge_index=[2, 1294], edge_attr=[1294, 1], y=[1])\n",
      "['edge_index', 'y', 'edge_attr', 'x']\n",
      "ValuesView({'x': tensor([[ 0.0000,  0.2857,  0.0804,  ...,  0.2032,  0.1674,  0.0906],\n",
      "        [ 0.2857,  0.0000, -0.3860,  ...,  0.1637, -0.0359,  0.1674],\n",
      "        [ 0.0804, -0.3860,  0.0000,  ..., -0.0175, -0.0309,  0.0296],\n",
      "        ...,\n",
      "        [ 0.2032,  0.1637, -0.0175,  ...,  0.0000,  0.2329, -0.1452],\n",
      "        [ 0.1674, -0.0359, -0.0309,  ...,  0.2329,  0.0000,  0.0183],\n",
      "        [ 0.0906,  0.1674,  0.0296,  ..., -0.1452,  0.0183,  0.0000]]), 'edge_index': tensor([[  0,   0,   0,  ..., 113, 113, 114],\n",
      "        [ 10,  12,  14,  ..., 111, 112, 108]]), 'edge_attr': tensor([[0.7785],\n",
      "        [0.6966],\n",
      "        [0.5463],\n",
      "        ...,\n",
      "        [0.6415],\n",
      "        [0.4745],\n",
      "        [0.5401]]), 'y': tensor([0])})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1604"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok, let's explore the data a bit more\n",
    "#dataset is a list object of torch_geometric.data objects\n",
    "\n",
    "#let's see the first element\n",
    "print(dataset[0])\n",
    "\n",
    "#it's a dictionary object, let's see the keys\n",
    "print(dataset[0].keys())\n",
    "\n",
    "#ok, let's see the values\n",
    "print(dataset[0].values())\n",
    "\n",
    "#it has 4 keys, 'x', 'edge_index', 'edge_attr' and 'y' where y=0 menas the patient is healthy and y=1 means the patient has Autism Spectrum Disorder (ASD)\n",
    "\"\"\"graph = Data(x=ROI.reshape(-1,116).float(),\n",
    "                     edge_index=G.indices().reshape(2,-1).long(),\n",
    "                     edge_attr=G.values().reshape(-1,1).float(),\n",
    "                     y=y.long())\"\"\"\n",
    "\n",
    "#how much data do we have?\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#set the seed\n",
    "import torch\n",
    "torch.manual_seed(37)\n",
    "\n",
    "DATASET = \"FC_MDD\"\n",
    "\n",
    "MODEL = \"GIN\"\n",
    "from models.models_FC import GIN_framework as framework # import the model\n",
    "gnn = framework(dataset)\n",
    "\n",
    "MODELbis = \"GINbis\"\n",
    "from models.models_FC import GIN_framework_bis as framework # import the model\n",
    "gnnbis = framework(dataset)\n",
    "\n",
    "MODELtri = \"GINtri\"\n",
    "from models.models_FC import GIN_framework_tri as framework # import the model\n",
    "gnntri = framework(dataset)\n",
    "\n",
    "MODEL2 = \"GIN2\"\n",
    "from models.models_FC import GIN_framework2 as framework2 # import the model\n",
    "gnn2 = framework2(dataset)\n",
    "\n",
    "MODEL3 = \"GIN3\"\n",
    "from models.models_FC import GIN_framework3 as framework3 # import the model\n",
    "gnn3 = framework3(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/GIN_best_model.pth\n",
      "Model saved in: models/GIN_best_model.pth\n",
      "Epoch: 005, Loss: 0.656, Test Loss: 0.730, Train Acc: 0.686 Test Acc: 0.543\n",
      "Model saved in: models/GIN_best_model.pth\n",
      "Epoch: 010, Loss: 0.443, Test Loss: 0.918, Train Acc: 0.752 Test Acc: 0.568\n",
      "Epoch: 015, Loss: 0.307, Test Loss: 1.743, Train Acc: 0.818 Test Acc: 0.432\n",
      "Epoch: 020, Loss: 0.237, Test Loss: 1.464, Train Acc: 0.871 Test Acc: 0.543\n",
      "Epoch: 025, Loss: 0.108, Test Loss: 1.793, Train Acc: 0.924 Test Acc: 0.543\n",
      "Model saved in: models/GIN_best_model.pth\n",
      "Model saved in: models/GIN_best_model.pth\n",
      "Epoch: 030, Loss: 0.130, Test Loss: 2.783, Train Acc: 0.783 Test Acc: 0.519\n",
      "Epoch: 035, Loss: 0.030, Test Loss: 2.006, Train Acc: 0.935 Test Acc: 0.642\n",
      "Epoch: 040, Loss: 0.036, Test Loss: 2.352, Train Acc: 0.922 Test Acc: 0.568\n",
      "Epoch: 045, Loss: 0.033, Test Loss: 2.696, Train Acc: 0.904 Test Acc: 0.481\n",
      "Epoch: 050, Loss: 0.011, Test Loss: 2.502, Train Acc: 0.972 Test Acc: 0.543\n",
      "Epoch: 055, Loss: 0.001, Test Loss: 2.639, Train Acc: 0.977 Test Acc: 0.531\n",
      "Epoch: 060, Loss: 0.001, Test Loss: 2.714, Train Acc: 0.976 Test Acc: 0.531\n",
      "Epoch: 065, Loss: 0.000, Test Loss: 2.764, Train Acc: 0.977 Test Acc: 0.531\n",
      "Epoch: 070, Loss: 0.000, Test Loss: 2.796, Train Acc: 0.977 Test Acc: 0.531\n",
      "Epoch: 075, Loss: 0.000, Test Loss: 2.817, Train Acc: 0.977 Test Acc: 0.531\n",
      "Epoch: 080, Loss: 0.000, Test Loss: 2.830, Train Acc: 0.977 Test Acc: 0.531\n",
      "Epoch: 085, Loss: 0.000, Test Loss: 2.834, Train Acc: 0.978 Test Acc: 0.531\n",
      "Epoch: 090, Loss: 0.000, Test Loss: 2.831, Train Acc: 0.979 Test Acc: 0.543\n",
      "Epoch: 095, Loss: 0.000, Test Loss: 2.825, Train Acc: 0.979 Test Acc: 0.543\n",
      "Epoch: 100, Loss: 0.000, Test Loss: 2.813, Train Acc: 0.978 Test Acc: 0.543\n",
      "Epoch: 105, Loss: 0.000, Test Loss: 2.794, Train Acc: 0.979 Test Acc: 0.543\n",
      "Epoch: 110, Loss: 0.000, Test Loss: 2.775, Train Acc: 0.978 Test Acc: 0.543\n",
      "Epoch: 115, Loss: 0.000, Test Loss: 2.756, Train Acc: 0.979 Test Acc: 0.543\n",
      "Epoch: 120, Loss: 0.000, Test Loss: 2.745, Train Acc: 0.978 Test Acc: 0.556\n",
      "Epoch: 125, Loss: 0.459, Test Loss: 0.812, Train Acc: 0.775 Test Acc: 0.605\n",
      "Epoch: 130, Loss: 0.188, Test Loss: 1.501, Train Acc: 0.936 Test Acc: 0.568\n",
      "Epoch: 135, Loss: 0.106, Test Loss: 1.535, Train Acc: 0.935 Test Acc: 0.617\n",
      "Epoch: 140, Loss: 0.117, Test Loss: 1.392, Train Acc: 0.947 Test Acc: 0.605\n",
      "Epoch: 145, Loss: 0.032, Test Loss: 2.128, Train Acc: 0.967 Test Acc: 0.642\n",
      "Epoch: 150, Loss: 0.005, Test Loss: 2.232, Train Acc: 0.981 Test Acc: 0.605\n",
      "Epoch: 155, Loss: 0.002, Test Loss: 2.374, Train Acc: 0.981 Test Acc: 0.593\n",
      "Epoch: 160, Loss: 0.001, Test Loss: 2.490, Train Acc: 0.981 Test Acc: 0.593\n",
      "Epoch: 165, Loss: 0.001, Test Loss: 2.560, Train Acc: 0.982 Test Acc: 0.605\n",
      "Epoch: 170, Loss: 0.000, Test Loss: 2.610, Train Acc: 0.982 Test Acc: 0.605\n",
      "Epoch: 175, Loss: 0.000, Test Loss: 2.653, Train Acc: 0.983 Test Acc: 0.605\n",
      "Epoch: 180, Loss: 0.000, Test Loss: 2.686, Train Acc: 0.982 Test Acc: 0.605\n",
      "Epoch: 185, Loss: 0.000, Test Loss: 2.702, Train Acc: 0.983 Test Acc: 0.605\n",
      "Epoch: 190, Loss: 0.000, Test Loss: 2.747, Train Acc: 0.983 Test Acc: 0.605\n",
      "Epoch: 195, Loss: 0.000, Test Loss: 2.760, Train Acc: 0.984 Test Acc: 0.605\n",
      "Epoch: 200, Loss: 0.000, Test Loss: 2.804, Train Acc: 0.983 Test Acc: 0.605\n",
      "Epoch: 205, Loss: 0.212, Test Loss: 1.157, Train Acc: 0.926 Test Acc: 0.568\n",
      "Model saved in: models/GIN_best_model.pth\n",
      "Epoch: 210, Loss: 0.101, Test Loss: 1.648, Train Acc: 0.953 Test Acc: 0.605\n",
      "Model saved in: models/GIN_best_model.pth\n",
      "Epoch: 215, Loss: 0.040, Test Loss: 1.661, Train Acc: 0.978 Test Acc: 0.691\n",
      "Epoch: 220, Loss: 0.005, Test Loss: 1.996, Train Acc: 0.991 Test Acc: 0.605\n",
      "Epoch: 225, Loss: 0.002, Test Loss: 2.227, Train Acc: 0.988 Test Acc: 0.593\n",
      "Epoch: 230, Loss: 0.001, Test Loss: 2.372, Train Acc: 0.990 Test Acc: 0.580\n",
      "Epoch: 235, Loss: 0.001, Test Loss: 2.475, Train Acc: 0.990 Test Acc: 0.593\n",
      "Epoch: 240, Loss: 0.000, Test Loss: 2.551, Train Acc: 0.990 Test Acc: 0.593\n",
      "Epoch: 245, Loss: 0.000, Test Loss: 2.622, Train Acc: 0.991 Test Acc: 0.593\n",
      "Epoch: 250, Loss: 0.000, Test Loss: 2.673, Train Acc: 0.990 Test Acc: 0.593\n",
      "Epoch: 255, Loss: 0.000, Test Loss: 2.731, Train Acc: 0.991 Test Acc: 0.593\n",
      "Epoch: 260, Loss: 0.000, Test Loss: 2.785, Train Acc: 0.991 Test Acc: 0.605\n",
      "Epoch: 265, Loss: 0.000, Test Loss: 2.807, Train Acc: 0.991 Test Acc: 0.617\n",
      "Epoch: 270, Loss: 0.555, Test Loss: 0.853, Train Acc: 0.717 Test Acc: 0.568\n",
      "Epoch: 275, Loss: 0.073, Test Loss: 1.740, Train Acc: 0.970 Test Acc: 0.580\n",
      "Epoch: 280, Loss: 0.045, Test Loss: 1.960, Train Acc: 0.970 Test Acc: 0.580\n",
      "Epoch: 285, Loss: 0.073, Test Loss: 2.483, Train Acc: 0.875 Test Acc: 0.556\n",
      "Epoch: 290, Loss: 0.008, Test Loss: 2.497, Train Acc: 0.986 Test Acc: 0.531\n",
      "Epoch: 295, Loss: 0.002, Test Loss: 2.839, Train Acc: 0.990 Test Acc: 0.506\n",
      "Epoch: 300, Loss: 0.001, Test Loss: 2.953, Train Acc: 0.990 Test Acc: 0.494\n",
      "Epoch: 305, Loss: 0.001, Test Loss: 3.053, Train Acc: 0.990 Test Acc: 0.494\n",
      "Epoch: 310, Loss: 0.000, Test Loss: 3.125, Train Acc: 0.989 Test Acc: 0.494\n",
      "Epoch: 315, Loss: 0.000, Test Loss: 3.181, Train Acc: 0.989 Test Acc: 0.506\n",
      "Epoch: 320, Loss: 0.000, Test Loss: 3.232, Train Acc: 0.989 Test Acc: 0.506\n",
      "Epoch: 325, Loss: 0.000, Test Loss: 3.261, Train Acc: 0.990 Test Acc: 0.506\n",
      "Epoch: 330, Loss: 0.000, Test Loss: 3.278, Train Acc: 0.990 Test Acc: 0.506\n",
      "Epoch: 335, Loss: 0.000, Test Loss: 3.282, Train Acc: 0.990 Test Acc: 0.506\n",
      "Epoch: 340, Loss: 0.000, Test Loss: 3.236, Train Acc: 0.990 Test Acc: 0.506\n",
      "Epoch: 345, Loss: 0.339, Test Loss: 1.625, Train Acc: 0.914 Test Acc: 0.506\n",
      "Epoch: 350, Loss: 0.046, Test Loss: 1.911, Train Acc: 0.984 Test Acc: 0.556\n",
      "Epoch: 355, Loss: 0.003, Test Loss: 2.408, Train Acc: 0.993 Test Acc: 0.568\n",
      "Epoch: 360, Loss: 0.001, Test Loss: 2.604, Train Acc: 0.993 Test Acc: 0.568\n",
      "Epoch: 365, Loss: 0.001, Test Loss: 2.739, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 370, Loss: 0.001, Test Loss: 2.838, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 375, Loss: 0.000, Test Loss: 2.924, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 380, Loss: 0.000, Test Loss: 2.962, Train Acc: 0.992 Test Acc: 0.556\n",
      "Epoch: 385, Loss: 0.000, Test Loss: 2.995, Train Acc: 0.992 Test Acc: 0.556\n",
      "Epoch: 390, Loss: 0.000, Test Loss: 3.065, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 395, Loss: 0.000, Test Loss: 3.066, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 400, Loss: 0.000, Test Loss: 3.059, Train Acc: 0.991 Test Acc: 0.556\n"
     ]
    }
   ],
   "source": [
    "gnn.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/GINbis_best_model.pth\n",
      "Model saved in: models/GINbis_best_model.pth\n",
      "Epoch: 005, Loss: 0.414, Test Loss: 1.183, Train Acc: 0.811 Test Acc: 0.519\n",
      "Epoch: 010, Loss: 0.153, Test Loss: 1.728, Train Acc: 0.902 Test Acc: 0.556\n",
      "Model saved in: models/GINbis_best_model.pth\n",
      "Epoch: 015, Loss: 0.076, Test Loss: 1.805, Train Acc: 0.923 Test Acc: 0.494\n",
      "Epoch: 020, Loss: 0.039, Test Loss: 1.719, Train Acc: 0.947 Test Acc: 0.519\n",
      "Model saved in: models/GINbis_best_model.pth\n",
      "Epoch: 025, Loss: 0.007, Test Loss: 1.945, Train Acc: 0.974 Test Acc: 0.630\n",
      "Epoch: 030, Loss: 0.001, Test Loss: 2.092, Train Acc: 0.977 Test Acc: 0.580\n",
      "Epoch: 035, Loss: 0.001, Test Loss: 2.137, Train Acc: 0.980 Test Acc: 0.593\n",
      "Epoch: 040, Loss: 0.001, Test Loss: 2.169, Train Acc: 0.980 Test Acc: 0.593\n",
      "Epoch: 045, Loss: 0.001, Test Loss: 2.196, Train Acc: 0.980 Test Acc: 0.580\n",
      "Epoch: 050, Loss: 0.000, Test Loss: 2.216, Train Acc: 0.980 Test Acc: 0.593\n",
      "Epoch: 055, Loss: 0.000, Test Loss: 2.232, Train Acc: 0.979 Test Acc: 0.593\n",
      "Epoch: 060, Loss: 0.000, Test Loss: 2.245, Train Acc: 0.979 Test Acc: 0.580\n",
      "Epoch: 065, Loss: 0.000, Test Loss: 2.256, Train Acc: 0.979 Test Acc: 0.568\n",
      "Epoch: 070, Loss: 0.000, Test Loss: 2.259, Train Acc: 0.979 Test Acc: 0.568\n",
      "Epoch: 075, Loss: 0.000, Test Loss: 2.333, Train Acc: 0.978 Test Acc: 0.605\n",
      "Epoch: 080, Loss: 0.000, Test Loss: 2.419, Train Acc: 0.980 Test Acc: 0.605\n",
      "Epoch: 085, Loss: 0.000, Test Loss: 2.471, Train Acc: 0.980 Test Acc: 0.605\n",
      "Epoch: 090, Loss: 0.000, Test Loss: 2.435, Train Acc: 0.978 Test Acc: 0.593\n",
      "Epoch: 095, Loss: 0.000, Test Loss: 2.537, Train Acc: 0.978 Test Acc: 0.605\n",
      "Epoch: 100, Loss: 0.000, Test Loss: 2.495, Train Acc: 0.976 Test Acc: 0.605\n",
      "Epoch: 105, Loss: 0.000, Test Loss: 2.618, Train Acc: 0.974 Test Acc: 0.605\n",
      "Epoch: 110, Loss: 0.000, Test Loss: 2.398, Train Acc: 0.977 Test Acc: 0.593\n",
      "Epoch: 115, Loss: 0.000, Test Loss: 2.418, Train Acc: 0.979 Test Acc: 0.605\n",
      "Epoch: 120, Loss: 0.000, Test Loss: 2.389, Train Acc: 0.972 Test Acc: 0.605\n",
      "Epoch: 125, Loss: 0.000, Test Loss: 2.303, Train Acc: 0.974 Test Acc: 0.580\n",
      "Epoch: 130, Loss: 0.000, Test Loss: 2.408, Train Acc: 0.979 Test Acc: 0.605\n",
      "Model saved in: models/GINbis_best_model.pth\n",
      "Epoch: 135, Loss: 0.000, Test Loss: 2.189, Train Acc: 0.982 Test Acc: 0.642\n",
      "Model saved in: models/GINbis_best_model.pth\n",
      "Epoch: 140, Loss: 0.000, Test Loss: 2.152, Train Acc: 0.982 Test Acc: 0.667\n",
      "Epoch: 145, Loss: 0.000, Test Loss: 2.530, Train Acc: 0.957 Test Acc: 0.630\n",
      "Epoch: 150, Loss: 0.689, Test Loss: 0.719, Train Acc: 0.596 Test Acc: 0.519\n",
      "Epoch: 155, Loss: 0.493, Test Loss: 1.002, Train Acc: 0.747 Test Acc: 0.506\n",
      "Epoch: 160, Loss: 0.305, Test Loss: 1.179, Train Acc: 0.825 Test Acc: 0.531\n",
      "Epoch: 165, Loss: 0.123, Test Loss: 1.548, Train Acc: 0.909 Test Acc: 0.556\n",
      "Epoch: 170, Loss: 0.111, Test Loss: 1.746, Train Acc: 0.903 Test Acc: 0.593\n",
      "Epoch: 175, Loss: 0.065, Test Loss: 2.135, Train Acc: 0.939 Test Acc: 0.580\n",
      "Epoch: 180, Loss: 0.012, Test Loss: 2.310, Train Acc: 0.968 Test Acc: 0.580\n",
      "Epoch: 185, Loss: 0.022, Test Loss: 2.824, Train Acc: 0.923 Test Acc: 0.593\n",
      "Epoch: 190, Loss: 0.042, Test Loss: 2.592, Train Acc: 0.956 Test Acc: 0.543\n",
      "Epoch: 195, Loss: 0.017, Test Loss: 2.394, Train Acc: 0.951 Test Acc: 0.605\n",
      "Epoch: 200, Loss: 0.002, Test Loss: 2.994, Train Acc: 0.967 Test Acc: 0.580\n",
      "Epoch: 205, Loss: 0.001, Test Loss: 3.148, Train Acc: 0.969 Test Acc: 0.593\n",
      "Epoch: 210, Loss: 0.001, Test Loss: 3.268, Train Acc: 0.970 Test Acc: 0.593\n",
      "Epoch: 215, Loss: 0.001, Test Loss: 3.364, Train Acc: 0.971 Test Acc: 0.593\n",
      "Epoch: 220, Loss: 0.000, Test Loss: 3.444, Train Acc: 0.970 Test Acc: 0.580\n",
      "Epoch: 225, Loss: 0.000, Test Loss: 3.510, Train Acc: 0.971 Test Acc: 0.580\n",
      "Epoch: 230, Loss: 0.000, Test Loss: 3.567, Train Acc: 0.972 Test Acc: 0.580\n",
      "Epoch: 235, Loss: 0.000, Test Loss: 3.613, Train Acc: 0.972 Test Acc: 0.580\n",
      "Epoch: 240, Loss: 0.000, Test Loss: 3.650, Train Acc: 0.972 Test Acc: 0.568\n",
      "Epoch: 245, Loss: 0.000, Test Loss: 3.680, Train Acc: 0.972 Test Acc: 0.568\n",
      "Epoch: 250, Loss: 0.000, Test Loss: 3.709, Train Acc: 0.971 Test Acc: 0.568\n",
      "Epoch: 255, Loss: 0.000, Test Loss: 3.734, Train Acc: 0.972 Test Acc: 0.568\n",
      "Epoch: 260, Loss: 0.000, Test Loss: 3.752, Train Acc: 0.970 Test Acc: 0.568\n",
      "Epoch: 265, Loss: 0.000, Test Loss: 3.766, Train Acc: 0.971 Test Acc: 0.568\n",
      "Epoch: 270, Loss: 0.000, Test Loss: 3.775, Train Acc: 0.971 Test Acc: 0.568\n",
      "Epoch: 275, Loss: 0.000, Test Loss: 3.779, Train Acc: 0.970 Test Acc: 0.568\n",
      "Epoch: 280, Loss: 0.000, Test Loss: 3.776, Train Acc: 0.970 Test Acc: 0.568\n",
      "Epoch: 285, Loss: 0.000, Test Loss: 3.771, Train Acc: 0.970 Test Acc: 0.568\n",
      "Epoch: 290, Loss: 0.000, Test Loss: 3.757, Train Acc: 0.970 Test Acc: 0.568\n",
      "Epoch: 295, Loss: 0.000, Test Loss: 3.739, Train Acc: 0.970 Test Acc: 0.568\n",
      "Epoch: 300, Loss: 0.000, Test Loss: 3.725, Train Acc: 0.971 Test Acc: 0.568\n",
      "Epoch: 305, Loss: 0.000, Test Loss: 3.701, Train Acc: 0.970 Test Acc: 0.580\n",
      "Epoch: 310, Loss: 0.000, Test Loss: 3.671, Train Acc: 0.971 Test Acc: 0.568\n",
      "Epoch: 315, Loss: 0.000, Test Loss: 3.655, Train Acc: 0.970 Test Acc: 0.568\n",
      "Epoch: 320, Loss: 0.453, Test Loss: 0.885, Train Acc: 0.615 Test Acc: 0.444\n",
      "Epoch: 325, Loss: 0.336, Test Loss: 1.185, Train Acc: 0.819 Test Acc: 0.531\n",
      "Epoch: 330, Loss: 0.127, Test Loss: 1.823, Train Acc: 0.926 Test Acc: 0.593\n",
      "Epoch: 335, Loss: 0.043, Test Loss: 2.199, Train Acc: 0.951 Test Acc: 0.519\n",
      "Epoch: 340, Loss: 0.076, Test Loss: 2.089, Train Acc: 0.958 Test Acc: 0.494\n",
      "Epoch: 345, Loss: 0.003, Test Loss: 2.381, Train Acc: 0.974 Test Acc: 0.568\n",
      "Epoch: 350, Loss: 0.001, Test Loss: 2.532, Train Acc: 0.976 Test Acc: 0.568\n",
      "Epoch: 355, Loss: 0.001, Test Loss: 2.652, Train Acc: 0.976 Test Acc: 0.543\n",
      "Epoch: 360, Loss: 0.001, Test Loss: 2.738, Train Acc: 0.977 Test Acc: 0.543\n",
      "Epoch: 365, Loss: 0.000, Test Loss: 2.809, Train Acc: 0.977 Test Acc: 0.543\n",
      "Epoch: 370, Loss: 0.000, Test Loss: 2.866, Train Acc: 0.978 Test Acc: 0.543\n",
      "Epoch: 375, Loss: 0.000, Test Loss: 2.916, Train Acc: 0.978 Test Acc: 0.543\n",
      "Epoch: 380, Loss: 0.000, Test Loss: 2.958, Train Acc: 0.977 Test Acc: 0.543\n",
      "Epoch: 385, Loss: 0.000, Test Loss: 2.996, Train Acc: 0.977 Test Acc: 0.556\n",
      "Epoch: 390, Loss: 0.000, Test Loss: 3.030, Train Acc: 0.978 Test Acc: 0.568\n",
      "Epoch: 395, Loss: 0.000, Test Loss: 3.060, Train Acc: 0.978 Test Acc: 0.568\n",
      "Epoch: 400, Loss: 0.000, Test Loss: 3.087, Train Acc: 0.978 Test Acc: 0.568\n",
      "Epoch: 405, Loss: 0.000, Test Loss: 3.111, Train Acc: 0.978 Test Acc: 0.556\n",
      "Epoch: 410, Loss: 0.000, Test Loss: 3.130, Train Acc: 0.978 Test Acc: 0.556\n",
      "Epoch: 415, Loss: 0.000, Test Loss: 3.145, Train Acc: 0.978 Test Acc: 0.543\n",
      "Epoch: 420, Loss: 0.000, Test Loss: 3.155, Train Acc: 0.979 Test Acc: 0.543\n",
      "Epoch: 425, Loss: 0.000, Test Loss: 3.159, Train Acc: 0.980 Test Acc: 0.531\n",
      "Epoch: 430, Loss: 0.000, Test Loss: 3.163, Train Acc: 0.980 Test Acc: 0.531\n",
      "Epoch: 435, Loss: 0.000, Test Loss: 3.163, Train Acc: 0.979 Test Acc: 0.531\n",
      "Epoch: 440, Loss: 0.000, Test Loss: 3.163, Train Acc: 0.979 Test Acc: 0.531\n",
      "Epoch: 445, Loss: 0.000, Test Loss: 3.201, Train Acc: 0.978 Test Acc: 0.543\n",
      "Epoch: 450, Loss: 0.000, Test Loss: 3.220, Train Acc: 0.979 Test Acc: 0.556\n",
      "Epoch: 455, Loss: 0.000, Test Loss: 3.184, Train Acc: 0.980 Test Acc: 0.556\n",
      "Epoch: 460, Loss: 0.301, Test Loss: 1.190, Train Acc: 0.857 Test Acc: 0.519\n",
      "Epoch: 465, Loss: 0.075, Test Loss: 1.630, Train Acc: 0.942 Test Acc: 0.519\n",
      "Epoch: 470, Loss: 0.046, Test Loss: 1.974, Train Acc: 0.939 Test Acc: 0.568\n",
      "Epoch: 475, Loss: 0.033, Test Loss: 2.076, Train Acc: 0.970 Test Acc: 0.543\n",
      "Epoch: 480, Loss: 0.001, Test Loss: 2.248, Train Acc: 0.982 Test Acc: 0.543\n",
      "Epoch: 485, Loss: 0.001, Test Loss: 2.315, Train Acc: 0.984 Test Acc: 0.543\n",
      "Epoch: 490, Loss: 0.001, Test Loss: 2.383, Train Acc: 0.985 Test Acc: 0.543\n",
      "Epoch: 495, Loss: 0.000, Test Loss: 2.423, Train Acc: 0.984 Test Acc: 0.543\n",
      "Epoch: 500, Loss: 0.000, Test Loss: 2.456, Train Acc: 0.985 Test Acc: 0.543\n",
      "Epoch: 505, Loss: 0.000, Test Loss: 2.492, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 510, Loss: 0.000, Test Loss: 2.521, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 515, Loss: 0.000, Test Loss: 2.546, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 520, Loss: 0.000, Test Loss: 2.559, Train Acc: 0.987 Test Acc: 0.556\n",
      "Epoch: 525, Loss: 0.000, Test Loss: 2.569, Train Acc: 0.988 Test Acc: 0.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 530, Loss: 0.000, Test Loss: 2.578, Train Acc: 0.988 Test Acc: 0.556\n",
      "Epoch: 535, Loss: 0.000, Test Loss: 2.585, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 540, Loss: 0.000, Test Loss: 2.590, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 545, Loss: 0.000, Test Loss: 2.591, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 550, Loss: 0.000, Test Loss: 2.589, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 555, Loss: 0.000, Test Loss: 2.594, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 560, Loss: 0.000, Test Loss: 2.595, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 565, Loss: 0.000, Test Loss: 2.578, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 570, Loss: 0.000, Test Loss: 2.561, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 575, Loss: 0.000, Test Loss: 2.541, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 580, Loss: 0.000, Test Loss: 2.520, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 585, Loss: 0.000, Test Loss: 2.503, Train Acc: 0.988 Test Acc: 0.556\n",
      "Epoch: 590, Loss: 0.000, Test Loss: 2.490, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 595, Loss: 0.000, Test Loss: 2.486, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 600, Loss: 0.665, Test Loss: 0.861, Train Acc: 0.745 Test Acc: 0.568\n",
      "Epoch: 605, Loss: 0.097, Test Loss: 1.469, Train Acc: 0.947 Test Acc: 0.580\n",
      "Epoch: 610, Loss: 0.023, Test Loss: 1.660, Train Acc: 0.969 Test Acc: 0.556\n",
      "Epoch: 615, Loss: 0.007, Test Loss: 1.876, Train Acc: 0.984 Test Acc: 0.580\n",
      "Epoch: 620, Loss: 0.004, Test Loss: 1.904, Train Acc: 0.985 Test Acc: 0.593\n",
      "Epoch: 625, Loss: 0.001, Test Loss: 1.999, Train Acc: 0.985 Test Acc: 0.556\n",
      "Epoch: 630, Loss: 0.001, Test Loss: 2.047, Train Acc: 0.987 Test Acc: 0.556\n",
      "Epoch: 635, Loss: 0.001, Test Loss: 2.080, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 640, Loss: 0.001, Test Loss: 2.108, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 645, Loss: 0.000, Test Loss: 2.133, Train Acc: 0.987 Test Acc: 0.531\n",
      "Epoch: 650, Loss: 0.000, Test Loss: 2.157, Train Acc: 0.986 Test Acc: 0.531\n",
      "Epoch: 655, Loss: 0.000, Test Loss: 2.186, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 660, Loss: 0.000, Test Loss: 2.201, Train Acc: 0.985 Test Acc: 0.543\n",
      "Epoch: 665, Loss: 0.000, Test Loss: 2.222, Train Acc: 0.985 Test Acc: 0.543\n",
      "Epoch: 670, Loss: 0.000, Test Loss: 2.239, Train Acc: 0.984 Test Acc: 0.556\n",
      "Epoch: 675, Loss: 0.000, Test Loss: 2.254, Train Acc: 0.985 Test Acc: 0.556\n",
      "Epoch: 680, Loss: 0.000, Test Loss: 2.262, Train Acc: 0.985 Test Acc: 0.556\n",
      "Epoch: 685, Loss: 0.000, Test Loss: 2.264, Train Acc: 0.985 Test Acc: 0.556\n",
      "Epoch: 690, Loss: 0.000, Test Loss: 2.267, Train Acc: 0.984 Test Acc: 0.556\n",
      "Epoch: 695, Loss: 0.000, Test Loss: 2.273, Train Acc: 0.984 Test Acc: 0.556\n",
      "Epoch: 700, Loss: 0.000, Test Loss: 2.292, Train Acc: 0.984 Test Acc: 0.568\n",
      "Epoch: 705, Loss: 0.000, Test Loss: 2.297, Train Acc: 0.984 Test Acc: 0.568\n",
      "Epoch: 710, Loss: 0.000, Test Loss: 2.281, Train Acc: 0.983 Test Acc: 0.568\n",
      "Epoch: 715, Loss: 0.000, Test Loss: 2.307, Train Acc: 0.985 Test Acc: 0.580\n",
      "Epoch: 720, Loss: 0.000, Test Loss: 2.313, Train Acc: 0.985 Test Acc: 0.556\n",
      "Epoch: 725, Loss: 0.000, Test Loss: 2.294, Train Acc: 0.983 Test Acc: 0.580\n",
      "Epoch: 730, Loss: 0.123, Test Loss: 1.573, Train Acc: 0.949 Test Acc: 0.531\n",
      "Epoch: 735, Loss: 0.084, Test Loss: 2.059, Train Acc: 0.940 Test Acc: 0.580\n",
      "Epoch: 740, Loss: 0.033, Test Loss: 2.004, Train Acc: 0.964 Test Acc: 0.531\n",
      "Epoch: 745, Loss: 0.010, Test Loss: 2.282, Train Acc: 0.970 Test Acc: 0.543\n",
      "Epoch: 750, Loss: 0.001, Test Loss: 2.451, Train Acc: 0.982 Test Acc: 0.506\n",
      "Epoch: 755, Loss: 0.001, Test Loss: 2.475, Train Acc: 0.984 Test Acc: 0.506\n",
      "Epoch: 760, Loss: 0.001, Test Loss: 2.506, Train Acc: 0.983 Test Acc: 0.506\n",
      "Epoch: 765, Loss: 0.000, Test Loss: 2.538, Train Acc: 0.984 Test Acc: 0.519\n",
      "Epoch: 770, Loss: 0.000, Test Loss: 2.570, Train Acc: 0.983 Test Acc: 0.519\n",
      "Epoch: 775, Loss: 0.000, Test Loss: 2.599, Train Acc: 0.982 Test Acc: 0.519\n",
      "Epoch: 780, Loss: 0.000, Test Loss: 2.626, Train Acc: 0.982 Test Acc: 0.519\n",
      "Epoch: 785, Loss: 0.000, Test Loss: 2.650, Train Acc: 0.982 Test Acc: 0.519\n",
      "Epoch: 790, Loss: 0.000, Test Loss: 2.673, Train Acc: 0.983 Test Acc: 0.519\n",
      "Epoch: 795, Loss: 0.000, Test Loss: 2.692, Train Acc: 0.984 Test Acc: 0.519\n",
      "Epoch: 800, Loss: 0.000, Test Loss: 2.709, Train Acc: 0.984 Test Acc: 0.519\n",
      "Epoch: 805, Loss: 0.000, Test Loss: 2.720, Train Acc: 0.984 Test Acc: 0.519\n",
      "Epoch: 810, Loss: 0.000, Test Loss: 2.728, Train Acc: 0.984 Test Acc: 0.506\n",
      "Epoch: 815, Loss: 0.000, Test Loss: 2.730, Train Acc: 0.986 Test Acc: 0.506\n",
      "Epoch: 820, Loss: 0.000, Test Loss: 2.735, Train Acc: 0.986 Test Acc: 0.506\n",
      "Epoch: 825, Loss: 0.000, Test Loss: 2.734, Train Acc: 0.986 Test Acc: 0.506\n",
      "Epoch: 830, Loss: 0.000, Test Loss: 2.729, Train Acc: 0.986 Test Acc: 0.506\n",
      "Epoch: 835, Loss: 0.000, Test Loss: 2.728, Train Acc: 0.985 Test Acc: 0.506\n",
      "Epoch: 840, Loss: 0.000, Test Loss: 2.717, Train Acc: 0.983 Test Acc: 0.506\n",
      "Epoch: 845, Loss: 0.000, Test Loss: 2.706, Train Acc: 0.984 Test Acc: 0.506\n",
      "Epoch: 850, Loss: 0.000, Test Loss: 2.692, Train Acc: 0.983 Test Acc: 0.506\n",
      "Epoch: 855, Loss: 0.000, Test Loss: 2.689, Train Acc: 0.982 Test Acc: 0.506\n",
      "Epoch: 860, Loss: 0.000, Test Loss: 2.685, Train Acc: 0.982 Test Acc: 0.506\n",
      "Epoch: 865, Loss: 0.000, Test Loss: 2.672, Train Acc: 0.983 Test Acc: 0.506\n",
      "Epoch: 870, Loss: 0.000, Test Loss: 2.646, Train Acc: 0.982 Test Acc: 0.519\n",
      "Epoch: 875, Loss: 0.452, Test Loss: 0.904, Train Acc: 0.813 Test Acc: 0.568\n",
      "Epoch: 880, Loss: 0.042, Test Loss: 1.643, Train Acc: 0.966 Test Acc: 0.543\n",
      "Epoch: 885, Loss: 0.004, Test Loss: 1.889, Train Acc: 0.983 Test Acc: 0.531\n",
      "Epoch: 890, Loss: 0.002, Test Loss: 1.984, Train Acc: 0.984 Test Acc: 0.531\n",
      "Epoch: 895, Loss: 0.001, Test Loss: 2.068, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 900, Loss: 0.001, Test Loss: 2.143, Train Acc: 0.987 Test Acc: 0.556\n",
      "Epoch: 905, Loss: 0.001, Test Loss: 2.201, Train Acc: 0.987 Test Acc: 0.556\n",
      "Epoch: 910, Loss: 0.001, Test Loss: 2.255, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 915, Loss: 0.000, Test Loss: 2.257, Train Acc: 0.985 Test Acc: 0.568\n",
      "Epoch: 920, Loss: 0.000, Test Loss: 2.364, Train Acc: 0.985 Test Acc: 0.568\n",
      "Epoch: 925, Loss: 0.000, Test Loss: 2.415, Train Acc: 0.985 Test Acc: 0.556\n",
      "Epoch: 930, Loss: 0.000, Test Loss: 2.455, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 935, Loss: 0.000, Test Loss: 2.496, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 940, Loss: 0.000, Test Loss: 2.530, Train Acc: 0.986 Test Acc: 0.531\n",
      "Epoch: 945, Loss: 0.000, Test Loss: 2.565, Train Acc: 0.986 Test Acc: 0.531\n",
      "Epoch: 950, Loss: 0.000, Test Loss: 2.596, Train Acc: 0.984 Test Acc: 0.543\n",
      "Epoch: 955, Loss: 0.000, Test Loss: 2.617, Train Acc: 0.984 Test Acc: 0.556\n",
      "Epoch: 960, Loss: 0.000, Test Loss: 2.649, Train Acc: 0.983 Test Acc: 0.556\n",
      "Epoch: 965, Loss: 0.000, Test Loss: 2.663, Train Acc: 0.982 Test Acc: 0.556\n",
      "Epoch: 970, Loss: 0.000, Test Loss: 2.682, Train Acc: 0.981 Test Acc: 0.556\n",
      "Epoch: 975, Loss: 0.000, Test Loss: 2.698, Train Acc: 0.980 Test Acc: 0.556\n",
      "Epoch: 980, Loss: 0.000, Test Loss: 2.728, Train Acc: 0.980 Test Acc: 0.556\n",
      "Epoch: 985, Loss: 0.000, Test Loss: 2.714, Train Acc: 0.980 Test Acc: 0.556\n",
      "Epoch: 990, Loss: 0.000, Test Loss: 2.720, Train Acc: 0.981 Test Acc: 0.556\n",
      "Epoch: 995, Loss: 0.000, Test Loss: 2.733, Train Acc: 0.981 Test Acc: 0.519\n",
      "Epoch: 1000, Loss: 0.000, Test Loss: 2.753, Train Acc: 0.981 Test Acc: 0.531\n",
      "Epoch: 1005, Loss: 0.000, Test Loss: 2.745, Train Acc: 0.980 Test Acc: 0.506\n",
      "Epoch: 1010, Loss: 0.000, Test Loss: 2.752, Train Acc: 0.980 Test Acc: 0.519\n",
      "Epoch: 1015, Loss: 0.000, Test Loss: 2.694, Train Acc: 0.976 Test Acc: 0.481\n",
      "Epoch: 1020, Loss: 0.266, Test Loss: 1.295, Train Acc: 0.892 Test Acc: 0.556\n",
      "Epoch: 1025, Loss: 0.037, Test Loss: 1.437, Train Acc: 0.968 Test Acc: 0.580\n",
      "Epoch: 1030, Loss: 0.004, Test Loss: 1.821, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 1035, Loss: 0.002, Test Loss: 1.941, Train Acc: 0.987 Test Acc: 0.556\n",
      "Epoch: 1040, Loss: 0.001, Test Loss: 2.025, Train Acc: 0.986 Test Acc: 0.556\n",
      "Epoch: 1045, Loss: 0.001, Test Loss: 2.098, Train Acc: 0.986 Test Acc: 0.568\n",
      "Epoch: 1050, Loss: 0.001, Test Loss: 2.162, Train Acc: 0.986 Test Acc: 0.580\n",
      "Epoch: 1055, Loss: 0.001, Test Loss: 2.221, Train Acc: 0.987 Test Acc: 0.568\n",
      "Epoch: 1060, Loss: 0.000, Test Loss: 2.274, Train Acc: 0.988 Test Acc: 0.568\n",
      "Epoch: 1065, Loss: 0.000, Test Loss: 2.319, Train Acc: 0.988 Test Acc: 0.568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1070, Loss: 0.000, Test Loss: 2.363, Train Acc: 0.988 Test Acc: 0.568\n",
      "Epoch: 1075, Loss: 0.000, Test Loss: 2.401, Train Acc: 0.988 Test Acc: 0.556\n",
      "Epoch: 1080, Loss: 0.000, Test Loss: 2.443, Train Acc: 0.987 Test Acc: 0.556\n",
      "Epoch: 1085, Loss: 0.000, Test Loss: 2.472, Train Acc: 0.987 Test Acc: 0.556\n",
      "Epoch: 1090, Loss: 0.000, Test Loss: 2.506, Train Acc: 0.988 Test Acc: 0.556\n",
      "Epoch: 1095, Loss: 0.000, Test Loss: 2.515, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 1100, Loss: 0.000, Test Loss: 2.568, Train Acc: 0.987 Test Acc: 0.543\n",
      "Epoch: 1105, Loss: 0.000, Test Loss: 2.587, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 1110, Loss: 0.000, Test Loss: 2.595, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 1115, Loss: 0.000, Test Loss: 2.612, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 1120, Loss: 0.000, Test Loss: 2.625, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 1125, Loss: 0.000, Test Loss: 2.629, Train Acc: 0.987 Test Acc: 0.531\n",
      "Epoch: 1130, Loss: 0.000, Test Loss: 2.635, Train Acc: 0.987 Test Acc: 0.531\n",
      "Epoch: 1135, Loss: 0.000, Test Loss: 2.634, Train Acc: 0.986 Test Acc: 0.519\n",
      "Epoch: 1140, Loss: 0.000, Test Loss: 2.643, Train Acc: 0.985 Test Acc: 0.506\n",
      "Epoch: 1145, Loss: 0.605, Test Loss: 0.761, Train Acc: 0.630 Test Acc: 0.580\n",
      "Epoch: 1150, Loss: 0.148, Test Loss: 1.134, Train Acc: 0.910 Test Acc: 0.568\n",
      "Epoch: 1155, Loss: 0.044, Test Loss: 1.441, Train Acc: 0.972 Test Acc: 0.593\n",
      "Epoch: 1160, Loss: 0.006, Test Loss: 1.691, Train Acc: 0.990 Test Acc: 0.556\n",
      "Epoch: 1165, Loss: 0.003, Test Loss: 1.811, Train Acc: 0.991 Test Acc: 0.531\n",
      "Epoch: 1170, Loss: 0.002, Test Loss: 1.949, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 1175, Loss: 0.001, Test Loss: 2.049, Train Acc: 0.991 Test Acc: 0.543\n",
      "Epoch: 1180, Loss: 0.001, Test Loss: 2.163, Train Acc: 0.991 Test Acc: 0.543\n",
      "Epoch: 1185, Loss: 0.000, Test Loss: 2.251, Train Acc: 0.991 Test Acc: 0.531\n",
      "Epoch: 1190, Loss: 0.000, Test Loss: 2.321, Train Acc: 0.991 Test Acc: 0.531\n",
      "Epoch: 1195, Loss: 0.000, Test Loss: 2.372, Train Acc: 0.991 Test Acc: 0.543\n",
      "Epoch: 1200, Loss: 0.000, Test Loss: 2.415, Train Acc: 0.991 Test Acc: 0.543\n",
      "Epoch: 1205, Loss: 0.000, Test Loss: 2.446, Train Acc: 0.991 Test Acc: 0.543\n",
      "Epoch: 1210, Loss: 0.000, Test Loss: 2.476, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 1215, Loss: 0.000, Test Loss: 2.494, Train Acc: 0.991 Test Acc: 0.543\n",
      "Epoch: 1220, Loss: 0.000, Test Loss: 2.515, Train Acc: 0.991 Test Acc: 0.531\n",
      "Epoch: 1225, Loss: 0.000, Test Loss: 2.523, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 1230, Loss: 0.000, Test Loss: 2.537, Train Acc: 0.992 Test Acc: 0.556\n",
      "Epoch: 1235, Loss: 0.000, Test Loss: 2.553, Train Acc: 0.993 Test Acc: 0.543\n",
      "Epoch: 1240, Loss: 0.000, Test Loss: 2.558, Train Acc: 0.993 Test Acc: 0.543\n",
      "Epoch: 1245, Loss: 0.000, Test Loss: 2.588, Train Acc: 0.993 Test Acc: 0.531\n",
      "Epoch: 1250, Loss: 0.000, Test Loss: 2.605, Train Acc: 0.992 Test Acc: 0.531\n",
      "Epoch: 1255, Loss: 0.000, Test Loss: 2.608, Train Acc: 0.992 Test Acc: 0.531\n",
      "Epoch: 1260, Loss: 0.000, Test Loss: 2.635, Train Acc: 0.991 Test Acc: 0.519\n",
      "Epoch: 1265, Loss: 0.554, Test Loss: 0.787, Train Acc: 0.743 Test Acc: 0.617\n",
      "Epoch: 1270, Loss: 0.093, Test Loss: 1.344, Train Acc: 0.924 Test Acc: 0.630\n",
      "Epoch: 1275, Loss: 0.077, Test Loss: 1.902, Train Acc: 0.947 Test Acc: 0.531\n",
      "Epoch: 1280, Loss: 0.011, Test Loss: 2.082, Train Acc: 0.984 Test Acc: 0.556\n",
      "Epoch: 1285, Loss: 0.002, Test Loss: 2.348, Train Acc: 0.987 Test Acc: 0.531\n",
      "Epoch: 1290, Loss: 0.001, Test Loss: 2.486, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 1295, Loss: 0.001, Test Loss: 2.591, Train Acc: 0.986 Test Acc: 0.531\n",
      "Epoch: 1300, Loss: 0.001, Test Loss: 2.712, Train Acc: 0.987 Test Acc: 0.531\n"
     ]
    }
   ],
   "source": [
    "# gnnbis.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/GINtri_best_model.pth\n",
      "Epoch: 005, Loss: 0.656, Test Loss: 0.705, Train Acc: 0.636 Test Acc: 0.543\n",
      "Epoch: 010, Loss: 0.573, Test Loss: 0.810, Train Acc: 0.704 Test Acc: 0.506\n",
      "Epoch: 015, Loss: 0.457, Test Loss: 1.003, Train Acc: 0.807 Test Acc: 0.580\n",
      "Model saved in: models/GINtri_best_model.pth\n",
      "Epoch: 020, Loss: 0.317, Test Loss: 0.935, Train Acc: 0.901 Test Acc: 0.593\n",
      "Epoch: 025, Loss: 0.220, Test Loss: 1.112, Train Acc: 0.937 Test Acc: 0.481\n",
      "Epoch: 030, Loss: 0.160, Test Loss: 1.107, Train Acc: 0.955 Test Acc: 0.556\n",
      "Epoch: 035, Loss: 0.110, Test Loss: 1.302, Train Acc: 0.962 Test Acc: 0.556\n",
      "Epoch: 040, Loss: 0.088, Test Loss: 1.475, Train Acc: 0.970 Test Acc: 0.506\n",
      "Epoch: 045, Loss: 0.071, Test Loss: 1.751, Train Acc: 0.961 Test Acc: 0.494\n",
      "Epoch: 050, Loss: 0.047, Test Loss: 1.450, Train Acc: 0.982 Test Acc: 0.593\n",
      "Epoch: 055, Loss: 0.048, Test Loss: 1.574, Train Acc: 0.970 Test Acc: 0.568\n",
      "Model saved in: models/GINtri_best_model.pth\n",
      "Epoch: 060, Loss: 0.042, Test Loss: 1.631, Train Acc: 0.980 Test Acc: 0.617\n",
      "Epoch: 065, Loss: 0.046, Test Loss: 1.641, Train Acc: 0.970 Test Acc: 0.580\n",
      "Epoch: 070, Loss: 0.038, Test Loss: 1.719, Train Acc: 0.963 Test Acc: 0.593\n",
      "Epoch: 075, Loss: 0.025, Test Loss: 1.845, Train Acc: 0.985 Test Acc: 0.580\n",
      "Epoch: 080, Loss: 0.029, Test Loss: 1.921, Train Acc: 0.979 Test Acc: 0.519\n",
      "Epoch: 085, Loss: 0.031, Test Loss: 1.889, Train Acc: 0.990 Test Acc: 0.556\n",
      "Epoch: 090, Loss: 0.037, Test Loss: 1.831, Train Acc: 0.984 Test Acc: 0.556\n",
      "Epoch: 095, Loss: 0.020, Test Loss: 2.032, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 100, Loss: 0.014, Test Loss: 2.427, Train Acc: 0.986 Test Acc: 0.506\n",
      "Epoch: 105, Loss: 0.016, Test Loss: 2.157, Train Acc: 0.987 Test Acc: 0.519\n",
      "Epoch: 110, Loss: 0.021, Test Loss: 2.111, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 115, Loss: 0.037, Test Loss: 2.101, Train Acc: 0.990 Test Acc: 0.519\n",
      "Epoch: 120, Loss: 0.017, Test Loss: 2.067, Train Acc: 0.988 Test Acc: 0.580\n",
      "Epoch: 125, Loss: 0.014, Test Loss: 1.959, Train Acc: 0.983 Test Acc: 0.580\n",
      "Epoch: 130, Loss: 0.017, Test Loss: 2.500, Train Acc: 0.980 Test Acc: 0.531\n",
      "Epoch: 135, Loss: 0.009, Test Loss: 2.090, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 140, Loss: 0.013, Test Loss: 2.203, Train Acc: 0.993 Test Acc: 0.580\n",
      "Epoch: 145, Loss: 0.008, Test Loss: 2.257, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 150, Loss: 0.025, Test Loss: 2.395, Train Acc: 0.982 Test Acc: 0.543\n",
      "Epoch: 155, Loss: 0.017, Test Loss: 2.490, Train Acc: 0.990 Test Acc: 0.519\n",
      "Epoch: 160, Loss: 0.020, Test Loss: 2.348, Train Acc: 0.983 Test Acc: 0.556\n",
      "Epoch: 165, Loss: 0.011, Test Loss: 1.902, Train Acc: 0.991 Test Acc: 0.531\n",
      "Epoch: 170, Loss: 0.020, Test Loss: 2.397, Train Acc: 0.991 Test Acc: 0.481\n",
      "Epoch: 175, Loss: 0.025, Test Loss: 2.457, Train Acc: 0.993 Test Acc: 0.506\n",
      "Epoch: 180, Loss: 0.009, Test Loss: 2.121, Train Acc: 0.993 Test Acc: 0.617\n",
      "Epoch: 185, Loss: 0.004, Test Loss: 2.404, Train Acc: 0.995 Test Acc: 0.556\n",
      "Epoch: 190, Loss: 0.002, Test Loss: 2.515, Train Acc: 0.996 Test Acc: 0.543\n",
      "Epoch: 195, Loss: 0.024, Test Loss: 2.705, Train Acc: 0.979 Test Acc: 0.568\n",
      "Epoch: 200, Loss: 0.016, Test Loss: 2.821, Train Acc: 0.986 Test Acc: 0.506\n",
      "Epoch: 205, Loss: 0.007, Test Loss: 2.816, Train Acc: 0.995 Test Acc: 0.568\n",
      "Epoch: 210, Loss: 0.004, Test Loss: 2.662, Train Acc: 0.992 Test Acc: 0.506\n",
      "Epoch: 215, Loss: 0.022, Test Loss: 2.686, Train Acc: 0.985 Test Acc: 0.481\n",
      "Epoch: 220, Loss: 0.015, Test Loss: 2.835, Train Acc: 0.991 Test Acc: 0.481\n",
      "Epoch: 225, Loss: 0.006, Test Loss: 2.448, Train Acc: 0.994 Test Acc: 0.556\n",
      "Epoch: 230, Loss: 0.004, Test Loss: 2.674, Train Acc: 0.996 Test Acc: 0.543\n",
      "Epoch: 235, Loss: 0.001, Test Loss: 2.733, Train Acc: 0.997 Test Acc: 0.556\n",
      "Model saved in: models/GINtri_best_model.pth\n",
      "Epoch: 240, Loss: 0.014, Test Loss: 2.125, Train Acc: 0.984 Test Acc: 0.630\n",
      "Epoch: 245, Loss: 0.008, Test Loss: 2.601, Train Acc: 0.992 Test Acc: 0.593\n",
      "Epoch: 250, Loss: 0.002, Test Loss: 2.764, Train Acc: 0.995 Test Acc: 0.568\n",
      "Epoch: 255, Loss: 0.003, Test Loss: 2.530, Train Acc: 0.994 Test Acc: 0.605\n",
      "Epoch: 260, Loss: 0.002, Test Loss: 2.641, Train Acc: 0.992 Test Acc: 0.531\n",
      "Epoch: 265, Loss: 0.003, Test Loss: 2.622, Train Acc: 0.994 Test Acc: 0.568\n",
      "Epoch: 270, Loss: 0.002, Test Loss: 2.792, Train Acc: 0.995 Test Acc: 0.556\n",
      "Epoch: 275, Loss: 0.013, Test Loss: 3.146, Train Acc: 0.980 Test Acc: 0.494\n",
      "Epoch: 280, Loss: 0.016, Test Loss: 3.167, Train Acc: 0.986 Test Acc: 0.506\n",
      "Epoch: 285, Loss: 0.005, Test Loss: 2.592, Train Acc: 0.989 Test Acc: 0.506\n",
      "Epoch: 290, Loss: 0.007, Test Loss: 2.352, Train Acc: 0.993 Test Acc: 0.556\n",
      "Epoch: 295, Loss: 0.005, Test Loss: 2.406, Train Acc: 0.992 Test Acc: 0.543\n",
      "Epoch: 300, Loss: 0.009, Test Loss: 2.553, Train Acc: 0.988 Test Acc: 0.568\n",
      "Epoch: 305, Loss: 0.007, Test Loss: 2.304, Train Acc: 0.990 Test Acc: 0.568\n",
      "Epoch: 310, Loss: 0.006, Test Loss: 2.383, Train Acc: 0.989 Test Acc: 0.580\n",
      "Epoch: 315, Loss: 0.001, Test Loss: 2.582, Train Acc: 0.995 Test Acc: 0.543\n",
      "Epoch: 320, Loss: 0.001, Test Loss: 2.621, Train Acc: 0.995 Test Acc: 0.519\n",
      "Model saved in: models/GINtri_best_model.pth\n",
      "Epoch: 325, Loss: 0.006, Test Loss: 2.282, Train Acc: 0.993 Test Acc: 0.642\n",
      "Epoch: 330, Loss: 0.004, Test Loss: 2.722, Train Acc: 0.993 Test Acc: 0.568\n",
      "Epoch: 335, Loss: 0.020, Test Loss: 2.191, Train Acc: 0.991 Test Acc: 0.580\n",
      "Epoch: 340, Loss: 0.014, Test Loss: 2.278, Train Acc: 0.992 Test Acc: 0.531\n",
      "Epoch: 345, Loss: 0.006, Test Loss: 2.431, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 350, Loss: 0.001, Test Loss: 2.289, Train Acc: 0.996 Test Acc: 0.568\n",
      "Epoch: 355, Loss: 0.004, Test Loss: 2.823, Train Acc: 0.988 Test Acc: 0.494\n",
      "Epoch: 360, Loss: 0.005, Test Loss: 2.336, Train Acc: 0.995 Test Acc: 0.580\n",
      "Epoch: 365, Loss: 0.001, Test Loss: 2.394, Train Acc: 0.998 Test Acc: 0.568\n",
      "Epoch: 370, Loss: 0.002, Test Loss: 2.370, Train Acc: 0.997 Test Acc: 0.543\n",
      "Epoch: 375, Loss: 0.001, Test Loss: 2.486, Train Acc: 0.995 Test Acc: 0.556\n",
      "Epoch: 380, Loss: 0.002, Test Loss: 2.637, Train Acc: 0.995 Test Acc: 0.519\n",
      "Epoch: 385, Loss: 0.001, Test Loss: 2.650, Train Acc: 0.997 Test Acc: 0.556\n",
      "Epoch: 390, Loss: 0.007, Test Loss: 2.927, Train Acc: 0.991 Test Acc: 0.519\n",
      "Epoch: 395, Loss: 0.040, Test Loss: 2.655, Train Acc: 0.974 Test Acc: 0.519\n",
      "Epoch: 400, Loss: 0.016, Test Loss: 2.810, Train Acc: 0.980 Test Acc: 0.543\n"
     ]
    }
   ],
   "source": [
    "# gnntri.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 005, Loss: 0.730, Test Loss: 0.693, Train Acc: 0.519 Test Acc: 0.517\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 010, Loss: 0.701, Test Loss: 0.682, Train Acc: 0.554 Test Acc: 0.542\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 015, Loss: 0.690, Test Loss: 0.686, Train Acc: 0.580 Test Acc: 0.598\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 020, Loss: 0.643, Test Loss: 0.666, Train Acc: 0.690 Test Acc: 0.595\n",
      "Epoch: 025, Loss: 0.568, Test Loss: 0.715, Train Acc: 0.709 Test Acc: 0.558\n",
      "Epoch: 030, Loss: 0.589, Test Loss: 0.658, Train Acc: 0.761 Test Acc: 0.601\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 035, Loss: 0.445, Test Loss: 0.771, Train Acc: 0.802 Test Acc: 0.626\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 040, Loss: 0.300, Test Loss: 0.964, Train Acc: 0.864 Test Acc: 0.620\n",
      "Epoch: 045, Loss: 0.600, Test Loss: 0.743, Train Acc: 0.677 Test Acc: 0.567\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 050, Loss: 0.294, Test Loss: 0.946, Train Acc: 0.920 Test Acc: 0.632\n",
      "Epoch: 055, Loss: 0.143, Test Loss: 1.202, Train Acc: 0.965 Test Acc: 0.642\n",
      "Epoch: 060, Loss: 0.084, Test Loss: 1.421, Train Acc: 0.994 Test Acc: 0.642\n",
      "Epoch: 065, Loss: 0.115, Test Loss: 1.345, Train Acc: 0.983 Test Acc: 0.620\n",
      "Epoch: 070, Loss: 0.050, Test Loss: 1.682, Train Acc: 0.995 Test Acc: 0.620\n",
      "Epoch: 075, Loss: 0.074, Test Loss: 1.843, Train Acc: 0.986 Test Acc: 0.607\n",
      "Epoch: 080, Loss: 0.101, Test Loss: 1.626, Train Acc: 0.991 Test Acc: 0.639\n",
      "Epoch: 085, Loss: 0.029, Test Loss: 1.638, Train Acc: 0.998 Test Acc: 0.617\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 090, Loss: 0.015, Test Loss: 1.748, Train Acc: 0.999 Test Acc: 0.648\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 095, Loss: 0.052, Test Loss: 1.481, Train Acc: 0.995 Test Acc: 0.651\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 100, Loss: 0.041, Test Loss: 1.583, Train Acc: 0.998 Test Acc: 0.651\n",
      "Epoch: 105, Loss: 0.016, Test Loss: 1.626, Train Acc: 0.998 Test Acc: 0.629\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 110, Loss: 0.166, Test Loss: 1.675, Train Acc: 0.975 Test Acc: 0.667\n",
      "Model saved in: models/GIN2_best_model.pth\n",
      "Epoch: 115, Loss: 0.028, Test Loss: 1.694, Train Acc: 0.998 Test Acc: 0.648\n",
      "Epoch: 120, Loss: 0.023, Test Loss: 1.874, Train Acc: 0.998 Test Acc: 0.645\n",
      "Epoch: 125, Loss: 0.016, Test Loss: 1.887, Train Acc: 0.999 Test Acc: 0.642\n",
      "Epoch: 130, Loss: 0.055, Test Loss: 1.700, Train Acc: 0.998 Test Acc: 0.639\n",
      "Epoch: 135, Loss: 0.026, Test Loss: 2.047, Train Acc: 0.999 Test Acc: 0.657\n",
      "Epoch: 140, Loss: 0.017, Test Loss: 1.960, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 145, Loss: 0.017, Test Loss: 2.118, Train Acc: 0.999 Test Acc: 0.676\n",
      "Epoch: 150, Loss: 0.038, Test Loss: 1.724, Train Acc: 0.999 Test Acc: 0.660\n",
      "Epoch: 155, Loss: 0.024, Test Loss: 1.669, Train Acc: 0.999 Test Acc: 0.645\n",
      "Epoch: 160, Loss: 0.012, Test Loss: 1.696, Train Acc: 0.999 Test Acc: 0.667\n",
      "Epoch: 165, Loss: 0.023, Test Loss: 1.969, Train Acc: 0.998 Test Acc: 0.651\n",
      "Epoch: 170, Loss: 0.034, Test Loss: 1.714, Train Acc: 0.998 Test Acc: 0.636\n",
      "Epoch: 175, Loss: 0.026, Test Loss: 1.710, Train Acc: 0.998 Test Acc: 0.657\n",
      "Epoch: 180, Loss: 0.014, Test Loss: 1.709, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 185, Loss: 0.038, Test Loss: 1.807, Train Acc: 0.996 Test Acc: 0.642\n",
      "Epoch: 190, Loss: 0.027, Test Loss: 1.839, Train Acc: 0.997 Test Acc: 0.629\n",
      "Epoch: 195, Loss: 0.016, Test Loss: 1.890, Train Acc: 0.998 Test Acc: 0.626\n",
      "Epoch: 200, Loss: 0.017, Test Loss: 1.924, Train Acc: 0.999 Test Acc: 0.632\n",
      "Epoch: 205, Loss: 0.014, Test Loss: 1.795, Train Acc: 0.999 Test Acc: 0.642\n",
      "Epoch: 210, Loss: 0.012, Test Loss: 1.751, Train Acc: 0.999 Test Acc: 0.639\n",
      "Epoch: 215, Loss: 0.013, Test Loss: 1.872, Train Acc: 0.999 Test Acc: 0.639\n",
      "Epoch: 220, Loss: 0.019, Test Loss: 1.937, Train Acc: 0.999 Test Acc: 0.636\n",
      "Epoch: 225, Loss: 0.011, Test Loss: 1.790, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 230, Loss: 0.014, Test Loss: 1.895, Train Acc: 0.999 Test Acc: 0.642\n",
      "Epoch: 235, Loss: 0.007, Test Loss: 1.921, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 240, Loss: 0.007, Test Loss: 1.822, Train Acc: 0.999 Test Acc: 0.642\n",
      "Epoch: 245, Loss: 0.010, Test Loss: 1.935, Train Acc: 0.998 Test Acc: 0.645\n",
      "Epoch: 250, Loss: 0.037, Test Loss: 1.980, Train Acc: 0.996 Test Acc: 0.639\n",
      "Epoch: 255, Loss: 0.010, Test Loss: 1.891, Train Acc: 0.999 Test Acc: 0.645\n",
      "Epoch: 260, Loss: 0.027, Test Loss: 1.996, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 265, Loss: 0.014, Test Loss: 1.993, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 270, Loss: 0.015, Test Loss: 1.889, Train Acc: 0.999 Test Acc: 0.642\n",
      "Epoch: 275, Loss: 0.015, Test Loss: 1.934, Train Acc: 0.999 Test Acc: 0.639\n",
      "Epoch: 280, Loss: 0.009, Test Loss: 1.993, Train Acc: 0.999 Test Acc: 0.639\n",
      "Epoch: 285, Loss: 0.009, Test Loss: 1.827, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 290, Loss: 0.013, Test Loss: 1.893, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 295, Loss: 0.007, Test Loss: 1.862, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 300, Loss: 0.006, Test Loss: 1.884, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 305, Loss: 0.014, Test Loss: 1.947, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 310, Loss: 0.008, Test Loss: 1.860, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 315, Loss: 0.007, Test Loss: 1.911, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 320, Loss: 0.015, Test Loss: 2.006, Train Acc: 0.999 Test Acc: 0.639\n",
      "Epoch: 325, Loss: 0.010, Test Loss: 2.105, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 330, Loss: 0.013, Test Loss: 1.998, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 335, Loss: 0.008, Test Loss: 1.862, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 340, Loss: 0.008, Test Loss: 1.935, Train Acc: 0.999 Test Acc: 0.645\n",
      "Epoch: 345, Loss: 0.009, Test Loss: 1.932, Train Acc: 0.999 Test Acc: 0.660\n",
      "Epoch: 350, Loss: 0.016, Test Loss: 1.869, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 355, Loss: 0.007, Test Loss: 1.979, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 360, Loss: 0.007, Test Loss: 1.960, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 365, Loss: 0.007, Test Loss: 1.822, Train Acc: 0.999 Test Acc: 0.657\n",
      "Epoch: 370, Loss: 0.009, Test Loss: 2.020, Train Acc: 0.999 Test Acc: 0.657\n",
      "Epoch: 375, Loss: 0.007, Test Loss: 1.839, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 380, Loss: 0.009, Test Loss: 1.769, Train Acc: 0.999 Test Acc: 0.639\n",
      "Epoch: 385, Loss: 0.009, Test Loss: 1.926, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 390, Loss: 0.006, Test Loss: 1.924, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 395, Loss: 0.008, Test Loss: 1.894, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 400, Loss: 0.011, Test Loss: 1.985, Train Acc: 0.999 Test Acc: 0.657\n",
      "Epoch: 405, Loss: 0.005, Test Loss: 2.013, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 410, Loss: 0.006, Test Loss: 1.997, Train Acc: 0.999 Test Acc: 0.636\n",
      "Epoch: 415, Loss: 0.008, Test Loss: 1.815, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 420, Loss: 0.008, Test Loss: 1.847, Train Acc: 0.999 Test Acc: 0.660\n",
      "Epoch: 425, Loss: 0.009, Test Loss: 1.856, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 430, Loss: 0.010, Test Loss: 2.065, Train Acc: 0.999 Test Acc: 0.657\n",
      "Epoch: 435, Loss: 0.009, Test Loss: 1.942, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 440, Loss: 0.004, Test Loss: 1.923, Train Acc: 0.999 Test Acc: 0.667\n",
      "Epoch: 445, Loss: 0.014, Test Loss: 2.037, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 450, Loss: 0.014, Test Loss: 2.112, Train Acc: 0.999 Test Acc: 0.642\n",
      "Epoch: 455, Loss: 0.006, Test Loss: 1.908, Train Acc: 0.999 Test Acc: 0.657\n",
      "Epoch: 460, Loss: 0.010, Test Loss: 2.111, Train Acc: 0.999 Test Acc: 0.629\n",
      "Epoch: 465, Loss: 0.007, Test Loss: 1.909, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 470, Loss: 0.007, Test Loss: 1.942, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 475, Loss: 0.011, Test Loss: 2.083, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 480, Loss: 0.006, Test Loss: 1.904, Train Acc: 0.999 Test Acc: 0.660\n",
      "Epoch: 485, Loss: 0.007, Test Loss: 1.905, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 490, Loss: 0.007, Test Loss: 1.964, Train Acc: 0.999 Test Acc: 0.642\n",
      "Epoch: 495, Loss: 0.006, Test Loss: 1.890, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 500, Loss: 0.009, Test Loss: 1.916, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 505, Loss: 0.011, Test Loss: 1.932, Train Acc: 0.999 Test Acc: 0.654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 510, Loss: 0.008, Test Loss: 1.981, Train Acc: 0.999 Test Acc: 0.648\n",
      "Epoch: 515, Loss: 0.009, Test Loss: 1.859, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 520, Loss: 0.012, Test Loss: 2.096, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 525, Loss: 0.006, Test Loss: 1.913, Train Acc: 1.000 Test Acc: 0.648\n",
      "Epoch: 530, Loss: 0.014, Test Loss: 1.979, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 535, Loss: 0.007, Test Loss: 1.903, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 540, Loss: 0.009, Test Loss: 1.998, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 545, Loss: 0.007, Test Loss: 1.958, Train Acc: 0.999 Test Acc: 0.651\n",
      "Epoch: 550, Loss: 0.008, Test Loss: 1.966, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 555, Loss: 0.009, Test Loss: 1.838, Train Acc: 0.999 Test Acc: 0.664\n",
      "Epoch: 560, Loss: 0.004, Test Loss: 1.964, Train Acc: 0.999 Test Acc: 0.657\n",
      "Epoch: 565, Loss: 0.009, Test Loss: 1.955, Train Acc: 0.999 Test Acc: 0.645\n",
      "Epoch: 570, Loss: 0.006, Test Loss: 1.948, Train Acc: 1.000 Test Acc: 0.657\n",
      "Epoch: 575, Loss: 0.009, Test Loss: 1.959, Train Acc: 0.999 Test Acc: 0.660\n",
      "Epoch: 580, Loss: 0.009, Test Loss: 1.993, Train Acc: 0.999 Test Acc: 0.654\n",
      "Epoch: 585, Loss: 0.006, Test Loss: 1.971, Train Acc: 0.999 Test Acc: 0.657\n",
      "Epoch: 590, Loss: 0.010, Test Loss: 1.907, Train Acc: 1.000 Test Acc: 0.648\n",
      "Epoch: 595, Loss: 0.006, Test Loss: 1.982, Train Acc: 0.999 Test Acc: 0.664\n",
      "Epoch: 600, Loss: 0.006, Test Loss: 1.879, Train Acc: 0.999 Test Acc: 0.651\n"
     ]
    }
   ],
   "source": [
    "# gnn2.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/GIN3_best_model.pth\n",
      "Model saved in: models/GIN3_best_model.pth\n",
      "Epoch: 005, Loss: 0.689, Test Loss: 0.701, Train Acc: 0.536 Test Acc: 0.519\n",
      "Model saved in: models/GIN3_best_model.pth\n",
      "Epoch: 010, Loss: 0.653, Test Loss: 0.764, Train Acc: 0.619 Test Acc: 0.506\n",
      "Epoch: 015, Loss: 0.578, Test Loss: 0.911, Train Acc: 0.737 Test Acc: 0.457\n",
      "Epoch: 020, Loss: 0.440, Test Loss: 0.892, Train Acc: 0.823 Test Acc: 0.519\n",
      "Epoch: 025, Loss: 0.353, Test Loss: 1.099, Train Acc: 0.860 Test Acc: 0.469\n",
      "Epoch: 030, Loss: 0.246, Test Loss: 1.353, Train Acc: 0.919 Test Acc: 0.481\n",
      "Epoch: 035, Loss: 0.154, Test Loss: 1.380, Train Acc: 0.961 Test Acc: 0.457\n",
      "Epoch: 040, Loss: 0.198, Test Loss: 1.591, Train Acc: 0.940 Test Acc: 0.506\n",
      "Epoch: 045, Loss: 0.145, Test Loss: 1.446, Train Acc: 0.966 Test Acc: 0.469\n",
      "Epoch: 050, Loss: 0.120, Test Loss: 1.654, Train Acc: 0.964 Test Acc: 0.481\n",
      "Epoch: 055, Loss: 0.073, Test Loss: 1.597, Train Acc: 0.980 Test Acc: 0.494\n",
      "Epoch: 060, Loss: 0.068, Test Loss: 1.672, Train Acc: 0.982 Test Acc: 0.543\n",
      "Epoch: 065, Loss: 0.052, Test Loss: 1.732, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 070, Loss: 0.052, Test Loss: 1.694, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 075, Loss: 0.071, Test Loss: 1.899, Train Acc: 0.979 Test Acc: 0.481\n",
      "Epoch: 080, Loss: 0.115, Test Loss: 2.029, Train Acc: 0.957 Test Acc: 0.481\n",
      "Epoch: 085, Loss: 0.070, Test Loss: 1.712, Train Acc: 0.984 Test Acc: 0.494\n",
      "Epoch: 090, Loss: 0.067, Test Loss: 1.756, Train Acc: 0.983 Test Acc: 0.519\n",
      "Epoch: 095, Loss: 0.074, Test Loss: 1.992, Train Acc: 0.974 Test Acc: 0.457\n",
      "Epoch: 100, Loss: 0.067, Test Loss: 1.884, Train Acc: 0.981 Test Acc: 0.519\n",
      "Epoch: 105, Loss: 0.060, Test Loss: 2.174, Train Acc: 0.983 Test Acc: 0.481\n",
      "Epoch: 110, Loss: 0.039, Test Loss: 2.089, Train Acc: 0.989 Test Acc: 0.494\n",
      "Epoch: 115, Loss: 0.047, Test Loss: 2.166, Train Acc: 0.986 Test Acc: 0.469\n",
      "Epoch: 120, Loss: 0.054, Test Loss: 2.170, Train Acc: 0.979 Test Acc: 0.506\n",
      "Epoch: 125, Loss: 0.048, Test Loss: 2.270, Train Acc: 0.982 Test Acc: 0.506\n",
      "Epoch: 130, Loss: 0.048, Test Loss: 2.084, Train Acc: 0.985 Test Acc: 0.543\n",
      "Epoch: 135, Loss: 0.043, Test Loss: 2.394, Train Acc: 0.987 Test Acc: 0.481\n",
      "Epoch: 140, Loss: 0.045, Test Loss: 2.317, Train Acc: 0.984 Test Acc: 0.469\n",
      "Epoch: 145, Loss: 0.049, Test Loss: 2.179, Train Acc: 0.986 Test Acc: 0.494\n",
      "Epoch: 150, Loss: 0.037, Test Loss: 2.128, Train Acc: 0.989 Test Acc: 0.506\n",
      "Epoch: 155, Loss: 0.038, Test Loss: 2.141, Train Acc: 0.988 Test Acc: 0.481\n",
      "Epoch: 160, Loss: 0.032, Test Loss: 2.114, Train Acc: 0.990 Test Acc: 0.519\n",
      "Epoch: 165, Loss: 0.047, Test Loss: 2.136, Train Acc: 0.982 Test Acc: 0.531\n",
      "Epoch: 170, Loss: 0.029, Test Loss: 2.101, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 175, Loss: 0.035, Test Loss: 2.144, Train Acc: 0.987 Test Acc: 0.506\n",
      "Epoch: 180, Loss: 0.037, Test Loss: 2.110, Train Acc: 0.986 Test Acc: 0.543\n",
      "Epoch: 185, Loss: 0.029, Test Loss: 2.167, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 190, Loss: 0.026, Test Loss: 2.150, Train Acc: 0.988 Test Acc: 0.506\n",
      "Epoch: 195, Loss: 0.030, Test Loss: 2.136, Train Acc: 0.988 Test Acc: 0.519\n",
      "Epoch: 200, Loss: 0.030, Test Loss: 2.198, Train Acc: 0.989 Test Acc: 0.506\n",
      "Epoch: 205, Loss: 0.039, Test Loss: 2.281, Train Acc: 0.986 Test Acc: 0.519\n",
      "Epoch: 210, Loss: 0.037, Test Loss: 2.260, Train Acc: 0.987 Test Acc: 0.494\n",
      "Epoch: 215, Loss: 0.035, Test Loss: 2.320, Train Acc: 0.987 Test Acc: 0.519\n",
      "Epoch: 220, Loss: 0.030, Test Loss: 2.312, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 225, Loss: 0.035, Test Loss: 2.344, Train Acc: 0.987 Test Acc: 0.506\n",
      "Epoch: 230, Loss: 0.038, Test Loss: 2.429, Train Acc: 0.986 Test Acc: 0.519\n",
      "Epoch: 235, Loss: 0.037, Test Loss: 2.340, Train Acc: 0.986 Test Acc: 0.519\n",
      "Epoch: 240, Loss: 0.034, Test Loss: 2.326, Train Acc: 0.987 Test Acc: 0.519\n",
      "Epoch: 245, Loss: 0.040, Test Loss: 2.290, Train Acc: 0.987 Test Acc: 0.519\n",
      "Epoch: 250, Loss: 0.037, Test Loss: 2.384, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 255, Loss: 0.032, Test Loss: 2.332, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 260, Loss: 0.034, Test Loss: 2.304, Train Acc: 0.988 Test Acc: 0.519\n",
      "Epoch: 265, Loss: 0.032, Test Loss: 2.321, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 270, Loss: 0.037, Test Loss: 2.277, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 275, Loss: 0.035, Test Loss: 2.277, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 280, Loss: 0.035, Test Loss: 2.312, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 285, Loss: 0.036, Test Loss: 2.288, Train Acc: 0.988 Test Acc: 0.519\n",
      "Epoch: 290, Loss: 0.031, Test Loss: 2.283, Train Acc: 0.991 Test Acc: 0.494\n",
      "Epoch: 295, Loss: 0.034, Test Loss: 2.261, Train Acc: 0.989 Test Acc: 0.494\n",
      "Epoch: 300, Loss: 0.035, Test Loss: 2.263, Train Acc: 0.989 Test Acc: 0.494\n",
      "Epoch: 305, Loss: 0.038, Test Loss: 2.303, Train Acc: 0.988 Test Acc: 0.506\n",
      "Epoch: 310, Loss: 0.034, Test Loss: 2.297, Train Acc: 0.989 Test Acc: 0.506\n",
      "Epoch: 315, Loss: 0.033, Test Loss: 2.299, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 320, Loss: 0.032, Test Loss: 2.330, Train Acc: 0.990 Test Acc: 0.531\n",
      "Epoch: 325, Loss: 0.037, Test Loss: 2.304, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 330, Loss: 0.034, Test Loss: 2.341, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 335, Loss: 0.033, Test Loss: 2.316, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 340, Loss: 0.038, Test Loss: 2.334, Train Acc: 0.988 Test Acc: 0.519\n",
      "Epoch: 345, Loss: 0.037, Test Loss: 2.384, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 350, Loss: 0.038, Test Loss: 2.341, Train Acc: 0.987 Test Acc: 0.519\n",
      "Epoch: 355, Loss: 0.036, Test Loss: 2.372, Train Acc: 0.988 Test Acc: 0.519\n",
      "Epoch: 360, Loss: 0.033, Test Loss: 2.367, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 365, Loss: 0.036, Test Loss: 2.382, Train Acc: 0.988 Test Acc: 0.519\n",
      "Epoch: 370, Loss: 0.033, Test Loss: 2.348, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 375, Loss: 0.035, Test Loss: 2.327, Train Acc: 0.989 Test Acc: 0.519\n",
      "Epoch: 380, Loss: 0.036, Test Loss: 2.349, Train Acc: 0.988 Test Acc: 0.519\n",
      "Epoch: 385, Loss: 0.034, Test Loss: 2.369, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 390, Loss: 0.036, Test Loss: 2.373, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 395, Loss: 0.037, Test Loss: 2.385, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 400, Loss: 0.038, Test Loss: 2.355, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 405, Loss: 0.035, Test Loss: 2.375, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 410, Loss: 0.039, Test Loss: 2.407, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 415, Loss: 0.035, Test Loss: 2.330, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 420, Loss: 0.035, Test Loss: 2.402, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 425, Loss: 0.034, Test Loss: 2.390, Train Acc: 0.990 Test Acc: 0.543\n",
      "Epoch: 430, Loss: 0.041, Test Loss: 2.385, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 435, Loss: 0.037, Test Loss: 2.365, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 440, Loss: 0.036, Test Loss: 2.358, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 445, Loss: 0.039, Test Loss: 2.372, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 450, Loss: 0.034, Test Loss: 2.376, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 455, Loss: 0.037, Test Loss: 2.379, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 460, Loss: 0.038, Test Loss: 2.370, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 465, Loss: 0.034, Test Loss: 2.365, Train Acc: 0.990 Test Acc: 0.531\n",
      "Epoch: 470, Loss: 0.038, Test Loss: 2.388, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 475, Loss: 0.036, Test Loss: 2.370, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 480, Loss: 0.037, Test Loss: 2.411, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 485, Loss: 0.034, Test Loss: 2.363, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 490, Loss: 0.033, Test Loss: 2.393, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 495, Loss: 0.038, Test Loss: 2.369, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 500, Loss: 0.040, Test Loss: 2.370, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 505, Loss: 0.035, Test Loss: 2.392, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 510, Loss: 0.037, Test Loss: 2.392, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 515, Loss: 0.036, Test Loss: 2.382, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 520, Loss: 0.037, Test Loss: 2.399, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 525, Loss: 0.035, Test Loss: 2.373, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 530, Loss: 0.037, Test Loss: 2.359, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 535, Loss: 0.035, Test Loss: 2.374, Train Acc: 0.989 Test Acc: 0.543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 540, Loss: 0.038, Test Loss: 2.357, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 545, Loss: 0.034, Test Loss: 2.390, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 550, Loss: 0.035, Test Loss: 2.367, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 555, Loss: 0.035, Test Loss: 2.369, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 560, Loss: 0.032, Test Loss: 2.419, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 565, Loss: 0.037, Test Loss: 2.365, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 570, Loss: 0.036, Test Loss: 2.367, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 575, Loss: 0.037, Test Loss: 2.370, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 580, Loss: 0.034, Test Loss: 2.368, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 585, Loss: 0.029, Test Loss: 2.371, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 590, Loss: 0.034, Test Loss: 2.386, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 595, Loss: 0.036, Test Loss: 2.407, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 600, Loss: 0.030, Test Loss: 2.359, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 605, Loss: 0.035, Test Loss: 2.379, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 610, Loss: 0.037, Test Loss: 2.410, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 615, Loss: 0.037, Test Loss: 2.398, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 620, Loss: 0.034, Test Loss: 2.411, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 625, Loss: 0.042, Test Loss: 2.412, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 630, Loss: 0.031, Test Loss: 2.376, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 635, Loss: 0.033, Test Loss: 2.397, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 640, Loss: 0.035, Test Loss: 2.384, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 645, Loss: 0.037, Test Loss: 2.406, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 650, Loss: 0.034, Test Loss: 2.435, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 655, Loss: 0.039, Test Loss: 2.408, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 660, Loss: 0.036, Test Loss: 2.393, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 665, Loss: 0.034, Test Loss: 2.414, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 670, Loss: 0.035, Test Loss: 2.395, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 675, Loss: 0.032, Test Loss: 2.418, Train Acc: 0.990 Test Acc: 0.543\n",
      "Epoch: 680, Loss: 0.037, Test Loss: 2.392, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 685, Loss: 0.039, Test Loss: 2.419, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 690, Loss: 0.035, Test Loss: 2.384, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 695, Loss: 0.033, Test Loss: 2.416, Train Acc: 0.991 Test Acc: 0.543\n",
      "Epoch: 700, Loss: 0.036, Test Loss: 2.404, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 705, Loss: 0.035, Test Loss: 2.395, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 710, Loss: 0.038, Test Loss: 2.394, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 715, Loss: 0.034, Test Loss: 2.377, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 720, Loss: 0.033, Test Loss: 2.431, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 725, Loss: 0.033, Test Loss: 2.367, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 730, Loss: 0.038, Test Loss: 2.405, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 735, Loss: 0.034, Test Loss: 2.377, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 740, Loss: 0.036, Test Loss: 2.372, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 745, Loss: 0.036, Test Loss: 2.390, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 750, Loss: 0.036, Test Loss: 2.418, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 755, Loss: 0.033, Test Loss: 2.389, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 760, Loss: 0.033, Test Loss: 2.384, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 765, Loss: 0.030, Test Loss: 2.408, Train Acc: 0.991 Test Acc: 0.556\n",
      "Epoch: 770, Loss: 0.036, Test Loss: 2.389, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 775, Loss: 0.033, Test Loss: 2.397, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 780, Loss: 0.036, Test Loss: 2.414, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 785, Loss: 0.040, Test Loss: 2.397, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 790, Loss: 0.037, Test Loss: 2.411, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 795, Loss: 0.032, Test Loss: 2.410, Train Acc: 0.991 Test Acc: 0.531\n",
      "Epoch: 800, Loss: 0.031, Test Loss: 2.407, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 805, Loss: 0.036, Test Loss: 2.383, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 810, Loss: 0.032, Test Loss: 2.406, Train Acc: 0.991 Test Acc: 0.543\n",
      "Epoch: 815, Loss: 0.035, Test Loss: 2.394, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 820, Loss: 0.034, Test Loss: 2.376, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 825, Loss: 0.033, Test Loss: 2.403, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 830, Loss: 0.039, Test Loss: 2.375, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 835, Loss: 0.037, Test Loss: 2.392, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 840, Loss: 0.038, Test Loss: 2.416, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 845, Loss: 0.034, Test Loss: 2.440, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 850, Loss: 0.036, Test Loss: 2.391, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 855, Loss: 0.036, Test Loss: 2.384, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 860, Loss: 0.036, Test Loss: 2.402, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 865, Loss: 0.037, Test Loss: 2.415, Train Acc: 0.988 Test Acc: 0.531\n",
      "Epoch: 870, Loss: 0.036, Test Loss: 2.399, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 875, Loss: 0.035, Test Loss: 2.403, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 880, Loss: 0.031, Test Loss: 2.426, Train Acc: 0.991 Test Acc: 0.531\n",
      "Epoch: 885, Loss: 0.036, Test Loss: 2.412, Train Acc: 0.988 Test Acc: 0.543\n",
      "Epoch: 890, Loss: 0.036, Test Loss: 2.408, Train Acc: 0.989 Test Acc: 0.543\n",
      "Epoch: 895, Loss: 0.034, Test Loss: 2.411, Train Acc: 0.989 Test Acc: 0.531\n",
      "Epoch: 900, Loss: 0.034, Test Loss: 2.420, Train Acc: 0.989 Test Acc: 0.543\n"
     ]
    }
   ],
   "source": [
    "# gnn3.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn3.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnntri.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/FC_MDD_GINserver.pt\n"
     ]
    }
   ],
   "source": [
    "# save the model \n",
    "gnn.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")\n",
    "\n",
    "# gnn2.save_model(path=\"models/\"+DATASET+\"_\"+MODEL2+\"server.pt\")\n",
    "\n",
    "# gnn3.save_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "# gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\", map_location='cpu')\n",
    "gnn.load_model(path=\"models/GIN_best_model.pth\")\n",
    "# gnn2.load_model(path=\"models/\"+DATASET+\"_\"+MODEL2+\"server.pt\")\n",
    "\n",
    "# gnn3.load_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server.pt\")\n",
    "# gnnbis.load_model(path=\"models/GINbis_best_model.pth\")\n",
    "# gnntri.load_model(path=\"models/\"+DATASET+\"_\"+MODELtri+\"server.pt\")\n",
    "#gnn3.save_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server_iterate.pt\")\n",
    "#gnn3.load_model(path=\"models/\"+DATASET+\"_\"+MODEL3+\"server_iterate.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.661, Train Acc: 0.978 Test Acc: 0.691\n"
     ]
    }
   ],
   "source": [
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnntri.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn3.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import DataLoader\n",
    "# test_loader = DataLoader(dataset[gnn.test_idx], batch_size=1, shuffle=False)\n",
    "\n",
    "# gnn3.evaluate2(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Alternative metrics for disconnected graphs\n",
    "        # Option 1: Use the average path length of the largest connected component\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "def calculate_small_world(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.algorithms.smallworld.sigma(G)\n",
    "    else:\n",
    "        # Alternative metrics for disconnected graphs\n",
    "        # Option 1: Use the small world coefficient of the largest connected component\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.algorithms.smallworld.sigma(largest_component)\n",
    "    \n",
    "def compute_swi(graph):\n",
    "    # Calculate clustering coefficient and average path length for the given graph\n",
    "    clustering_coeff = nx.average_clustering(graph)\n",
    "    avg_path_len = calculate_avg_path_length(graph)\n",
    "    \n",
    "    # Generate a random graph with the same number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    \n",
    "    # Generate a lattice graph with the same number of nodes and edges\n",
    "    lattice_graph = nx.watts_strogatz_graph(num_nodes, k=4, p=0)  # Adjust k as needed\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the random graph\n",
    "    random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "    random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the lattice graph\n",
    "    lattice_clustering_coeff = nx.average_clustering(lattice_graph)\n",
    "    lattice_avg_path_len = calculate_avg_path_length(lattice_graph)\n",
    "    \n",
    "    # Compute the Small-World Index (SWI)\n",
    "    swi = ((avg_path_len - lattice_avg_path_len) / (random_avg_path_len - lattice_avg_path_len)) * \\\n",
    "          ((clustering_coeff - random_clustering_coeff) / (lattice_clustering_coeff - random_clustering_coeff))\n",
    "    \n",
    "    return swi\n",
    "    \n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        num_edges = G.number_of_edges()\n",
    "        density = nx.density(G)\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        number_of_node_in_the_largest_fully_connected_component = len(max(nx.connected_components(G), key=len))\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "        small_world = compute_swi(G)\n",
    "            \n",
    "        properties.append((num_nodes, num_edges, density, avg_path_len, num_cliques, num_triangles, num_squares, number_of_node_in_the_largest_fully_connected_component, assortativity, small_world))\n",
    "    return properties\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties = compute_graph_properties(selected_dataset)\n",
    "\n",
    "# Save the properties to files\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_with_sm.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_with_sm.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_properties))\n",
    "# train_properties[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#save the properties in a file\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(train_properties, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(test_properties, f)\n",
    "\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_with_sm.pkl\", \"rb\") as f:\n",
    "    train_properties = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_with_sm.pkl\", \"rb\") as f:\n",
    "    test_properties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_features[0]))\n",
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embeddings of GIN are like this:\n",
    "return F.log_softmax(x7, dim=-1), (x1, x2, x3, x4, x5, x_global, x6, x7, x8)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "output_size = 1  # Predicting one property at a time\n",
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Assume we have already evaluated to get features\n",
    "# train_features, test_features = gnn.evaluate_with_features2()\n",
    "\n",
    "# Extract x embeddings\n",
    "train_x = np.array([feat[0] for feat in train_features])\n",
    "test_x = np.array([feat[0] for feat in test_features])\n",
    "\n",
    "# Extract 2, 3, 4, global, 5, 6, 7 embeddings\n",
    "train_x2 = np.array([feat[1] for feat in train_features])\n",
    "test_x2 = np.array([feat[1] for feat in test_features])\n",
    "\n",
    "train_x3 = np.array([feat[2] for feat in train_features])\n",
    "test_x3 = np.array([feat[2] for feat in test_features])\n",
    "\n",
    "train_x4 = np.array([feat[3] for feat in train_features])\n",
    "test_x4 = np.array([feat[3] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[4] for feat in train_features])\n",
    "test_x5 = np.array([feat[4] for feat in test_features])\n",
    "\n",
    "train_x_global = np.array([feat[5] for feat in train_features])\n",
    "test_x_global = np.array([feat[5] for feat in test_features])\n",
    "\n",
    "train_x6 = np.array([feat[6] for feat in train_features])\n",
    "test_x6 = np.array([feat[6] for feat in test_features])\n",
    "\n",
    "train_x7 = np.array([feat[7] for feat in train_features])\n",
    "test_x7 = np.array([feat[7] for feat in test_features])\n",
    "\n",
    "train_x8 = np.array([feat[8] for feat in train_features])\n",
    "test_x8 = np.array([feat[8] for feat in test_features])\n",
    "\n",
    "\n",
    "# Compute graph properties\n",
    "# train_properties = compute_graph_properties(gnn.dataset[gnn.train_idx])\n",
    "# test_properties = compute_graph_properties(gnn.dataset[gnn.test_idx])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_x2 = torch.tensor(train_x2, dtype=torch.float32)\n",
    "train_x3 = torch.tensor(train_x3, dtype=torch.float32)\n",
    "train_x4 = torch.tensor(train_x4, dtype=torch.float32)\n",
    "train_x5 = torch.tensor(train_x5, dtype=torch.float32)\n",
    "train_x_global = torch.tensor(train_x_global, dtype=torch.float32)\n",
    "train_x6 = torch.tensor(train_x6, dtype=torch.float32)\n",
    "train_x7 = torch.tensor(train_x7, dtype=torch.float32)\n",
    "train_x8 = torch.tensor(train_x8, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x2 = torch.tensor(test_x2, dtype=torch.float32)\n",
    "test_x3 = torch.tensor(test_x3, dtype=torch.float32)\n",
    "test_x4 = torch.tensor(test_x4, dtype=torch.float32)\n",
    "test_x_global = torch.tensor(test_x_global, dtype=torch.float32)\n",
    "test_x5 = torch.tensor(test_x5, dtype=torch.float32)\n",
    "test_x6 = torch.tensor(test_x6, dtype=torch.float32)\n",
    "test_x7 = torch.tensor(test_x7, dtype=torch.float32)\n",
    "test_x8 = torch.tensor(test_x8, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_properties, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_properties, dtype=torch.float32)\n",
    "\n",
    "# Train and evaluate a model for each graph property and each embedding\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings = [(train_x, test_x), (train_x2, test_x2), (train_x3, test_x3), (train_x4, test_x4), (train_x5, test_x5), (train_x_global, test_x_global), (train_x6, test_x6), (train_x7, test_x7), (train_x8, test_x8)]\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global',  'x6', 'x7', 'x8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    # Flatten the embeddings before determining the input size\n",
    "    train_embedding_flat = train_embedding.view(train_embedding.size(0), -1)\n",
    "    test_embedding_flat = test_embedding.view(test_embedding.size(0), -1)\n",
    "    #print the shapes\n",
    "    print(train_embedding_flat.shape)\n",
    "    print(test_embedding_flat.shape)\n",
    "    input_size = train_embedding_flat.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 2000000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding_flat).squeeze()\n",
    "            target = train_y[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding_flat).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding_flat).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y[:, i].cpu().numpy()\n",
    "            test_target = test_y[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R: {train_r2:.4f}, Test R: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_full_embedding.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_full_embedding.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'grey', 'orange', 'purple']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_name)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig(\"results/\"+DATASET+\"_\"+MODEL+\"test_R2_plot_full_embedding.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'grey', 'orange', 'purple']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_name)][2]\n",
    "        if train_r2 < -0.05:\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "\n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig(\"results/\"+DATASET+\"_\"+MODEL+\"train_R2_plot_full_embedding.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with more properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Use the average path length of the largest connected component for disconnected graphs\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "   \n",
    "def compute_swi(graph):\n",
    "    # Calculate clustering coefficient and average path length for the given graph\n",
    "    clustering_coeff = nx.average_clustering(graph)\n",
    "    avg_path_len = calculate_avg_path_length(graph)\n",
    "    \n",
    "    # Generate a random graph with the same number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    \n",
    "    # Generate a lattice graph with the same number of nodes and edges\n",
    "    lattice_graph = nx.watts_strogatz_graph(num_nodes, k=4, p=0)  # Adjust k as needed\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the random graph\n",
    "    random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "    random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the lattice graph\n",
    "    lattice_clustering_coeff = nx.average_clustering(lattice_graph)\n",
    "    lattice_avg_path_len = calculate_avg_path_length(lattice_graph)\n",
    "    \n",
    "    # Compute the Small-World Index (SWI)\n",
    "    swi = ((avg_path_len - lattice_avg_path_len) / (random_avg_path_len - lattice_avg_path_len)) * \\\n",
    "          ((clustering_coeff - random_clustering_coeff) / (lattice_clustering_coeff - random_clustering_coeff))\n",
    "    \n",
    "    return swi\n",
    "\n",
    "\n",
    "def betweenness_centralization(G):\n",
    "    n = len(G)\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    max_betweenness = max(betweenness.values())\n",
    "    centralization = sum(max_betweenness - bet for bet in betweenness.values())\n",
    "    if n > 2:\n",
    "        centralization /= (n - 1) * (n - 2) / 2\n",
    "    return centralization\n",
    "\n",
    "def pagerank_centralization(G, alpha=0.85):\n",
    "    n = len(G)\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    max_pagerank = max(pagerank.values())\n",
    "    centralization = sum(max_pagerank - pr for pr in pagerank.values())\n",
    "    if n > 1:\n",
    "        centralization /= (n - 1)\n",
    "    return centralization\n",
    "\n",
    "def clustering_properties(G):\n",
    "    average_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "    return average_clustering, transitivity\n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        \n",
    "        # Number of nodes\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        \n",
    "        # Number of edges\n",
    "        num_edges = G.number_of_edges()\n",
    "        \n",
    "        # Density\n",
    "        density = nx.density(G)\n",
    "        \n",
    "        # Average Path Length\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        \n",
    "        # Diameter\n",
    "        if nx.is_connected(G):\n",
    "            diameter = nx.diameter(G)\n",
    "        else:\n",
    "            # Use the diameter of the largest connected component for disconnected graphs\n",
    "            components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "            largest_component = max(components, key=len)\n",
    "            diameter = nx.diameter(largest_component)\n",
    "        \n",
    "        # Radius\n",
    "        if nx.is_connected(G):\n",
    "            radius = nx.radius(G)\n",
    "        else:\n",
    "            radius = nx.radius(largest_component)\n",
    "        \n",
    "        # Clustering Coefficient\n",
    "        clustering_coeff = nx.average_clustering(G)\n",
    "        \n",
    "        # Transitivity\n",
    "        transitivity = nx.transitivity(G)\n",
    "        \n",
    "        # Assortativity\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "        \n",
    "        # Number of Cliques\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        \n",
    "        # Number of Triangles\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        \n",
    "        # Number of Squares (4-cycles)\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        \n",
    "        # Size of the Largest Connected Component\n",
    "        largest_component_size = len(max(nx.connected_components(G), key=len))\n",
    "        \n",
    "        # Average Degree\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        avg_degree = np.mean(degrees)\n",
    "        \n",
    "        # Betweenness Centrality\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        avg_betweenness_centrality = np.mean(list(betweenness_centrality.values()))\n",
    "        \n",
    "        # Eigenvalues of the Adjacency Matrix (for spectral properties)\n",
    "        eigenvalues = np.linalg.eigvals(nx.adjacency_matrix(G).todense())\n",
    "        spectral_radius = max(eigenvalues)\n",
    "        algebraic_connectivity = sorted(eigenvalues)[1]  # second smallest eigenvalue\n",
    "        \n",
    "        # Graph Laplacian Eigenvalues\n",
    "        laplacian_eigenvalues = np.linalg.eigvals(nx.laplacian_matrix(G).todense())\n",
    "        graph_energy = sum(abs(laplacian_eigenvalues))\n",
    "        \n",
    "        # Small-World-ness\n",
    "        # Compare clustering coefficient and average path length with those of a random graph\n",
    "        random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "        random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "        random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "        small_world_coefficient = (clustering_coeff / random_clustering_coeff) / (avg_path_len / random_avg_path_len)\n",
    "\n",
    "        small_world_index = compute_swi(G)\n",
    "\n",
    "        # Calculate Betweenness Centralization\n",
    "        betweenness_cent = betweenness_centralization(G)\n",
    "        print(f\"Betweenness Centralization: {betweenness_cent}\")\n",
    "\n",
    "        # Calculate PageRank Centralization\n",
    "        pagerank_cent = pagerank_centralization(G)\n",
    "        print(f\"PageRank Centralization: {pagerank_cent}\")\n",
    "\n",
    "        # Calculate Clustering properties\n",
    "        avg_clustering, transitivity = clustering_properties(G)\n",
    "        print(f\"Average Clustering Coefficient: {avg_clustering}\")\n",
    "        print(f\"Transitivity: {transitivity}\")\n",
    "        \n",
    "        properties.append((\n",
    "            num_nodes,\n",
    "            num_edges,\n",
    "            density,\n",
    "            avg_path_len,\n",
    "            diameter,\n",
    "            radius,\n",
    "            clustering_coeff,\n",
    "            transitivity,\n",
    "            assortativity,\n",
    "            num_cliques,\n",
    "            num_triangles,\n",
    "            num_squares,\n",
    "            largest_component_size,\n",
    "            avg_degree,\n",
    "            avg_betweenness_centrality,\n",
    "            spectral_radius,\n",
    "            algebraic_connectivity,\n",
    "            graph_energy,\n",
    "            small_world_coefficient, \n",
    "            betweenness_cent,\n",
    "            pagerank_cent,\n",
    "            avg_clustering,\n",
    "            small_world_index           \n",
    "\n",
    "        ))\n",
    "    return properties\n",
    "\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties_long = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties_long = compute_graph_properties(selected_dataset)\n",
    "\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties_long, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties_long, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"rb\") as f:\n",
    "    train_properties_long = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"rb\") as f:\n",
    "    test_properties_long = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "train_y_long = torch.tensor(train_properties_long, dtype=torch.float32)\n",
    "test_y_long = torch.tensor(test_properties_long, dtype=torch.float32)\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    # Flatten the embeddings before determining the input size\n",
    "    train_embedding_flat = train_embedding.view(train_embedding.size(0), -1)\n",
    "    test_embedding_flat = test_embedding.view(test_embedding.size(0), -1)\n",
    "    #print the shapes\n",
    "    print(train_embedding_flat.shape)\n",
    "    print(test_embedding_flat.shape)\n",
    "    input_size = train_embedding_flat.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names_long):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 2000000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding_flat).squeeze()\n",
    "            target = train_y_long[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding_flat).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding_flat).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y_long[:, i].cpu().numpy()\n",
    "            test_target = test_y_long[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R: {train_r2:.4f}, Test R: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long_full_embedding.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long_full_embedding.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "#make a color list for the properties names but with different colors\n",
    "colors_long = [\n",
    "    (0.0, 0.45, 0.70),  # Blue\n",
    "    (0.85, 0.37, 0.01),  # Orange\n",
    "    (0.8, 0.47, 0.74),   # Magenta\n",
    "    (0.0, 0.62, 0.45),   # Green\n",
    "    (0.95, 0.90, 0.25),  # Yellow\n",
    "    (0.9, 0.6, 0.0),     # Brown\n",
    "    (0.35, 0.7, 0.9),    # Sky Blue\n",
    "    (0.8, 0.6, 0.7),     # Light Pink\n",
    "    (0.3, 0.3, 0.3),     # Dark Gray\n",
    "    (0.5, 0.5, 0.0),     # Olive\n",
    "    (0.0, 0.75, 0.75),   # Cyan\n",
    "    (0.6, 0.6, 0.6),     # Light Gray\n",
    "    (0.7, 0.3, 0.1),     # Dark Orange\n",
    "    (0.6, 0.2, 0.5),     # Purple\n",
    "    (0.9, 0.4, 0.3),     # Salmon\n",
    "    (0.4, 0.4, 0.8),     # Light Blue\n",
    "    (0.2, 0.8, 0.2),     # Light Green\n",
    "    (0.6, 0.6, 0.3),     # Mustard\n",
    "    (0.3, 0.55, 0.55),    # Teal\n",
    "    (0.8, 0.5, 0.2),     # Dark Salmon\n",
    "    (0.5, 0.5, 0.5),     # Gray\n",
    "    (0.2, 0.2, 0.2),      # Black-Gray\n",
    "    (0.0, 0.0, 0.0)      # Black\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_names_long)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'test_R2_plot_long.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the R2 values in order (bigger to smaller) for x_global\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "#sort the R2 values for x_global\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x_global', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for x6\n",
    "\n",
    "#sort the R2 values for x6\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x6', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for x7\n",
    "\n",
    "#sort the R2 values for x7\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x7', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for x8\n",
    "\n",
    "#sort the R2 values for x8\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x8', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_names_long)][2]\n",
    "        if train_r2 < -0.05:  # Handle negative R values\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "\n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'train_R2_plot_long_full_embedding.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with a gnn train on random (the y are shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_MDD()\n",
    "MODEL = \"GIN3\"\n",
    "DATASET = \"FC_MDD_suffled\"\n",
    "\n",
    "from models.models_FC import GIN_framework3 as framework3 # import the model\n",
    "\n",
    "print(gnn.model)\n",
    "print(gnn.train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the y values of the dataset\n",
    "# y = np.array([data.y for data in dataset])\n",
    "# print(y)\n",
    "# y = np.array(y, dtype=np.int64)  # Ensure y is a numeric array of type int64\n",
    "\n",
    "# np.random.shuffle(y)\n",
    "\n",
    "# #make y torch.int64, tensor([0]) instead of [0]\n",
    "# y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# for i in range(len(dataset)):\n",
    "#     dataset[i].y = y[i]\n",
    "\n",
    "# # check if the y values are shuffled\n",
    "# y = np.array([data.y for data in dataset])\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataset\n",
    "# import pickle as pkl\n",
    "# with open(\"Datasets/FC/\"+DATASET+\"_\"+MODEL+\".pkl\", \"wb\") as f:\n",
    "#     pkl.dump(dataset, f)\n",
    "\n",
    "#load the dataset\n",
    "import pickle as pkl\n",
    "with open(\"Datasets/FC/\"+DATASET+\"_\"+MODEL+\".pkl\", \"rb\") as f:\n",
    "    dataset = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = framework3(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "# gnn.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\".pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties_long = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties_long = compute_graph_properties(selected_dataset)\n",
    "\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties_long, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties_long, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#save the properties in a file\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(train_properties, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(test_properties, f)\n",
    "\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"rb\") as f:\n",
    "    train_properties = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"rb\") as f:\n",
    "    test_properties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embeddings of GIN are like this:\n",
    "return F.log_softmax(x7, dim=-1), (x1, x2, x3, x4, x5, x_global, x6, x7, x8)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "output_size = 1  # Predicting one property at a time\n",
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Assume we have already evaluated to get features\n",
    "#train_features, test_features = gnn.evaluate_with_features2()\n",
    "\n",
    "# Extract x embeddings\n",
    "train_x = np.array([feat[0] for feat in train_features])\n",
    "test_x = np.array([feat[0] for feat in test_features])\n",
    "\n",
    "# Extract 2, 3, 4, global, 5, 6, 7 embeddings\n",
    "train_x2 = np.array([feat[1] for feat in train_features])\n",
    "test_x2 = np.array([feat[1] for feat in test_features])\n",
    "\n",
    "train_x3 = np.array([feat[2] for feat in train_features])\n",
    "test_x3 = np.array([feat[2] for feat in test_features])\n",
    "\n",
    "train_x4 = np.array([feat[3] for feat in train_features])\n",
    "test_x4 = np.array([feat[3] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[4] for feat in train_features])\n",
    "test_x5 = np.array([feat[4] for feat in test_features])\n",
    "\n",
    "train_x_global = np.array([feat[5] for feat in train_features])\n",
    "test_x_global = np.array([feat[5] for feat in test_features])\n",
    "\n",
    "train_x6 = np.array([feat[6] for feat in train_features])\n",
    "test_x6 = np.array([feat[6] for feat in test_features])\n",
    "\n",
    "train_x7 = np.array([feat[7] for feat in train_features])\n",
    "test_x7 = np.array([feat[7] for feat in test_features])\n",
    "\n",
    "train_x8 = np.array([feat[8] for feat in train_features])\n",
    "test_x8 = np.array([feat[8] for feat in test_features])\n",
    "\n",
    "# Compute graph properties\n",
    "# train_properties = compute_graph_properties(gnn.dataset[gnn.train_idx])\n",
    "# test_properties = compute_graph_properties(gnn.dataset[gnn.test_idx])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_x2 = torch.tensor(train_x2, dtype=torch.float32)\n",
    "train_x3 = torch.tensor(train_x3, dtype=torch.float32)\n",
    "train_x4 = torch.tensor(train_x4, dtype=torch.float32)\n",
    "train_x5 = torch.tensor(train_x5, dtype=torch.float32)\n",
    "train_x_global = torch.tensor(train_x_global, dtype=torch.float32)\n",
    "train_x6 = torch.tensor(train_x6, dtype=torch.float32)\n",
    "train_x7 = torch.tensor(train_x7, dtype=torch.float32)\n",
    "train_x8 = torch.tensor(train_x8, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x2 = torch.tensor(test_x2, dtype=torch.float32)\n",
    "test_x3 = torch.tensor(test_x3, dtype=torch.float32)\n",
    "test_x4 = torch.tensor(test_x4, dtype=torch.float32)\n",
    "test_x5 = torch.tensor(test_x5, dtype=torch.float32)\n",
    "test_x_global = torch.tensor(test_x_global, dtype=torch.float32)\n",
    "test_x6 = torch.tensor(test_x6, dtype=torch.float32)\n",
    "test_x7 = torch.tensor(test_x7, dtype=torch.float32)\n",
    "test_x8 = torch.tensor(test_x8, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_properties, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_properties, dtype=torch.float32)\n",
    "\n",
    "# Train and evaluate a model for each graph property and each embedding\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "embeddings = [(train_x, test_x), (train_x2, test_x2), (train_x3, test_x3), (train_x4, test_x4), (train_x5, test_x5), (train_x_global, test_x_global), (train_x6, test_x6), (train_x7, test_x7), (train_x8, test_x8)]\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "train_y_long = torch.tensor(train_properties_long, dtype=torch.float32)\n",
    "test_y_long = torch.tensor(test_properties_long, dtype=torch.float32)\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    # Flatten the embeddings before determining the input size\n",
    "    train_embedding_flat = train_embedding.view(train_embedding.size(0), -1)\n",
    "    test_embedding_flat = test_embedding.view(test_embedding.size(0), -1)\n",
    "    #print the shapes\n",
    "    print(train_embedding_flat.shape)\n",
    "    print(test_embedding_flat.shape)\n",
    "    input_size = train_embedding_flat.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names_long):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 2000000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding_flat).squeeze()\n",
    "            target = train_y_long[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding_flat).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding_flat).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y_long[:, i].cpu().numpy()\n",
    "            test_target = test_y_long[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R: {train_r2:.4f}, Test R: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long_full_embedding.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results \n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long_full_embedding.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors_long = [\n",
    "    (0.0, 0.45, 0.70),  # Blue\n",
    "    (0.85, 0.37, 0.01),  # Orange\n",
    "    (0.8, 0.47, 0.74),   # Magenta\n",
    "    (0.0, 0.62, 0.45),   # Green\n",
    "    (0.95, 0.90, 0.25),  # Yellow\n",
    "    (0.9, 0.6, 0.0),     # Brown\n",
    "    (0.35, 0.7, 0.9),    # Sky Blue\n",
    "    (0.8, 0.6, 0.7),     # Light Pink\n",
    "    (0.3, 0.3, 0.3),     # Dark Gray\n",
    "    (0.5, 0.5, 0.0),     # Olive\n",
    "    (0.0, 0.75, 0.75),   # Cyan\n",
    "    (0.6, 0.6, 0.6),     # Light Gray\n",
    "    (0.7, 0.3, 0.1),     # Dark Orange\n",
    "    (0.6, 0.2, 0.5),     # Purple\n",
    "    (0.9, 0.4, 0.3),     # Salmon\n",
    "    (0.4, 0.4, 0.8),     # Light Blue\n",
    "    (0.2, 0.8, 0.2),     # Light Green\n",
    "    (0.6, 0.6, 0.3),     # Mustard\n",
    "    (0.3, 0.55, 0.55),    # Teal\n",
    "    (0.8, 0.5, 0.2),     # Dark Salmon\n",
    "    (0.5, 0.5, 0.5),     # Gray\n",
    "    (0.2, 0.2, 0.2),      # Black-Gray\n",
    "    (0.0, 0.0, 0.0)      # Black\n",
    "]\n",
    "\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7', 'x8']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_names_long)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'test_R2_plot_long_random_full_embedding.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the R2 values in order (bigger to smaller) for x_global\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "\n",
    "embeddings_names = ['x', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']\n",
    "\n",
    "#sort the R2 values for x_global\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x_global', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for x6\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "\n",
    "#sort the R2 values for x6\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x6', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for x7\n",
    "\n",
    "#sort the R2 values for x7\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x7', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for x8\n",
    "\n",
    "#sort the R2 values for x8\n",
    "r2_values = []\n",
    "for property_name in property_names_long:\n",
    "    r2_values.append(results[('x8', property_name)][3])\n",
    "\n",
    "r2_values_sorted = sorted(r2_values, reverse=True)\n",
    "print(r2_values_sorted)\n",
    "\n",
    "#sort the property names\n",
    "property_names_long_sorted = [x for _, x in sorted(zip(r2_values, property_names_long), reverse=True)]\n",
    "print(property_names_long_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node embedding probing\n",
    "\n",
    "/!\\ Try to not forget that we need to change the batch_size to 1 if we want to probe for node properties as we need the forward pass to be made 1 graph by 1 graph at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset_MDD()\n",
    "import torch\n",
    "torch.manual_seed(37)\n",
    "MODEL = \"GIN\"\n",
    "DATASET = \"FC_MDD\"\n",
    "from models.models_FC import GIN_framework as framework # import the model\n",
    "gnn = framework(dataset)\n",
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\", map_location='cpu')\n",
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "def compute_node_properties(data, indices):\n",
    "    properties = []\n",
    "    for idx in indices:\n",
    "        graph_data = data[idx]\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        \n",
    "        # Add all nodes to the graph to handle disconnected nodes\n",
    "        all_nodes = set(range(len(graph_data.x)))\n",
    "        connected_nodes = set(G.nodes())\n",
    "        disconnected_nodes = all_nodes - connected_nodes\n",
    "        \n",
    "        # Calculate node properties using NetworkX for connected nodes\n",
    "        degree = dict(G.degree())\n",
    "        clustering = nx.clustering(G)\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        closeness = nx.closeness_centrality(G)\n",
    "        eigenvector = nx.eigenvector_centrality(G, max_iter=10000)\n",
    "        pagerank = nx.pagerank(G)\n",
    "\n",
    "        # Initialize properties with zeros for all nodes\n",
    "        node_properties = [{'degree': 0, 'clustering': 0, 'betweenness': 0, 'closeness': 0, 'eigenvector': 0, 'pagerank': 0} for _ in all_nodes]\n",
    "        \n",
    "        # Store properties for each connected node in the graph\n",
    "        for node in connected_nodes:\n",
    "            node_properties[node] = {\n",
    "                'degree': degree[node],\n",
    "                'clustering': clustering[node],\n",
    "                'betweenness': betweenness[node],\n",
    "                'closeness': closeness[node],\n",
    "                'eigenvector': eigenvector[node],\n",
    "                'pagerank': pagerank[node]\n",
    "            }\n",
    "\n",
    "        #if there a disconnected nodes : print \n",
    "        # if disconnected_nodes:\n",
    "        #     print(f\"Graph {idx}: Disconnected nodes: {disconnected_nodes}\")\n",
    "        #     print(node_properties)\n",
    "\n",
    "        properties.append(node_properties)\n",
    "    return properties\n",
    "\n",
    "# Ensure gnn.train_idx and gnn.test_idx are lists of integers\n",
    "train_idx = gnn.train_idx.tolist() if isinstance(gnn.train_idx, torch.Tensor) else gnn.train_idx\n",
    "test_idx = gnn.test_idx.tolist() if isinstance(gnn.test_idx, torch.Tensor) else gnn.test_idx\n",
    "\n",
    "# Compute node-level properties for train and test sets\n",
    "train_node_properties = compute_node_properties(gnn.dataset, train_idx)\n",
    "test_node_properties = compute_node_properties(gnn.dataset, test_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2(return_node_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the first graph's features\n",
    "first_graph_features = train_features[1]\n",
    "for i, feature in enumerate(first_graph_features):\n",
    "    print(f\"Feature {i+1} shape:\", feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[1][0][115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_features))\n",
    "print(len(train_features[0]))\n",
    "print(train_features[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_node_properties))\n",
    "print(len(train_node_properties[0]))\n",
    "print(train_node_properties[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier\n",
    "\n",
    "**Probe**\n",
    "\n",
    "Example data structure for multiple graphs\n",
    "\n",
    "train_features: List of graphs, each with multiple layers of features\n",
    "\n",
    "```plaintext\n",
    "train_features = [\n",
    "    [np.array([...]), np.array([...]), ...],  # Graph 1: features for each layer\n",
    "    [np.array([...]), np.array([...]), ...],  # Graph 2: features for each layer\n",
    "    ...\n",
    "]\n",
    "```\n",
    "train_node_properties: List of graphs, each with a list of node properties\n",
    "\n",
    "```plaintext\n",
    "train_node_properties = [\n",
    "    [{'degree': ..., 'clustering': ..., ...}, {'degree': ..., ...}, ...],  # Graph 1: properties for each node\n",
    "    [{'degree': ..., 'clustering': ..., ...}, {'degree': ..., ...}, ...],  # Graph 2: properties for each node\n",
    "    ...\n",
    "]\n",
    "```\n",
    "test_features and test_node_properties would be similarly structured for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probe_for_layer(features, property_values, test_features, test_property_values, num_epochs=10000, learning_rate=0.01):\n",
    "    # Convert features and property values to PyTorch tensors if they are NumPy arrays\n",
    "    if isinstance(features, np.ndarray):\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "    if isinstance(property_values, np.ndarray):\n",
    "        property_values = torch.tensor(property_values, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(test_features, np.ndarray):\n",
    "        test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "    if isinstance(test_property_values, np.ndarray):\n",
    "        test_property_values = torch.tensor(test_property_values, dtype=torch.float32)\n",
    "\n",
    "    print(f\"Training on features with shape: {features.shape} for property values shape: {property_values.shape}\")\n",
    "\n",
    "    model = LinearModel(features.shape[1])  # Features should be 2D: (num_nodes, feature_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features).squeeze()  # Remove single-dimensional entries\n",
    "        loss = criterion(output, property_values)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(test_features).squeeze()\n",
    "        mse = criterion(pred, test_property_values).item()\n",
    "        # Flatten the tensors for proper use of r2_score\n",
    "        r2 = r2_score(test_property_values.cpu().numpy(), pred.cpu().numpy())\n",
    "\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Option 2: Train a Single Classifier Across All Graphs\n",
    "\n",
    "This approach involves combining data from all graphs to train a single probe for each property across the graphs. This assumes that the properties across different graphs share some common structure that can be captured by a single model. We modify the `evaluate_layer_probes` function to aggregate features and properties across all graphs before training.\n",
    "\n",
    "This approach results in a single probe being trained for each property at each layer, but the probe is trained on data aggregated from all graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_layer_probes_across_graphs(train_features_list, test_features_list, train_properties_list, test_properties_list):\n",
    "    num_layers = len(train_features_list[0])  # Assuming all graphs have the same number of layers\n",
    "    results = []\n",
    "\n",
    "    num_test_graphs = len(test_features_list)  # Number of graphs in the test set\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        combined_train_features = []\n",
    "        combined_test_features = []\n",
    "        combined_train_properties = []\n",
    "        combined_test_properties = []\n",
    "\n",
    "        # Aggregate features and properties across all graphs\n",
    "        for graph_idx in range(len(train_features_list)):\n",
    "            combined_train_features.append(np.vstack(train_features_list[graph_idx][layer_idx]))\n",
    "\n",
    "            # Use modulo to cycle through the test graphs\n",
    "            test_idx = graph_idx % num_test_graphs\n",
    "            combined_test_features.append(np.vstack(test_features_list[test_idx][layer_idx]))\n",
    "\n",
    "            combined_train_properties.extend(train_properties_list[graph_idx])\n",
    "            combined_test_properties.extend(test_properties_list[test_idx])\n",
    "\n",
    "        combined_train_features = np.vstack(combined_train_features)  # Combine features across graphs\n",
    "        combined_test_features = np.vstack(combined_test_features)\n",
    "\n",
    "        # Train and evaluate probe for each property across all graphs\n",
    "        for property_name in combined_train_properties[0].keys():  # Assuming all nodes have the same properties\n",
    "            train_property_values = np.array([node[property_name] for node in combined_train_properties])\n",
    "            test_property_values = np.array([node[property_name] for node in combined_test_properties])\n",
    "\n",
    "            mse, r2 = train_probe_for_layer(combined_train_features, train_property_values, combined_test_features, test_property_values)\n",
    "            \n",
    "            results.append({\n",
    "                'layer': layer_idx,\n",
    "                'property': property_name,\n",
    "                'mse': mse,\n",
    "                'r2': r2\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_across_graphs = evaluate_layer_probes_across_graphs(train_features, test_features, train_node_properties, test_node_properties)\n",
    "\n",
    "import pickle as pkl\n",
    "#save the results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_node_results_across_graphs.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results_across_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_node_results_across_graphs.pkl\", \"rb\") as f:\n",
    "    results_across_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Between Options:\n",
    "\n",
    "- If our graphs are very similar in nature and you expect the relationships between node embeddings and their properties to be consistent across all graphs, **Option 2** (Single Classifier Across Graphs) will be the better choice.\n",
    "  \n",
    "- If our graphs are diverse, or we expect the relationships to vary significantly between graphs, *Option 1* (Separate Classifiers) might be more appropriate.\n",
    "\n",
    "We experiment with both approaches and compare the performance to see which one gives us the best results : we decided to keep option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "- *Layers and Properties*: The function iterate over the layers and properties to aggregate and visualize the R scores.\n",
    "  \n",
    "- *Mean R Calculation*: The mean R scores are calculated for each layer and property. If any R value is below `-0.05`, it is set to `-0.05` to prevent extreme values from skewing the visualization.\n",
    "\n",
    "- *Plotting*:\n",
    "  - *Option 1 (`plot_results_per_graph`)*: Plots a separate line for each graph, allowing you to see how the R scores vary across layers and graphs.\n",
    "  - *Option 2 (`plot_results_across_graphs`)*: Plots a single line for each property, aggregating the results across all graphs. This provides a high-level view of how each property behaves across layers when considering all graphs together.\n",
    "  \n",
    "We decided to keep option 2 as it makes more sense. This function is designed to visualize the results from the `evaluate_layer_probes_across_graphs` function. It will plot the R scores for each property across layers, combining the results from all graphs into single lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_results_across_graphs(results):\n",
    "    layers = sorted(set(result['layer'] for result in results))\n",
    "    properties = sorted(set(result['property'] for result in results))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))  # Increase the figure size for better readability\n",
    "    \n",
    "    for property_name in properties:\n",
    "        r2_scores = []\n",
    "        for layer in layers:\n",
    "            layer_results = [r for r in results if r['layer'] == layer and r['property'] == property_name]\n",
    "            # Calculate mean R score for the layer\n",
    "            mean_r2 = np.mean([r['r2'] for r in layer_results])\n",
    "            # Set any R value below -0.05 to -0.05\n",
    "            if mean_r2 < -0.05:\n",
    "                mean_r2 = -0.05\n",
    "            r2_scores.append(mean_r2)\n",
    "        \n",
    "        # Plot the R scores with crosses and lines, one line per property across all graphs\n",
    "        plt.plot(layers, r2_scores, marker='x', linestyle='-', label=property_name)\n",
    "\n",
    "    plt.title('R Scores Across Layers for Different Properties (Aggregated Across Graphs)')\n",
    "    plt.xticks(layers, [f'Layer {i+1}' for i in layers])  # Set the x-axis labels to layer numbers\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('R Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)  # Add grid for better visibility of points and lines\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    with open(\"results/\"+DATASET+\"_\"+MODEL+\"_node_results_across_graphs.pkl\", \"wb\") as f:\n",
    "        pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results across all graphs\n",
    "plot_results_across_graphs(results_across_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
