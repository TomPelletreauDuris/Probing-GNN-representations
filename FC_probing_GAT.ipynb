{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll first be loading the FC matrices and explore their structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using read_dataset from Datasets/FC/create_dataset.py to read the dataset\n",
    "from Datasets.FC.create_dataset import read_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[116, 116], edge_index=[2, 1016], edge_attr=[1016, 1], y=[1])\n",
      "['y', 'edge_index', 'x', 'edge_attr']\n",
      "ValuesView({'x': tensor([[ 0.0000,  0.4543,  0.2477,  ...,  0.1753,  0.2247, -0.1751],\n",
      "        [ 0.4543,  0.0000, -0.2204,  ..., -0.1947, -0.2258, -0.1434],\n",
      "        [ 0.2477, -0.2204,  0.0000,  ..., -0.0521, -0.0804, -0.2025],\n",
      "        ...,\n",
      "        [ 0.1753, -0.1947, -0.0521,  ...,  0.0000,  0.6875, -0.1364],\n",
      "        [ 0.2247, -0.2258, -0.0804,  ...,  0.6875,  0.0000,  0.0929],\n",
      "        [-0.1751, -0.1434, -0.2025,  ..., -0.1364,  0.0929,  0.0000]]), 'edge_index': tensor([[  0,   0,   0,  ..., 114, 115, 115],\n",
      "        [  1,  10,  12,  ..., 113,  94, 109]]), 'edge_attr': tensor([[0.4543],\n",
      "        [0.5913],\n",
      "        [0.4224],\n",
      "        ...,\n",
      "        [0.6875],\n",
      "        [0.4846],\n",
      "        [0.5437]]), 'y': tensor([0])})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1099"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok, let's explore the data a bit more\n",
    "#dataset is a list object of torch_geometric.data objects\n",
    "\n",
    "#let's see the first element\n",
    "print(dataset[0])\n",
    "\n",
    "#it's a dictionary object, let's see the keys\n",
    "print(dataset[0].keys())\n",
    "\n",
    "#ok, let's see the values\n",
    "print(dataset[0].values())\n",
    "\n",
    "#it has 4 keys, 'x', 'edge_index', 'edge_attr' and 'y' where y=0 menas the patient is healthy and y=1 means the patient has Autism Spectrum Disorder (ASD)\n",
    "\"\"\"graph = Data(x=ROI.reshape(-1,116).float(),\n",
    "                     edge_index=G.indices().reshape(2,-1).long(),\n",
    "                     edge_attr=G.values().reshape(-1,1).float(),\n",
    "                     y=y.long())\"\"\"\n",
    "\n",
    "#how much data do we have?\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): GATConv(116, 128, heads=4)\n",
      "    (1-4): 4 x GATConv(512, 128, heads=4)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0-4): 5 x BatchNorm(512)\n",
      "  )\n",
      "  (lin1): Linear(512, 128, bias=True)\n",
      "  (lin2): Linear(128, 2, bias=True)\n",
      "  (bn1): BatchNorm(128)\n",
      "  (bn2): BatchNorm(2)\n",
      ")\n",
      "tensor([217, 137, 426,  ..., 643, 205, 434])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpelletreaudur/.local/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#set the seed\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "MODEL = \"GAT\"\n",
    "DATASET = \"FC\"\n",
    "\n",
    "from models.models_FC import GAT_framework as framework # import the model\n",
    "\n",
    "gnn = framework(dataset)\n",
    "\n",
    "print(gnn.model)\n",
    "print(gnn.train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.242, Test Loss: 0.690, Train Acc: 0.674 Test Acc: 0.436\n",
      "Epoch: 010, Loss: 0.171, Test Loss: 0.779, Train Acc: 0.942 Test Acc: 0.491\n",
      "Epoch: 015, Loss: 0.145, Test Loss: 0.722, Train Acc: 0.998 Test Acc: 0.564\n",
      "Epoch: 020, Loss: 0.125, Test Loss: 0.754, Train Acc: 0.999 Test Acc: 0.582\n",
      "Epoch: 025, Loss: 0.115, Test Loss: 0.761, Train Acc: 1.000 Test Acc: 0.564\n",
      "Epoch: 030, Loss: 0.116, Test Loss: 0.819, Train Acc: 0.999 Test Acc: 0.491\n",
      "Epoch: 035, Loss: 0.111, Test Loss: 0.755, Train Acc: 0.999 Test Acc: 0.545\n",
      "Epoch: 040, Loss: 0.100, Test Loss: 0.747, Train Acc: 1.000 Test Acc: 0.582\n",
      "Epoch: 045, Loss: 0.101, Test Loss: 0.795, Train Acc: 0.999 Test Acc: 0.582\n",
      "Epoch: 050, Loss: 0.093, Test Loss: 0.770, Train Acc: 1.000 Test Acc: 0.564\n",
      "Epoch: 055, Loss: 0.090, Test Loss: 0.745, Train Acc: 1.000 Test Acc: 0.582\n",
      "Epoch: 060, Loss: 0.089, Test Loss: 0.750, Train Acc: 0.998 Test Acc: 0.564\n",
      "Epoch: 065, Loss: 0.082, Test Loss: 0.752, Train Acc: 1.000 Test Acc: 0.582\n",
      "Epoch: 070, Loss: 0.077, Test Loss: 0.758, Train Acc: 1.000 Test Acc: 0.564\n",
      "Epoch: 075, Loss: 0.075, Test Loss: 0.755, Train Acc: 1.000 Test Acc: 0.545\n",
      "Epoch: 080, Loss: 0.075, Test Loss: 0.797, Train Acc: 1.000 Test Acc: 0.527\n",
      "Epoch: 085, Loss: 0.070, Test Loss: 0.755, Train Acc: 1.000 Test Acc: 0.545\n",
      "Epoch: 090, Loss: 0.068, Test Loss: 0.764, Train Acc: 0.999 Test Acc: 0.564\n",
      "Epoch: 095, Loss: 0.065, Test Loss: 0.794, Train Acc: 1.000 Test Acc: 0.582\n",
      "Epoch: 100, Loss: 0.064, Test Loss: 0.798, Train Acc: 1.000 Test Acc: 0.600\n",
      "Epoch: 105, Loss: 0.061, Test Loss: 0.736, Train Acc: 1.000 Test Acc: 0.545\n",
      "Epoch: 110, Loss: 0.060, Test Loss: 0.756, Train Acc: 1.000 Test Acc: 0.618\n",
      "Epoch: 115, Loss: 0.059, Test Loss: 0.763, Train Acc: 1.000 Test Acc: 0.582\n",
      "Epoch: 120, Loss: 0.116, Test Loss: 0.817, Train Acc: 0.995 Test Acc: 0.527\n",
      "Epoch: 125, Loss: 0.070, Test Loss: 0.809, Train Acc: 0.999 Test Acc: 0.618\n",
      "Epoch: 130, Loss: 0.057, Test Loss: 0.821, Train Acc: 1.000 Test Acc: 0.582\n",
      "Epoch: 135, Loss: 0.052, Test Loss: 0.771, Train Acc: 0.999 Test Acc: 0.600\n",
      "Epoch: 140, Loss: 0.050, Test Loss: 0.803, Train Acc: 1.000 Test Acc: 0.618\n",
      "Epoch: 145, Loss: 0.048, Test Loss: 0.853, Train Acc: 1.000 Test Acc: 0.582\n",
      "Epoch: 150, Loss: 0.046, Test Loss: 0.819, Train Acc: 1.000 Test Acc: 0.600\n"
     ]
    }
   ],
   "source": [
    "gnn.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: models/FC_GATserver.pt\n"
     ]
    }
   ],
   "source": [
    "#save the model \n",
    "gnn.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.819, Train Acc: 1.000 Test Acc: 0.600\n"
     ]
    }
   ],
   "source": [
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1044, 55)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_features[0]))\n",
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Alternative metrics for disconnected graphs\n",
    "        # Option 1: Use the average path length of the largest connected component\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        num_edges = G.number_of_edges()\n",
    "        density = nx.density(G)\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        number_of_node_in_the_largest_fully_connected_component = len(max(nx.connected_components(G), key=len))\n",
    "        #small_world = nx.algorithms.smallworld.sigma(G)\n",
    "\n",
    "        properties.append((num_nodes, num_edges, density, avg_path_len, num_cliques, num_triangles, num_squares, number_of_node_in_the_largest_fully_connected_component)) #, small_world))\n",
    "    return properties\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties = compute_graph_properties(selected_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(116,\n",
       "  717,\n",
       "  0.10749625187406296,\n",
       "  2.6724137931034484,\n",
       "  263,\n",
       "  1735.0,\n",
       "  6.64946643069199,\n",
       "  116),\n",
       " (116,\n",
       "  473,\n",
       "  0.07091454272863568,\n",
       "  3.4743628185907047,\n",
       "  158,\n",
       "  708.0,\n",
       "  7.095858020333574,\n",
       "  116),\n",
       " (116,\n",
       "  997,\n",
       "  0.1494752623688156,\n",
       "  2.469265367316342,\n",
       "  343,\n",
       "  3591.0,\n",
       "  7.864813698203321,\n",
       "  116),\n",
       " (116,\n",
       "  892,\n",
       "  0.13373313343328336,\n",
       "  2.874662668665667,\n",
       "  144,\n",
       "  3409.0,\n",
       "  10.613595627764592,\n",
       "  116),\n",
       " (116,\n",
       "  504,\n",
       "  0.07556221889055473,\n",
       "  3.1734202763546033,\n",
       "  155,\n",
       "  861.0,\n",
       "  6.351955841015705,\n",
       "  114)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_properties))\n",
    "train_properties[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#save the properties in a file\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(train_properties, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(test_properties, f)\n",
    "\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"rb\") as f:\n",
    "    train_properties = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"rb\") as f:\n",
    "    test_properties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embeddings of GCN are like this:\n",
    "return F.log_softmax(x7, dim=-1), (x1, x2, x3, x4, x5, x_global, x6, x7)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "output_size = 1  # Predicting one property at a time\n",
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Assume we have already evaluated to get features\n",
    "# train_features, test_features = gnn.evaluate_with_features2()\n",
    "\n",
    "# Extract x embeddings\n",
    "train_x = np.array([feat[0] for feat in train_features])\n",
    "test_x = np.array([feat[0] for feat in test_features])\n",
    "\n",
    "# Extract 2, 3, 4, global, 5, 6, 7 embeddings\n",
    "train_x2 = np.array([feat[1] for feat in train_features])\n",
    "test_x2 = np.array([feat[1] for feat in test_features])\n",
    "\n",
    "train_x3 = np.array([feat[2] for feat in train_features])\n",
    "test_x3 = np.array([feat[2] for feat in test_features])\n",
    "\n",
    "train_x4 = np.array([feat[3] for feat in train_features])\n",
    "test_x4 = np.array([feat[3] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[4] for feat in train_features])\n",
    "test_x5 = np.array([feat[4] for feat in test_features])\n",
    "\n",
    "train_x_global = np.array([feat[5] for feat in train_features])\n",
    "test_x_global = np.array([feat[5] for feat in test_features])\n",
    "\n",
    "train_x6 = np.array([feat[6] for feat in train_features])\n",
    "test_x6 = np.array([feat[6] for feat in test_features])\n",
    "\n",
    "train_x7 = np.array([feat[7] for feat in train_features])\n",
    "test_x7 = np.array([feat[7] for feat in test_features])\n",
    "\n",
    "# Compute graph properties\n",
    "# train_properties = compute_graph_properties(gnn.dataset[gnn.train_idx])\n",
    "# test_properties = compute_graph_properties(gnn.dataset[gnn.test_idx])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_x2 = torch.tensor(train_x2, dtype=torch.float32)\n",
    "train_x3 = torch.tensor(train_x3, dtype=torch.float32)\n",
    "train_x4 = torch.tensor(train_x4, dtype=torch.float32)\n",
    "train_x5 = torch.tensor(train_x5, dtype=torch.float32)\n",
    "train_x_global = torch.tensor(train_x_global, dtype=torch.float32)\n",
    "train_x6 = torch.tensor(train_x6, dtype=torch.float32)\n",
    "train_x7 = torch.tensor(train_x7, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x2 = torch.tensor(test_x2, dtype=torch.float32)\n",
    "test_x3 = torch.tensor(test_x3, dtype=torch.float32)\n",
    "test_x4 = torch.tensor(test_x4, dtype=torch.float32)\n",
    "test_x5 = torch.tensor(test_x5, dtype=torch.float32)\n",
    "test_x_global = torch.tensor(test_x_global, dtype=torch.float32)\n",
    "test_x6 = torch.tensor(test_x6, dtype=torch.float32)\n",
    "test_x7 = torch.tensor(test_x7, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_properties, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_properties, dtype=torch.float32)\n",
    "\n",
    "# Train and evaluate a model for each graph property and each embedding\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component']\n",
    "embeddings = [(train_x, test_x), (train_x2, test_x2), (train_x3, test_x3), (train_x4, test_x4), (train_x5, test_x5), (train_x_global, test_x_global), (train_x6, test_x6), (train_x7, test_x7)]\n",
    "embeddings_names = ['x', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/800000], Property: num_nodes, Loss: 6849.4170\n",
      "Epoch [2000/800000], Property: num_nodes, Loss: 4051.0732\n",
      "Epoch [3000/800000], Property: num_nodes, Loss: 2482.5432\n",
      "Epoch [4000/800000], Property: num_nodes, Loss: 1560.6089\n",
      "Epoch [5000/800000], Property: num_nodes, Loss: 1022.4446\n",
      "Epoch [6000/800000], Property: num_nodes, Loss: 709.7937\n",
      "Epoch [7000/800000], Property: num_nodes, Loss: 522.1395\n",
      "Epoch [8000/800000], Property: num_nodes, Loss: 399.9267\n",
      "Epoch [9000/800000], Property: num_nodes, Loss: 313.9452\n",
      "Epoch [10000/800000], Property: num_nodes, Loss: 250.6084\n",
      "Epoch [11000/800000], Property: num_nodes, Loss: 203.2401\n",
      "Epoch [12000/800000], Property: num_nodes, Loss: 167.6162\n",
      "Epoch [13000/800000], Property: num_nodes, Loss: 140.4022\n",
      "Epoch [14000/800000], Property: num_nodes, Loss: 119.2704\n",
      "Epoch [15000/800000], Property: num_nodes, Loss: 102.8279\n",
      "Epoch [16000/800000], Property: num_nodes, Loss: 90.1296\n",
      "Epoch [17000/800000], Property: num_nodes, Loss: 80.4074\n",
      "Epoch [18000/800000], Property: num_nodes, Loss: 72.9827\n",
      "Epoch [19000/800000], Property: num_nodes, Loss: 67.2926\n",
      "Epoch [20000/800000], Property: num_nodes, Loss: 62.9017\n",
      "Epoch [21000/800000], Property: num_nodes, Loss: 59.4708\n",
      "Epoch [22000/800000], Property: num_nodes, Loss: 56.7404\n",
      "Epoch [23000/800000], Property: num_nodes, Loss: 54.5249\n",
      "Epoch [24000/800000], Property: num_nodes, Loss: 52.6878\n",
      "Epoch [25000/800000], Property: num_nodes, Loss: 51.1188\n",
      "Epoch [26000/800000], Property: num_nodes, Loss: 49.7361\n",
      "Epoch [27000/800000], Property: num_nodes, Loss: 48.4845\n",
      "Epoch [28000/800000], Property: num_nodes, Loss: 47.3264\n",
      "Epoch [29000/800000], Property: num_nodes, Loss: 46.2344\n",
      "Epoch [30000/800000], Property: num_nodes, Loss: 45.1895\n",
      "Epoch [31000/800000], Property: num_nodes, Loss: 44.1802\n",
      "Epoch [32000/800000], Property: num_nodes, Loss: 43.1999\n",
      "Epoch [33000/800000], Property: num_nodes, Loss: 42.2452\n",
      "Epoch [34000/800000], Property: num_nodes, Loss: 41.3114\n",
      "Epoch [35000/800000], Property: num_nodes, Loss: 40.3951\n",
      "Epoch [36000/800000], Property: num_nodes, Loss: 39.4925\n",
      "Epoch [37000/800000], Property: num_nodes, Loss: 38.6000\n",
      "Epoch [38000/800000], Property: num_nodes, Loss: 37.7183\n",
      "Epoch [39000/800000], Property: num_nodes, Loss: 36.8471\n",
      "Epoch [40000/800000], Property: num_nodes, Loss: 35.9865\n",
      "Epoch [41000/800000], Property: num_nodes, Loss: 35.1364\n",
      "Epoch [42000/800000], Property: num_nodes, Loss: 34.2967\n",
      "Epoch [43000/800000], Property: num_nodes, Loss: 33.4676\n",
      "Epoch [44000/800000], Property: num_nodes, Loss: 32.6488\n",
      "Epoch [45000/800000], Property: num_nodes, Loss: 31.8406\n",
      "Epoch [46000/800000], Property: num_nodes, Loss: 31.0428\n",
      "Epoch [47000/800000], Property: num_nodes, Loss: 30.2554\n",
      "Epoch [48000/800000], Property: num_nodes, Loss: 29.4786\n",
      "Epoch [49000/800000], Property: num_nodes, Loss: 28.7122\n",
      "Epoch [50000/800000], Property: num_nodes, Loss: 27.9564\n",
      "Epoch [51000/800000], Property: num_nodes, Loss: 27.2129\n",
      "Epoch [52000/800000], Property: num_nodes, Loss: 26.4807\n",
      "Epoch [53000/800000], Property: num_nodes, Loss: 25.7588\n",
      "Epoch [54000/800000], Property: num_nodes, Loss: 25.0474\n",
      "Epoch [55000/800000], Property: num_nodes, Loss: 24.3463\n",
      "Epoch [56000/800000], Property: num_nodes, Loss: 23.6557\n",
      "Epoch [57000/800000], Property: num_nodes, Loss: 22.9754\n",
      "Epoch [58000/800000], Property: num_nodes, Loss: 22.3055\n",
      "Epoch [59000/800000], Property: num_nodes, Loss: 21.6460\n",
      "Epoch [60000/800000], Property: num_nodes, Loss: 20.9968\n",
      "Epoch [61000/800000], Property: num_nodes, Loss: 20.3581\n",
      "Epoch [62000/800000], Property: num_nodes, Loss: 19.7297\n",
      "Epoch [63000/800000], Property: num_nodes, Loss: 19.1118\n",
      "Epoch [64000/800000], Property: num_nodes, Loss: 18.5042\n",
      "Epoch [65000/800000], Property: num_nodes, Loss: 17.9070\n",
      "Epoch [66000/800000], Property: num_nodes, Loss: 17.3205\n",
      "Epoch [67000/800000], Property: num_nodes, Loss: 16.7462\n",
      "Epoch [68000/800000], Property: num_nodes, Loss: 16.1822\n",
      "Epoch [69000/800000], Property: num_nodes, Loss: 15.6286\n",
      "Epoch [70000/800000], Property: num_nodes, Loss: 15.0854\n",
      "Epoch [71000/800000], Property: num_nodes, Loss: 14.5518\n",
      "Epoch [72000/800000], Property: num_nodes, Loss: 14.0290\n",
      "Epoch [73000/800000], Property: num_nodes, Loss: 13.5185\n",
      "Epoch [74000/800000], Property: num_nodes, Loss: 13.0183\n",
      "Epoch [75000/800000], Property: num_nodes, Loss: 12.5283\n",
      "Epoch [76000/800000], Property: num_nodes, Loss: 12.0486\n",
      "Epoch [77000/800000], Property: num_nodes, Loss: 11.5791\n",
      "Epoch [78000/800000], Property: num_nodes, Loss: 11.1199\n",
      "Epoch [79000/800000], Property: num_nodes, Loss: 10.6708\n",
      "Epoch [80000/800000], Property: num_nodes, Loss: 10.2321\n",
      "Epoch [81000/800000], Property: num_nodes, Loss: 9.8034\n",
      "Epoch [82000/800000], Property: num_nodes, Loss: 9.3850\n",
      "Epoch [83000/800000], Property: num_nodes, Loss: 8.9769\n",
      "Epoch [84000/800000], Property: num_nodes, Loss: 8.5790\n",
      "Epoch [85000/800000], Property: num_nodes, Loss: 8.1913\n",
      "Epoch [86000/800000], Property: num_nodes, Loss: 7.8142\n",
      "Epoch [87000/800000], Property: num_nodes, Loss: 7.4496\n",
      "Epoch [88000/800000], Property: num_nodes, Loss: 7.0952\n",
      "Epoch [89000/800000], Property: num_nodes, Loss: 6.7510\n",
      "Epoch [90000/800000], Property: num_nodes, Loss: 6.4168\n",
      "Epoch [91000/800000], Property: num_nodes, Loss: 6.0926\n",
      "Epoch [92000/800000], Property: num_nodes, Loss: 5.7785\n",
      "Epoch [93000/800000], Property: num_nodes, Loss: 5.4745\n",
      "Epoch [94000/800000], Property: num_nodes, Loss: 5.1805\n",
      "Epoch [95000/800000], Property: num_nodes, Loss: 4.8985\n",
      "Epoch [96000/800000], Property: num_nodes, Loss: 4.6269\n",
      "Epoch [97000/800000], Property: num_nodes, Loss: 4.3649\n",
      "Epoch [98000/800000], Property: num_nodes, Loss: 4.1129\n",
      "Epoch [99000/800000], Property: num_nodes, Loss: 3.8709\n",
      "Epoch [100000/800000], Property: num_nodes, Loss: 3.6397\n",
      "Epoch [101000/800000], Property: num_nodes, Loss: 3.4192\n",
      "Epoch [102000/800000], Property: num_nodes, Loss: 3.2084\n",
      "Epoch [103000/800000], Property: num_nodes, Loss: 3.0073\n",
      "Epoch [104000/800000], Property: num_nodes, Loss: 2.8172\n",
      "Epoch [105000/800000], Property: num_nodes, Loss: 2.6370\n",
      "Epoch [106000/800000], Property: num_nodes, Loss: 2.4665\n",
      "Epoch [107000/800000], Property: num_nodes, Loss: 2.3067\n",
      "Epoch [108000/800000], Property: num_nodes, Loss: 2.1564\n",
      "Epoch [109000/800000], Property: num_nodes, Loss: 2.0165\n",
      "Epoch [110000/800000], Property: num_nodes, Loss: 1.8863\n",
      "Epoch [111000/800000], Property: num_nodes, Loss: 1.7662\n",
      "Epoch [112000/800000], Property: num_nodes, Loss: 1.6557\n",
      "Epoch [113000/800000], Property: num_nodes, Loss: 1.5550\n",
      "Epoch [114000/800000], Property: num_nodes, Loss: 1.4640\n",
      "Epoch [115000/800000], Property: num_nodes, Loss: 1.3826\n",
      "Epoch [116000/800000], Property: num_nodes, Loss: 1.3103\n",
      "Epoch [117000/800000], Property: num_nodes, Loss: 1.2474\n",
      "Epoch [118000/800000], Property: num_nodes, Loss: 1.1936\n",
      "Epoch [119000/800000], Property: num_nodes, Loss: 1.1487\n",
      "Epoch [120000/800000], Property: num_nodes, Loss: 1.1122\n",
      "Epoch [121000/800000], Property: num_nodes, Loss: 1.0838\n",
      "Epoch [122000/800000], Property: num_nodes, Loss: 1.0629\n",
      "Epoch [123000/800000], Property: num_nodes, Loss: 1.0486\n",
      "Epoch [124000/800000], Property: num_nodes, Loss: 1.0400\n",
      "Epoch [125000/800000], Property: num_nodes, Loss: 1.0354\n",
      "Epoch [126000/800000], Property: num_nodes, Loss: 1.0335\n",
      "Epoch [127000/800000], Property: num_nodes, Loss: 1.0330\n",
      "Epoch [128000/800000], Property: num_nodes, Loss: 1.0326\n",
      "Epoch [129000/800000], Property: num_nodes, Loss: 1.0326\n",
      "Epoch [130000/800000], Property: num_nodes, Loss: 1.0325\n",
      "Epoch [131000/800000], Property: num_nodes, Loss: 1.0326\n",
      "Epoch [132000/800000], Property: num_nodes, Loss: 1.0325\n",
      "Epoch [133000/800000], Property: num_nodes, Loss: 1.0326\n",
      "Epoch [134000/800000], Property: num_nodes, Loss: 1.0325\n",
      "Epoch [135000/800000], Property: num_nodes, Loss: 1.0325\n",
      "Early stopping at epoch 135881\n",
      "Embedding: torch.Size([1044, 512])\n",
      "Property: num_nodes\n",
      "  Train MSE: 1.0325, Test MSE: 12.7570\n",
      "  Train R²: 0.4644, Test R²: -13.4858\n",
      "Epoch [1000/800000], Property: num_edges, Loss: 247876.1406\n",
      "Epoch [2000/800000], Property: num_edges, Loss: 214950.4688\n",
      "Epoch [3000/800000], Property: num_edges, Loss: 188477.7031\n",
      "Epoch [4000/800000], Property: num_edges, Loss: 166425.1094\n",
      "Epoch [5000/800000], Property: num_edges, Loss: 147606.7344\n",
      "Epoch [6000/800000], Property: num_edges, Loss: 131284.5938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7000/800000], Property: num_edges, Loss: 116984.3594\n",
      "Epoch [8000/800000], Property: num_edges, Loss: 104401.4297\n",
      "Epoch [9000/800000], Property: num_edges, Loss: 93330.4219\n",
      "Epoch [10000/800000], Property: num_edges, Loss: 83605.0234\n",
      "Epoch [11000/800000], Property: num_edges, Loss: 75077.3672\n",
      "Epoch [12000/800000], Property: num_edges, Loss: 67610.7188\n",
      "Epoch [13000/800000], Property: num_edges, Loss: 61067.6016\n",
      "Epoch [14000/800000], Property: num_edges, Loss: 55336.3359\n",
      "Epoch [15000/800000], Property: num_edges, Loss: 50342.0664\n",
      "Epoch [16000/800000], Property: num_edges, Loss: 46019.8945\n",
      "Epoch [17000/800000], Property: num_edges, Loss: 42299.3945\n",
      "Epoch [18000/800000], Property: num_edges, Loss: 39105.8281\n",
      "Epoch [19000/800000], Property: num_edges, Loss: 36374.8047\n",
      "Epoch [20000/800000], Property: num_edges, Loss: 34045.5430\n",
      "Epoch [21000/800000], Property: num_edges, Loss: 32056.3418\n",
      "Epoch [22000/800000], Property: num_edges, Loss: 30353.0625\n",
      "Epoch [23000/800000], Property: num_edges, Loss: 28889.0508\n",
      "Epoch [24000/800000], Property: num_edges, Loss: 27621.6758\n",
      "Epoch [25000/800000], Property: num_edges, Loss: 26509.5508\n",
      "Epoch [26000/800000], Property: num_edges, Loss: 25523.4004\n",
      "Epoch [27000/800000], Property: num_edges, Loss: 24644.4922\n",
      "Epoch [28000/800000], Property: num_edges, Loss: 23854.8613\n",
      "Epoch [29000/800000], Property: num_edges, Loss: 23139.8906\n",
      "Epoch [30000/800000], Property: num_edges, Loss: 22488.5801\n",
      "Epoch [31000/800000], Property: num_edges, Loss: 21893.7773\n",
      "Epoch [32000/800000], Property: num_edges, Loss: 21350.4023\n",
      "Epoch [33000/800000], Property: num_edges, Loss: 20853.4941\n",
      "Epoch [34000/800000], Property: num_edges, Loss: 20398.3340\n",
      "Epoch [35000/800000], Property: num_edges, Loss: 19980.2383\n",
      "Epoch [36000/800000], Property: num_edges, Loss: 19594.3770\n",
      "Epoch [37000/800000], Property: num_edges, Loss: 19236.6113\n",
      "Epoch [38000/800000], Property: num_edges, Loss: 18904.0703\n",
      "Epoch [39000/800000], Property: num_edges, Loss: 18594.8652\n",
      "Epoch [40000/800000], Property: num_edges, Loss: 18307.4375\n",
      "Epoch [41000/800000], Property: num_edges, Loss: 18039.9688\n",
      "Epoch [42000/800000], Property: num_edges, Loss: 17790.6230\n",
      "Epoch [43000/800000], Property: num_edges, Loss: 17557.3184\n",
      "Epoch [44000/800000], Property: num_edges, Loss: 17337.9453\n",
      "Epoch [45000/800000], Property: num_edges, Loss: 17130.7852\n",
      "Epoch [46000/800000], Property: num_edges, Loss: 16934.4336\n",
      "Epoch [47000/800000], Property: num_edges, Loss: 16747.7090\n",
      "Epoch [48000/800000], Property: num_edges, Loss: 16569.8438\n",
      "Epoch [49000/800000], Property: num_edges, Loss: 16400.2012\n",
      "Epoch [50000/800000], Property: num_edges, Loss: 16238.1377\n",
      "Epoch [51000/800000], Property: num_edges, Loss: 16083.1572\n",
      "Epoch [52000/800000], Property: num_edges, Loss: 15934.8457\n",
      "Epoch [53000/800000], Property: num_edges, Loss: 15792.7686\n",
      "Epoch [54000/800000], Property: num_edges, Loss: 15656.5674\n",
      "Epoch [55000/800000], Property: num_edges, Loss: 15525.7588\n",
      "Epoch [56000/800000], Property: num_edges, Loss: 15399.9561\n",
      "Epoch [57000/800000], Property: num_edges, Loss: 15278.8662\n",
      "Epoch [58000/800000], Property: num_edges, Loss: 15162.1455\n",
      "Epoch [59000/800000], Property: num_edges, Loss: 15049.4854\n",
      "Epoch [60000/800000], Property: num_edges, Loss: 14940.6172\n",
      "Epoch [61000/800000], Property: num_edges, Loss: 14835.3545\n",
      "Epoch [62000/800000], Property: num_edges, Loss: 14733.5898\n",
      "Epoch [63000/800000], Property: num_edges, Loss: 14635.2090\n",
      "Epoch [64000/800000], Property: num_edges, Loss: 14540.1211\n",
      "Epoch [65000/800000], Property: num_edges, Loss: 14448.2314\n",
      "Epoch [66000/800000], Property: num_edges, Loss: 14359.4336\n",
      "Epoch [67000/800000], Property: num_edges, Loss: 14273.6143\n",
      "Epoch [68000/800000], Property: num_edges, Loss: 14190.6406\n",
      "Epoch [69000/800000], Property: num_edges, Loss: 14110.3955\n",
      "Epoch [70000/800000], Property: num_edges, Loss: 14032.7324\n",
      "Epoch [71000/800000], Property: num_edges, Loss: 13957.5293\n",
      "Epoch [72000/800000], Property: num_edges, Loss: 13884.6875\n",
      "Epoch [73000/800000], Property: num_edges, Loss: 13814.1338\n",
      "Epoch [74000/800000], Property: num_edges, Loss: 13745.8047\n",
      "Epoch [75000/800000], Property: num_edges, Loss: 13679.6445\n",
      "Epoch [76000/800000], Property: num_edges, Loss: 13615.5928\n",
      "Epoch [77000/800000], Property: num_edges, Loss: 13553.5840\n",
      "Epoch [78000/800000], Property: num_edges, Loss: 13493.5635\n",
      "Epoch [79000/800000], Property: num_edges, Loss: 13435.4561\n",
      "Epoch [80000/800000], Property: num_edges, Loss: 13379.1572\n",
      "Epoch [81000/800000], Property: num_edges, Loss: 13324.5762\n",
      "Epoch [82000/800000], Property: num_edges, Loss: 13271.6426\n",
      "Epoch [83000/800000], Property: num_edges, Loss: 13220.2920\n",
      "Epoch [84000/800000], Property: num_edges, Loss: 13170.4561\n",
      "Epoch [85000/800000], Property: num_edges, Loss: 13122.0615\n",
      "Epoch [86000/800000], Property: num_edges, Loss: 13075.0625\n",
      "Epoch [87000/800000], Property: num_edges, Loss: 13029.4141\n",
      "Epoch [88000/800000], Property: num_edges, Loss: 12985.0898\n",
      "Epoch [89000/800000], Property: num_edges, Loss: 12942.0459\n",
      "Epoch [90000/800000], Property: num_edges, Loss: 12900.1943\n",
      "Epoch [91000/800000], Property: num_edges, Loss: 12859.5508\n",
      "Epoch [92000/800000], Property: num_edges, Loss: 12820.0908\n",
      "Epoch [93000/800000], Property: num_edges, Loss: 12781.7812\n",
      "Epoch [94000/800000], Property: num_edges, Loss: 12744.5967\n",
      "Epoch [95000/800000], Property: num_edges, Loss: 12708.5166\n",
      "Epoch [96000/800000], Property: num_edges, Loss: 12673.5156\n",
      "Epoch [97000/800000], Property: num_edges, Loss: 12639.5752\n",
      "Epoch [98000/800000], Property: num_edges, Loss: 12606.6777\n",
      "Epoch [99000/800000], Property: num_edges, Loss: 12574.7979\n",
      "Epoch [100000/800000], Property: num_edges, Loss: 12543.8818\n",
      "Epoch [101000/800000], Property: num_edges, Loss: 12513.8936\n",
      "Epoch [102000/800000], Property: num_edges, Loss: 12484.7783\n",
      "Epoch [103000/800000], Property: num_edges, Loss: 12456.4766\n",
      "Epoch [104000/800000], Property: num_edges, Loss: 12428.9492\n",
      "Epoch [105000/800000], Property: num_edges, Loss: 12402.1777\n",
      "Epoch [106000/800000], Property: num_edges, Loss: 12376.1416\n",
      "Epoch [107000/800000], Property: num_edges, Loss: 12350.8252\n",
      "Epoch [108000/800000], Property: num_edges, Loss: 12326.2100\n",
      "Epoch [109000/800000], Property: num_edges, Loss: 12302.2764\n",
      "Epoch [110000/800000], Property: num_edges, Loss: 12279.0059\n",
      "Epoch [111000/800000], Property: num_edges, Loss: 12256.3701\n",
      "Epoch [112000/800000], Property: num_edges, Loss: 12234.3672\n",
      "Epoch [113000/800000], Property: num_edges, Loss: 12212.9717\n",
      "Epoch [114000/800000], Property: num_edges, Loss: 12192.1582\n",
      "Epoch [115000/800000], Property: num_edges, Loss: 12171.9033\n",
      "Epoch [116000/800000], Property: num_edges, Loss: 12152.1895\n",
      "Epoch [117000/800000], Property: num_edges, Loss: 12132.9883\n",
      "Epoch [118000/800000], Property: num_edges, Loss: 12114.2852\n",
      "Epoch [119000/800000], Property: num_edges, Loss: 12096.0664\n",
      "Epoch [120000/800000], Property: num_edges, Loss: 12078.3174\n",
      "Epoch [121000/800000], Property: num_edges, Loss: 12061.0254\n",
      "Epoch [122000/800000], Property: num_edges, Loss: 12044.1689\n",
      "Epoch [123000/800000], Property: num_edges, Loss: 12027.7295\n",
      "Epoch [124000/800000], Property: num_edges, Loss: 12011.6943\n",
      "Epoch [125000/800000], Property: num_edges, Loss: 11996.0498\n",
      "Epoch [126000/800000], Property: num_edges, Loss: 11980.7832\n",
      "Epoch [127000/800000], Property: num_edges, Loss: 11965.8887\n",
      "Epoch [128000/800000], Property: num_edges, Loss: 11951.3486\n",
      "Epoch [129000/800000], Property: num_edges, Loss: 11937.1475\n",
      "Epoch [130000/800000], Property: num_edges, Loss: 11923.2773\n",
      "Epoch [131000/800000], Property: num_edges, Loss: 11909.7246\n",
      "Epoch [132000/800000], Property: num_edges, Loss: 11896.4785\n",
      "Epoch [133000/800000], Property: num_edges, Loss: 11883.5264\n",
      "Epoch [134000/800000], Property: num_edges, Loss: 11870.8545\n",
      "Epoch [135000/800000], Property: num_edges, Loss: 11858.4590\n",
      "Epoch [136000/800000], Property: num_edges, Loss: 11846.3271\n",
      "Epoch [137000/800000], Property: num_edges, Loss: 11834.4570\n",
      "Epoch [138000/800000], Property: num_edges, Loss: 11822.8350\n",
      "Epoch [139000/800000], Property: num_edges, Loss: 11811.4668\n",
      "Epoch [140000/800000], Property: num_edges, Loss: 11800.3369\n",
      "Epoch [141000/800000], Property: num_edges, Loss: 11789.4561\n",
      "Epoch [142000/800000], Property: num_edges, Loss: 11778.8066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [143000/800000], Property: num_edges, Loss: 11768.3838\n",
      "Epoch [144000/800000], Property: num_edges, Loss: 11758.1816\n",
      "Epoch [145000/800000], Property: num_edges, Loss: 11748.1885\n",
      "Epoch [146000/800000], Property: num_edges, Loss: 11738.4014\n",
      "Epoch [147000/800000], Property: num_edges, Loss: 11728.8154\n",
      "Epoch [148000/800000], Property: num_edges, Loss: 11719.4248\n",
      "Epoch [149000/800000], Property: num_edges, Loss: 11710.2158\n",
      "Epoch [150000/800000], Property: num_edges, Loss: 11701.1992\n",
      "Epoch [151000/800000], Property: num_edges, Loss: 11692.3633\n",
      "Epoch [152000/800000], Property: num_edges, Loss: 11683.6963\n",
      "Epoch [153000/800000], Property: num_edges, Loss: 11675.1963\n",
      "Epoch [154000/800000], Property: num_edges, Loss: 11666.8555\n",
      "Epoch [155000/800000], Property: num_edges, Loss: 11658.6719\n",
      "Epoch [156000/800000], Property: num_edges, Loss: 11650.6416\n",
      "Epoch [157000/800000], Property: num_edges, Loss: 11642.7637\n",
      "Epoch [158000/800000], Property: num_edges, Loss: 11635.0234\n",
      "Epoch [159000/800000], Property: num_edges, Loss: 11627.4258\n",
      "Epoch [160000/800000], Property: num_edges, Loss: 11619.9629\n",
      "Epoch [161000/800000], Property: num_edges, Loss: 11612.6387\n",
      "Epoch [162000/800000], Property: num_edges, Loss: 11605.4443\n",
      "Epoch [163000/800000], Property: num_edges, Loss: 11598.3799\n",
      "Epoch [164000/800000], Property: num_edges, Loss: 11591.4502\n",
      "Epoch [165000/800000], Property: num_edges, Loss: 11584.6514\n",
      "Epoch [166000/800000], Property: num_edges, Loss: 11577.9746\n",
      "Epoch [167000/800000], Property: num_edges, Loss: 11571.4180\n",
      "Epoch [168000/800000], Property: num_edges, Loss: 11564.9785\n",
      "Epoch [169000/800000], Property: num_edges, Loss: 11558.6572\n",
      "Epoch [170000/800000], Property: num_edges, Loss: 11552.4502\n",
      "Epoch [171000/800000], Property: num_edges, Loss: 11546.3594\n",
      "Epoch [172000/800000], Property: num_edges, Loss: 11540.3828\n",
      "Epoch [173000/800000], Property: num_edges, Loss: 11534.5156\n",
      "Epoch [174000/800000], Property: num_edges, Loss: 11528.7539\n",
      "Epoch [175000/800000], Property: num_edges, Loss: 11523.0928\n",
      "Epoch [176000/800000], Property: num_edges, Loss: 11517.5293\n",
      "Epoch [177000/800000], Property: num_edges, Loss: 11512.0566\n",
      "Epoch [178000/800000], Property: num_edges, Loss: 11506.6729\n",
      "Epoch [179000/800000], Property: num_edges, Loss: 11501.3828\n",
      "Epoch [180000/800000], Property: num_edges, Loss: 11496.1797\n",
      "Epoch [181000/800000], Property: num_edges, Loss: 11491.0654\n",
      "Epoch [182000/800000], Property: num_edges, Loss: 11486.0361\n",
      "Epoch [183000/800000], Property: num_edges, Loss: 11481.0859\n",
      "Epoch [184000/800000], Property: num_edges, Loss: 11476.2168\n",
      "Epoch [185000/800000], Property: num_edges, Loss: 11471.4238\n",
      "Epoch [186000/800000], Property: num_edges, Loss: 11466.7021\n",
      "Epoch [187000/800000], Property: num_edges, Loss: 11462.0557\n",
      "Epoch [188000/800000], Property: num_edges, Loss: 11457.4814\n",
      "Epoch [189000/800000], Property: num_edges, Loss: 11452.9756\n",
      "Epoch [190000/800000], Property: num_edges, Loss: 11448.5410\n",
      "Epoch [191000/800000], Property: num_edges, Loss: 11444.1748\n",
      "Epoch [192000/800000], Property: num_edges, Loss: 11439.8750\n",
      "Epoch [193000/800000], Property: num_edges, Loss: 11435.6377\n",
      "Epoch [194000/800000], Property: num_edges, Loss: 11431.4678\n",
      "Epoch [195000/800000], Property: num_edges, Loss: 11427.3574\n",
      "Epoch [196000/800000], Property: num_edges, Loss: 11423.3066\n",
      "Epoch [197000/800000], Property: num_edges, Loss: 11419.3154\n",
      "Epoch [198000/800000], Property: num_edges, Loss: 11415.3740\n",
      "Epoch [199000/800000], Property: num_edges, Loss: 11411.4824\n",
      "Epoch [200000/800000], Property: num_edges, Loss: 11407.6357\n",
      "Epoch [201000/800000], Property: num_edges, Loss: 11403.8379\n",
      "Epoch [202000/800000], Property: num_edges, Loss: 11400.0820\n",
      "Epoch [203000/800000], Property: num_edges, Loss: 11396.3711\n",
      "Epoch [204000/800000], Property: num_edges, Loss: 11392.7012\n",
      "Epoch [205000/800000], Property: num_edges, Loss: 11389.0732\n",
      "Epoch [206000/800000], Property: num_edges, Loss: 11385.4814\n",
      "Epoch [207000/800000], Property: num_edges, Loss: 11381.9277\n",
      "Epoch [208000/800000], Property: num_edges, Loss: 11378.4082\n",
      "Epoch [209000/800000], Property: num_edges, Loss: 11374.9287\n",
      "Epoch [210000/800000], Property: num_edges, Loss: 11371.4844\n",
      "Epoch [211000/800000], Property: num_edges, Loss: 11368.0801\n",
      "Epoch [212000/800000], Property: num_edges, Loss: 11364.7109\n",
      "Epoch [213000/800000], Property: num_edges, Loss: 11361.3789\n",
      "Epoch [214000/800000], Property: num_edges, Loss: 11358.0811\n",
      "Epoch [215000/800000], Property: num_edges, Loss: 11354.8174\n",
      "Epoch [216000/800000], Property: num_edges, Loss: 11351.5918\n",
      "Epoch [217000/800000], Property: num_edges, Loss: 11348.4023\n",
      "Epoch [218000/800000], Property: num_edges, Loss: 11345.2500\n",
      "Epoch [219000/800000], Property: num_edges, Loss: 11342.1270\n",
      "Epoch [220000/800000], Property: num_edges, Loss: 11339.0439\n",
      "Epoch [221000/800000], Property: num_edges, Loss: 11335.9961\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m target \u001b[38;5;241m=\u001b[39m train_y[:, i]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Print every 1000 epochs\u001b[39;00m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    input_size = train_embedding.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 800000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding).squeeze()\n",
    "            target = train_y[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y[:, i].cpu().numpy()\n",
    "            test_target = test_y[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component']\n",
    "embeddings_names = ['x', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_name)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R² values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save\n",
    "plt.savefig(\"results/\"+DATASET+\"_\"+MODEL+\"test_R2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_name)][2]\n",
    "        if train_r2 < -0.05:  # Handle negative R² values\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'train_R2.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with more properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Use the average path length of the largest connected component for disconnected graphs\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "def betweenness_centralization(G):\n",
    "    n = len(G)\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    max_betweenness = max(betweenness.values())\n",
    "    centralization = sum(max_betweenness - bet for bet in betweenness.values())\n",
    "    if n > 2:\n",
    "        centralization /= (n - 1) * (n - 2) / 2\n",
    "    return centralization\n",
    "\n",
    "def pagerank_centralization(G, alpha=0.85):\n",
    "    n = len(G)\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    max_pagerank = max(pagerank.values())\n",
    "    centralization = sum(max_pagerank - pr for pr in pagerank.values())\n",
    "    if n > 1:\n",
    "        centralization /= (n - 1)\n",
    "    return centralization\n",
    "\n",
    "def clustering_properties(G):\n",
    "    average_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "    return average_clustering, transitivity\n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        \n",
    "        # Number of nodes\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        \n",
    "        # Number of edges\n",
    "        num_edges = G.number_of_edges()\n",
    "        \n",
    "        # Density\n",
    "        density = nx.density(G)\n",
    "        \n",
    "        # Average Path Length\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        \n",
    "        # Diameter\n",
    "        if nx.is_connected(G):\n",
    "            diameter = nx.diameter(G)\n",
    "        else:\n",
    "            # Use the diameter of the largest connected component for disconnected graphs\n",
    "            components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "            largest_component = max(components, key=len)\n",
    "            diameter = nx.diameter(largest_component)\n",
    "        \n",
    "        # Radius\n",
    "        if nx.is_connected(G):\n",
    "            radius = nx.radius(G)\n",
    "        else:\n",
    "            radius = nx.radius(largest_component)\n",
    "        \n",
    "        # Clustering Coefficient\n",
    "        clustering_coeff = nx.average_clustering(G)\n",
    "        \n",
    "        # Transitivity\n",
    "        transitivity = nx.transitivity(G)\n",
    "        \n",
    "        # Assortativity\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "        \n",
    "        # Number of Cliques\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        \n",
    "        # Number of Triangles\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        \n",
    "        # Number of Squares (4-cycles)\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        \n",
    "        # Size of the Largest Connected Component\n",
    "        largest_component_size = len(max(nx.connected_components(G), key=len))\n",
    "        \n",
    "        # Average Degree\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        avg_degree = np.mean(degrees)\n",
    "        \n",
    "        # Betweenness Centrality\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        avg_betweenness_centrality = np.mean(list(betweenness_centrality.values()))\n",
    "        \n",
    "        # Eigenvalues of the Adjacency Matrix (for spectral properties)\n",
    "        eigenvalues = np.linalg.eigvals(nx.adjacency_matrix(G).todense())\n",
    "        spectral_radius = max(eigenvalues)\n",
    "        algebraic_connectivity = sorted(eigenvalues)[1]  # second smallest eigenvalue\n",
    "        \n",
    "        # Graph Laplacian Eigenvalues\n",
    "        laplacian_eigenvalues = np.linalg.eigvals(nx.laplacian_matrix(G).todense())\n",
    "        graph_energy = sum(abs(laplacian_eigenvalues))\n",
    "        \n",
    "        # Small-World-ness\n",
    "        # Compare clustering coefficient and average path length with those of a random graph\n",
    "        random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "        random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "        random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "        small_world_coefficient = (clustering_coeff / random_clustering_coeff) / (avg_path_len / random_avg_path_len)\n",
    "\n",
    "        # Calculate Betweenness Centralization\n",
    "        betweenness_cent = betweenness_centralization(G)\n",
    "        print(f\"Betweenness Centralization: {betweenness_cent}\")\n",
    "\n",
    "        # Calculate PageRank Centralization\n",
    "        pagerank_cent = pagerank_centralization(G)\n",
    "        print(f\"PageRank Centralization: {pagerank_cent}\")\n",
    "\n",
    "        # Calculate Clustering properties\n",
    "        avg_clustering, transitivity = clustering_properties(G)\n",
    "        print(f\"Average Clustering Coefficient: {avg_clustering}\")\n",
    "        print(f\"Transitivity: {transitivity}\")\n",
    "        \n",
    "        properties.append((\n",
    "            num_nodes,\n",
    "            num_edges,\n",
    "            density,\n",
    "            avg_path_len,\n",
    "            diameter,\n",
    "            radius,\n",
    "            clustering_coeff,\n",
    "            transitivity,\n",
    "            assortativity,\n",
    "            num_cliques,\n",
    "            num_triangles,\n",
    "            num_squares,\n",
    "            largest_component_size,\n",
    "            avg_degree,\n",
    "            avg_betweenness_centrality,\n",
    "            spectral_radius,\n",
    "            algebraic_connectivity,\n",
    "            graph_energy,\n",
    "            small_world_coefficient\n",
    "        ))\n",
    "    return properties\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties_long = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties_long = compute_graph_properties(selected_dataset)\n",
    "\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties_long, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties_long, f)\n",
    "\n",
    "#load the properties\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"rb\") as f:\n",
    "#     train_properties_long = pkl.load(f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"rb\") as f:\n",
    "#     test_properties_long = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient']\n",
    "train_y_long = torch.tensor(train_properties_long, dtype=torch.float32)\n",
    "test_y_long = torch.tensor(test_properties_long, dtype=torch.float32)\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    input_size = train_embedding.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names_long):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 800000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding).squeeze()\n",
    "            target = train_y_long[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y_long[:, i].cpu().numpy()\n",
    "            test_target = test_y_long[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors_long = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_names_long)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R² values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/\"+DATASET+\"_\"+MODEL+\"_test_R2_plot_long.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_names_long)][2]\n",
    "        if train_r2 < -0.05:  # Handle negative R² values\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "\n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R² for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/\"+DATASET+\"_\"+MODEL+\"_train_R2_plot_long.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
