============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
GPU available? False
GPU available? False
545.23.08
Python 3.11.4
545.23.08
Python 3.11.4
/home/tpelletreaudur/.local/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
/home/tpelletreaudur/.local/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
/gpfs/home3/tpelletreaudur/Probing-GNN-representations/FC_probing_GAT.py:158: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  train_x = np.array([feat[0] for feat in train_features])
Data(x=[116, 116], edge_index=[2, 1016], edge_attr=[1016, 1], y=[1])
['y', 'edge_attr', 'x', 'edge_index']
ValuesView({'x': tensor([[ 0.0000,  0.4543,  0.2477,  ...,  0.1753,  0.2247, -0.1751],
        [ 0.4543,  0.0000, -0.2204,  ..., -0.1947, -0.2258, -0.1434],
        [ 0.2477, -0.2204,  0.0000,  ..., -0.0521, -0.0804, -0.2025],
        ...,
        [ 0.1753, -0.1947, -0.0521,  ...,  0.0000,  0.6875, -0.1364],
        [ 0.2247, -0.2258, -0.0804,  ...,  0.6875,  0.0000,  0.0929],
        [-0.1751, -0.1434, -0.2025,  ..., -0.1364,  0.0929,  0.0000]]), 'edge_index': tensor([[  0,   0,   0,  ..., 114, 115, 115],
        [  1,  10,  12,  ..., 113,  94, 109]]), 'edge_attr': tensor([[0.4543],
        [0.5913],
        [0.4224],
        ...,
        [0.6875],
        [0.4846],
        [0.5437]]), 'y': tensor([0])})
Net(
  (conv_layers): ModuleList(
    (0): GATConv(116, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
  )
  (batch_norms): ModuleList(
    (0-4): 5 x BatchNorm(128)
  )
  (lin1): Linear(128, 128, bias=True)
  (lin2): Linear(128, 2, bias=True)
  (bn1): BatchNorm(128)
  (bn2): BatchNorm(2)
)
tensor([217, 137, 426,  ..., 643, 205, 434])
Epoch: 020, Loss: 0.138, Test Loss: 0.780, Train Acc: 0.999 Test Acc: 0.545
Epoch: 040, Loss: 0.118, Test Loss: 0.776, Train Acc: 1.000 Test Acc: 0.545
Epoch: 060, Loss: 0.097, Test Loss: 0.844, Train Acc: 0.999 Test Acc: 0.600
Epoch: 080, Loss: 0.087, Test Loss: 0.782, Train Acc: 1.000 Test Acc: 0.582
Epoch: 100, Loss: 0.078, Test Loss: 0.797, Train Acc: 1.000 Test Acc: 0.582
Epoch: 120, Loss: 0.073, Test Loss: 0.810, Train Acc: 1.000 Test Acc: 0.618
Epoch: 140, Loss: 0.068, Test Loss: 0.813, Train Acc: 1.000 Test Acc: 0.600
Epoch: 160, Loss: 0.065, Test Loss: 0.809, Train Acc: 1.000 Test Acc: 0.618
Epoch: 180, Loss: 0.068, Test Loss: 0.810, Train Acc: 1.000 Test Acc: 0.582
Epoch: 200, Loss: 0.064, Test Loss: 0.823, Train Acc: 1.000 Test Acc: 0.618
Model saved in: models/FC_GATserver.pt
Test Loss: 0.823, Train Acc: 1.000 Test Acc: 0.618
29696
1044
Traceback (most recent call last):
  File "/gpfs/home3/tpelletreaudur/Probing-GNN-representations/FC_probing_GAT.py", line 158, in <module>
    train_x = np.array([feat[0] for feat in train_features])
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (40,) + inhomogeneous part.

JOB STATISTICS
==============
Job ID: 6942450
Cluster: snellius
User/Group: tpelletreaudur/tpelletreaudur
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:02
CPU Efficiency: 0.02% of 03:32:24 core-walltime
Job Wall-clock time: 00:11:48
Memory Utilized: 1.20 MB
Memory Efficiency: 0.00% of 120.00 GB
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
Data(x=[116, 116], edge_index=[2, 1016], edge_attr=[1016, 1], y=[1])
['edge_attr', 'edge_index', 'y', 'x']
ValuesView({'x': tensor([[ 0.0000,  0.4543,  0.2477,  ...,  0.1753,  0.2247, -0.1751],
        [ 0.4543,  0.0000, -0.2204,  ..., -0.1947, -0.2258, -0.1434],
        [ 0.2477, -0.2204,  0.0000,  ..., -0.0521, -0.0804, -0.2025],
        ...,
        [ 0.1753, -0.1947, -0.0521,  ...,  0.0000,  0.6875, -0.1364],
        [ 0.2247, -0.2258, -0.0804,  ...,  0.6875,  0.0000,  0.0929],
        [-0.1751, -0.1434, -0.2025,  ..., -0.1364,  0.0929,  0.0000]]), 'edge_index': tensor([[  0,   0,   0,  ..., 114, 115, 115],
        [  1,  10,  12,  ..., 113,  94, 109]]), 'edge_attr': tensor([[0.4543],
        [0.5913],
        [0.4224],
        ...,
        [0.6875],
        [0.4846],
        [0.5437]]), 'y': tensor([0])})
Net(
  (gin_layers): ModuleList(
    (0): GINConv(nn=Sequential(
      (0): Linear(in_features=116, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): BatchNorm(128)
    ))
    (1-4): 4 x GINConv(nn=Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): BatchNorm(128)
    ))
  )
  (lin1): Linear(128, 128, bias=True)
  (lin2): Linear(128, 2, bias=True)
  (bn1): BatchNorm(128)
)
tensor([ 311,  907,  748,  223,  815,  929,  422,  772,  744,  218, 1052,  571,
         702,  323,  227,  365,  627,  568,  784, 1071,  210,  249,   73,  574,
         782,  331,  430,  959,  517,  248,  564,  342,  134, 1011,  409,   99,
         334,  376,  460,  327,  919, 1096,  418,  293,  926,  707,  390,  940,
         876,  301, 1026, 1041,  170,  628,  911,  347, 1089,  441,   43,  314,
         993,  511,  340,   45,  499,  150,  851,  914,    1,   38,  790,  754,
         490,  168,   47,  161,  589, 1035,  324,  649,  295,  789,  175,  288,
         442,  820, 1024,   94,  500,  320,   76,  484,  777,   85,  414,  566,
         558,  757,  758,    5,  794,   29,  213,  260,  330,  880,  780, 1002,
         841,  600,  429,  378,  875,  977,  121,  279,   23,  710,  578,  463,
         158,   66,  806,  796,  487, 1087,   40,  786,  801,  300,  113,   22,
        1017,  476,  399,  814,  676,   78,  450,  200, 1090,  180, 1066,  111,
         580,  363,  826,  634,  658,  397, 1029,  933, 1075, 1023,  664,  211,
         383,  259,   86,  473,  147,  294, 1031,  470, 1086,  677,  958, 1060,
         533,  230,  234, 1020,  701,  464,  410,  787,  935,  853,  870,  377,
         509,  426,   63,  866,  898,   74,  329,  242,  337,   98,  905,  812,
         685,  528,  261,  640,  729,  424,   20,  888, 1021,  318,  700,  668,
         923,  479,  765,  493,   13,  854,  741,  545,  874,  778,  492,  683,
         613,   19,  943, 1004,  802,  752,  749,  428,  176,   90,  275,  412,
        1059,  716,  319,  577, 1094,  747,  438, 1009,  505,  339,  232,  308,
         131,   60, 1077,  753,  270,  128,  371,  529,   88,  401,  395,  728,
         185,  717,  846,  785,   41,  351,  986, 1044,  963,  108,  598,  903,
         101,  767, 1084,  114,  927,  225,  290,  603,   17,  125,  930,  149,
         712,  427,   39,  466,  653,  140,  104,  219,  203,   11,  665,  446,
         560,  471,  215,  151,  229,  468,  264,  304,  654,  553,  656,  897,
         718,  388,  713,  403,  715,  865,  353,  356,  546,  961,  995,  265,
         335,  357,  238,  724,  527,  681,  182,  475,  452,  115,  799,  281,
        1030,  495,  100,  831,  915, 1003,  942,  906, 1055,  908,   46,  655,
         822,  296,  156,   55,  523,  465,  662,  938,  448,  896,  332,  804,
         974,   30,  309,  997,  302,  313,  844,  657,  709,  966,  483,  837,
         737,  510,  881,  110,  810,  612,  694,  859,  126,  731,   35,  387,
         186,  922,   89,  271,    2,  909,  838,  693,  703,   92,  989, 1088,
         436,  243,  405,  593,  673,  216,  827, 1091,  878,  269,  572,  191,
        1034,  349,  106, 1048,  755,  135,  992,  645,  950,  650,  240,  783,
         809,  871,  591,  439,   57,  190,  432,  369,  849,  807, 1068,  848,
          62,  585, 1062,  241,  526,  166,  107,  825, 1046,  632,  202,  178,
        1074,  520,   14,  852,   71,  714,  534,  338,  392,  444,  145,   83,
         606,  373,   75,  981,  663,   96,  797,  643,   50,   70,  280,  604,
         609,  183,  532,  530,   32,  555,  524,  769,  892,  968,  443,  760,
         196,  486,  254,  695,  565,  761,  537,  316,  425,  169,  287,  286,
         406,  904,  419,  413,  590,  823,  277,  514,  393,  776,  946,  711,
         937,  863,  482,  364,  678,  633,  850,   81,  306,  637, 1065, 1001,
          15,  688,  660,   79,  924,  847,  148, 1047,  541,  575,  569,  736,
         719,   67,  389,  417,  733,  573,  217,  307,  679,  467,  597,  954,
         181,  461,  480,  631,  877,  416,  194,  407,  842,  420,  298, 1063,
         322,  570,  803,  297,  626,  548,  936,  583,  840,  592,   27, 1092,
         947,  682,  630,  380,  887,  800,  921,  272,  141,  697,  133,  833,
         636,  538,  502,  925,  105,  109,  102,  689,  680, 1095,   54,  855,
         690,  453,  917,  488,  224, 1037,  167,  204,  522,  433,  123,  516,
         661,  858,  973, 1058,   56,  491,  792,  980,  402, 1027,  976,  214,
         236,  941,  494,  317,   69,  458,  670,  172,  481,  912,   80,  431,
         839,  836,  982,  902,  811, 1049,  771,  669,   72,  120, 1051,  705,
         386,  116,  732,  639,  127,  945,  824,  362,  798,  999,  374,  671,
           9,  228, 1042,  920,  691,  355, 1043,  474,  872,  918,  971,  118,
         285,  379,  146,   65,  239,   36,  122,  721, 1000,   59, 1078,  646,
         447,  605,  817,  581,  184,  768,  652,  651,  965,  706,  883,  303,
         939,  440,  835,  562,  644,   48,  154, 1053,  610,  346,  199,  139,
         813,  975,  132,  607,  496,  160,  456,   42,  635, 1061,  899,  252,
         672,  188,  586,  208,    3, 1028,  391,  791,   33,  231,  624,  620,
         513,  396, 1016,  984,  449,   34,   64,  366,  299,  828,  437,  207,
         336,  614,  955,  727, 1010,  739,  478,  563,  244,  596,  775,  552,
         507,  869, 1014,  504,  137,  746,  130,  375,  821,  684,  189, 1098,
         333,  893, 1057,  857,  192,  967,  985,  601,  928,   87,  735,  756,
          18, 1008,  282,  879,  262,  808,  220,  209,  618,  477, 1056,  328,
         284,  136,  725,  956,  970, 1067,  155, 1032,  960,  326,  457,    7,
         250,  856,  886,  638, 1036,  360,  549,    4,  257,  129,  352,  305,
          97,  112,  247, 1082,  615,  198,  949,  834, 1064,   25,  987,  994,
         341, 1070, 1019, 1018,   68,  226,   61,  611,  667,  764,  542,  206,
         144,  882,  358,  263,  692,  142,  445,  246,  793,  964,  547,  292,
         894,  723,  551,    0, 1013,  608,  587,   37,  372, 1054,  152, 1085,
         708,  159,   52,  599,  157,   49,  544,  212,  873,  253,  621,  498,
         519, 1012,  998,  361,  400,  845,  554,  948, 1038,  629,  745,  743,
         868,  381,  704,  641,  540,  819, 1097,  497,  222,  766,  321,  531,
         934,  394,  434])
Epoch: 020, Loss: 0.019, Test Loss: 1.661, Train Acc: 0.999 Test Acc: 0.559
Epoch: 040, Loss: 0.001, Test Loss: 2.159, Train Acc: 0.999 Test Acc: 0.568
Epoch: 060, Loss: 0.001, Test Loss: 2.279, Train Acc: 1.000 Test Acc: 0.573
Epoch: 080, Loss: 0.001, Test Loss: 2.333, Train Acc: 1.000 Test Acc: 0.564
Epoch: 100, Loss: 0.001, Test Loss: 2.382, Train Acc: 1.000 Test Acc: 0.564
Epoch: 120, Loss: 0.000, Test Loss: 2.407, Train Acc: 1.000 Test Acc: 0.559
Epoch: 140, Loss: 0.000, Test Loss: 2.429, Train Acc: 1.000 Test Acc: 0.555
Epoch: 160, Loss: 0.000, Test Loss: 2.447, Train Acc: 1.000 Test Acc: 0.555
Epoch: 180, Loss: 0.000, Test Loss: 2.458, Train Acc: 1.000 Test Acc: 0.555
Epoch: 200, Loss: 0.000, Test Loss: 2.469, Train Acc: 1.000 Test Acc: 0.555
Epoch: 220, Loss: 0.000, Test Loss: 2.476, Train Acc: 1.000 Test Acc: 0.550
Epoch: 240, Loss: 0.000, Test Loss: 2.482, Train Acc: 1.000 Test Acc: 0.550
Epoch: 260, Loss: 0.000, Test Loss: 2.487, Train Acc: 0.999 Test Acc: 0.550
Epoch: 280, Loss: 0.000, Test Loss: 2.490, Train Acc: 0.999 Test Acc: 0.545
Epoch: 300, Loss: 0.000, Test Loss: 2.494, Train Acc: 0.999 Test Acc: 0.550
Epoch: 320, Loss: 0.000, Test Loss: 2.496, Train Acc: 0.999 Test Acc: 0.550
Epoch: 340, Loss: 0.000, Test Loss: 2.498, Train Acc: 0.999 Test Acc: 0.550
Epoch: 360, Loss: 0.000, Test Loss: 2.500, Train Acc: 0.999 Test Acc: 0.550
Epoch: 380, Loss: 0.000, Test Loss: 2.501, Train Acc: 0.999 Test Acc: 0.550
Epoch: 400, Loss: 0.000, Test Loss: 2.502, Train Acc: 0.999 Test Acc: 0.550
Epoch: 420, Loss: 0.000, Test Loss: 2.503, Train Acc: 0.999 Test Acc: 0.550
Epoch: 440, Loss: 0.000, Test Loss: 2.503, Train Acc: 0.999 Test Acc: 0.550
Epoch: 460, Loss: 0.000, Test Loss: 2.504, Train Acc: 0.999 Test Acc: 0.550
Epoch: 480, Loss: 0.000, Test Loss: 2.504, Train Acc: 0.999 Test Acc: 0.545
Epoch: 500, Loss: 0.000, Test Loss: 2.504, Train Acc: 0.999 Test Acc: 0.545
Model saved in: models/FC_GINserver.pt
Test Loss: 2.504, Train Acc: 0.999 Test Acc: 0.545
9
879
Traceback (most recent call last):
  File "/gpfs/home3/tpelletreaudur/Probing-GNN-representations/FC_probing_GIN.py", line 186, in <module>
    train_x8 = np.array([feat[9] for feat in train_features])
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home3/tpelletreaudur/Probing-GNN-representations/FC_probing_GIN.py", line 186, in <listcomp>
    train_x8 = np.array([feat[9] for feat in train_features])
                         ~~~~^^^
IndexError: tuple index out of range

JOB STATISTICS
==============
Job ID: 6942448
Cluster: snellius
User/Group: tpelletreaudur/tpelletreaudur
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 03:40:22
CPU Efficiency: 90.35% of 04:03:54 core-walltime
Job Wall-clock time: 00:13:33
Memory Utilized: 2.76 GB
Memory Efficiency: 2.30% of 120.00 GB
