{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing GCN on ClinTox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll first be loading the FC matrices and explore their structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from rdkit import Chem\n",
    "\n",
    "# Load the Tox21 dataset\n",
    "# dataset = MoleculeNet(root='data/Tox21', name='Tox21')\n",
    "# Load the ClinTox dataset\n",
    "dataset = MoleculeNet(root='data/ClinTox', name='ClinTox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[24, 9], edge_index=[2, 46], edge_attr=[46, 3], smiles='*C(=O)[C@H](CCCCNC(=O)OCCOC)NC(=O)OCCOC', y=[1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "\n",
    "def transform_clintox_dataset():\n",
    "    transformed_dataset = []\n",
    "\n",
    "    for data in dataset:\n",
    "        # Ensure the data is in the correct format\n",
    "        x = data.x.float()  # Node features\n",
    "        edge_index = data.edge_index.long()  # Edge indices\n",
    "        edge_attr = data.edge_attr.float()  # Edge features\n",
    "        #use only of of the two y's\n",
    "        # y = data.y[:,0].float()  # Target variable\n",
    "        #y as int\n",
    "        y = data.y[:,0].long()  # Target variable\n",
    "\n",
    "\n",
    "        # Create a Data object\n",
    "        graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        transformed_dataset.append(graph)\n",
    "\n",
    "    return transformed_dataset\n",
    "\n",
    "# Example usage\n",
    "transformed_clintox_dataset = transform_clintox_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_clintox_dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = transformed_clintox_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[24, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1])\n",
      "['edge_attr', 'x', 'y', 'edge_index']\n",
      "ValuesView({'x': tensor([[0., 0., 1., 5., 0., 0., 0., 0., 0.],\n",
      "        [6., 0., 3., 5., 0., 0., 3., 0., 0.],\n",
      "        [8., 0., 1., 5., 0., 0., 3., 0., 0.],\n",
      "        [6., 2., 4., 5., 1., 0., 4., 0., 0.],\n",
      "        [6., 0., 4., 5., 2., 0., 4., 0., 0.],\n",
      "        [6., 0., 4., 5., 2., 0., 4., 0., 0.],\n",
      "        [6., 0., 4., 5., 2., 0., 4., 0., 0.],\n",
      "        [6., 0., 4., 5., 2., 0., 4., 0., 0.],\n",
      "        [7., 0., 3., 5., 1., 0., 3., 0., 0.],\n",
      "        [6., 0., 3., 5., 0., 0., 3., 0., 0.],\n",
      "        [8., 0., 1., 5., 0., 0., 3., 0., 0.],\n",
      "        [8., 0., 2., 5., 0., 0., 3., 0., 0.],\n",
      "        [6., 0., 4., 5., 2., 0., 4., 0., 0.],\n",
      "        [6., 0., 4., 5., 2., 0., 4., 0., 0.],\n",
      "        [8., 0., 2., 5., 0., 0., 4., 0., 0.],\n",
      "        [6., 0., 4., 5., 3., 0., 4., 0., 0.],\n",
      "        [7., 0., 3., 5., 1., 0., 3., 0., 0.],\n",
      "        [6., 0., 3., 5., 0., 0., 3., 0., 0.],\n",
      "        [8., 0., 1., 5., 0., 0., 3., 0., 0.],\n",
      "        [8., 0., 2., 5., 0., 0., 3., 0., 0.],\n",
      "        [6., 0., 4., 5., 2., 0., 4., 0., 0.],\n",
      "        [6., 0., 4., 5., 2., 0., 4., 0., 0.],\n",
      "        [8., 0., 2., 5., 0., 0., 4., 0., 0.],\n",
      "        [6., 0., 4., 5., 3., 0., 4., 0., 0.]]), 'edge_index': tensor([[ 0,  1,  1,  1,  2,  3,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,\n",
      "          9,  9,  9, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 16, 16, 17, 17, 17,\n",
      "         18, 19, 19, 20, 20, 21, 21, 22, 22, 23],\n",
      "        [ 1,  0,  2,  3,  1,  1,  4, 16,  3,  5,  4,  6,  5,  7,  6,  8,  7,  9,\n",
      "          8, 10, 11,  9,  9, 12, 11, 13, 12, 14, 13, 15, 14,  3, 17, 16, 18, 19,\n",
      "         17, 17, 20, 19, 21, 20, 22, 21, 23, 22]]), 'edge_attr': tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [2., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [2., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [2., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [2., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [2., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [2., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]]), 'y': tensor([1])})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1484"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok, let's explore the data a bit more\n",
    "#dataset is a list object of torch_geometric.data objects\n",
    "\n",
    "#let's see the first element\n",
    "print(dataset[0])\n",
    "\n",
    "#it's a dictionary object, let's see the keys\n",
    "print(dataset[0].keys())\n",
    "\n",
    "#ok, let's see the values\n",
    "print(dataset[0].values())\n",
    "\n",
    "#it has 4 keys, 'x', 'edge_index', 'edge_attr' and 'y' where y=0 menas the patient is healthy and y=1 means the patient has Autism Spectrum Disorder (ASD)\n",
    "\"\"\"graph = Data(x=ROI.reshape(-1,116).float(),\n",
    "                     edge_index=G.indices().reshape(2,-1).long(),\n",
    "                     edge_attr=G.values().reshape(-1,1).float(),\n",
    "                     y=y.long())\"\"\"\n",
    "\n",
    "#how much data do we have?\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _infer_num_classes(self):\n",
    "    # Ensure labels are integers\n",
    "    max_label = max(int(data.y.max().item()) for data in self)\n",
    "    return max_label + 1\n",
    "\n",
    "# Example usage\n",
    "num_classes = _infer_num_classes(transformed_clintox_dataset)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): GCNConv(9, 128)\n",
      "    (1): GCNConv(128, 128)\n",
      "    (2): GCNConv(128, 128)\n",
      "    (3): GCNConv(128, 128)\n",
      "    (4): GCNConv(128, 128)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm(128)\n",
      "    (1): BatchNorm(128)\n",
      "    (2): BatchNorm(128)\n",
      "    (3): BatchNorm(128)\n",
      "    (4): BatchNorm(128)\n",
      "  )\n",
      "  (lin1): Linear(128, 128, bias=True)\n",
      "  (lin2): Linear(128, 2, bias=True)\n",
      "  (bn1): BatchNorm(128)\n",
      "  (bn2): BatchNorm(2)\n",
      ")\n",
      "tensor([ 238,  659, 1047,  ...,  117,  162,  706])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#set the seed\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "MODEL = \"GCN_wo_edge_weight\"\n",
    "DATASET = \"CLINTOX\"\n",
    "\n",
    "from models.models_ClinTox import GCN_framework_wo_edge_weight as framework # import the model\n",
    "\n",
    "gnn = framework(dataset,device=\"cpu\")\n",
    "\n",
    "print(gnn.model)\n",
    "print(gnn.train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1484"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "# gnn.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.542, Train Acc: 0.917 Test Acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "302\n",
      "1219\n",
      "1220\n"
     ]
    }
   ],
   "source": [
    "#check for empty graphs in selected_dataset\n",
    "for i in range(len(dataset)):\n",
    "    if len(dataset[i].x) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpelletreaudur/.local/lib/python3.10/site-packages/networkx/algorithms/assortativity/correlation.py:302: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return float((xy * (M - ab)).sum() / np.sqrt(vara * varb))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes must be greater than 1\n",
      "Number of nodes must be greater than 1\n",
      "Number of nodes must be greater than 1\n",
      "Number of nodes must be greater than 1\n",
      "Number of nodes must be greater than 1\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_avg_path_length(G):\n",
    "    if G.number_of_nodes() == 0:\n",
    "        raise nx.NetworkXPointlessConcept(\"Connectivity is undefined for the null graph.\")\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Alternative metrics for disconnected graphs\n",
    "        # Option 1: Use the average path length of the largest connected component\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "def calculate_small_world(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.algorithms.smallworld.sigma(G)\n",
    "    else:\n",
    "        # Alternative metrics for disconnected graphs\n",
    "        # Option 1: Use the small world coefficient of the largest connected component\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.algorithms.smallworld.sigma(largest_component)\n",
    "    \n",
    "def compute_swi(graph):\n",
    "    # Calculate clustering coefficient and average path length for the given graph\n",
    "    clustering_coeff = nx.average_clustering(graph)\n",
    "    avg_path_len = calculate_avg_path_length(graph)\n",
    "    \n",
    "    # Generate a random graph with the same number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    \n",
    "    # Generate a lattice graph with the same number of nodes and edges\n",
    "    if num_nodes > 2:\n",
    "        k = min(4, num_nodes - 1)\n",
    "    else:\n",
    "        print(\"Number of nodes must be greater than 1\")\n",
    "        return float('inf') \n",
    "\n",
    "    lattice_graph = nx.watts_strogatz_graph(num_nodes, k=k, p=0)  # Adjust k as needed\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the random graph\n",
    "    random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "    random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the lattice graph\n",
    "    lattice_clustering_coeff = nx.average_clustering(lattice_graph)\n",
    "    lattice_avg_path_len = calculate_avg_path_length(lattice_graph)\n",
    "    \n",
    "    # Check for zero denominator\n",
    "    if (random_avg_path_len - lattice_avg_path_len) == 0 or (lattice_clustering_coeff - random_clustering_coeff) == 0:\n",
    "        return float('inf')  # or some other value indicating an undefined SWI\n",
    "    \n",
    "    swi = ((avg_path_len - lattice_avg_path_len) / (random_avg_path_len - lattice_avg_path_len)) * \\\n",
    "          ((clustering_coeff - random_clustering_coeff) / (lattice_clustering_coeff - random_clustering_coeff))\n",
    "    \n",
    "    return swi\n",
    "    \n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        if G.number_of_nodes() == 0:\n",
    "            properties.append((0, 0, -1, 0, 0, 0, 0, -1, -1))\n",
    "            continue\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        num_edges = G.number_of_edges()\n",
    "        density = nx.density(G)\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        if avg_path_len == float('inf') or avg_path_len == float('nan'):\n",
    "            avg_path_len = -1\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        number_of_node_in_the_largest_fully_connected_component = len(max(nx.connected_components(G), key=len))\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "        if assortativity == float('nan'):\n",
    "            assortativity = -1\n",
    "        small_world = compute_swi(G)\n",
    "        if small_world == float('inf'):\n",
    "            small_world = -1\n",
    "        \n",
    "        properties.append((num_nodes, num_edges, density, avg_path_len, num_cliques, num_triangles, num_squares, number_of_node_in_the_largest_fully_connected_component, assortativity, small_world))\n",
    "    return properties\n",
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties = compute_graph_properties(selected_dataset)\n",
    "\n",
    "# Save the properties to files\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_with_sm.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_with_sm.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_properties))\n",
    "# train_properties[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#save the properties in a file\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(train_properties, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(test_properties, f)\n",
    "\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_with_sm.pkl\", \"rb\") as f:\n",
    "    train_properties = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_with_sm.pkl\", \"rb\") as f:\n",
    "    test_properties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n"
     ]
    }
   ],
   "source": [
    "train_properties\n",
    "\n",
    "for i in range(len(train_properties)):\n",
    "    if len(train_properties[i]) == 10:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27,\n",
       " 28,\n",
       " 0.07977207977207977,\n",
       " 5.452991452991453,\n",
       " 28,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 27,\n",
       " 0.13513513513513856,\n",
       " 0.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_properties[96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/tpelletreaudur.7536726/ipykernel_2061930/2871301966.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_properties = np.array(train_properties)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m train_properties \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_properties)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Replace NaN values with -1\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_properties[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_properties\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# If you need to convert it back to a list of lists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_properties \u001b[38;5;241m=\u001b[39m train_properties\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert train_properties to a numpy array\n",
    "train_properties = np.array(train_properties)\n",
    "\n",
    "# Replace NaN values with -1\n",
    "train_properties[np.isnan(train_properties)] = -1\n",
    "\n",
    "# If you need to convert it back to a list of lists\n",
    "train_properties = train_properties.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_features[0]))\n",
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embeddings of GCN are like this:\n",
    "return F.log_softmax(x7, dim=-1), (x1, x2, x3, x4, x5, x_global, x6, x7)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "output_size = 1  # Predicting one property at a time\n",
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Assume we have already evaluated to get features\n",
    "#train_features, test_features = gnn.evaluate_with_features2()\n",
    "\n",
    "# Extract x embeddings\n",
    "train_x = np.array([feat[0] for feat in train_features])\n",
    "test_x = np.array([feat[0] for feat in test_features])\n",
    "\n",
    "# Extract 2, 3, 4, global, 5, 6, 7 embeddings\n",
    "train_x2 = np.array([feat[1] for feat in train_features])\n",
    "test_x2 = np.array([feat[1] for feat in test_features])\n",
    "\n",
    "train_x3 = np.array([feat[2] for feat in train_features])\n",
    "test_x3 = np.array([feat[2] for feat in test_features])\n",
    "\n",
    "train_x4 = np.array([feat[3] for feat in train_features])\n",
    "test_x4 = np.array([feat[3] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[4] for feat in train_features])\n",
    "test_x5 = np.array([feat[4] for feat in test_features])\n",
    "\n",
    "train_x_global = np.array([feat[5] for feat in train_features])\n",
    "test_x_global = np.array([feat[5] for feat in test_features])\n",
    "\n",
    "train_x6 = np.array([feat[6] for feat in train_features])\n",
    "test_x6 = np.array([feat[6] for feat in test_features])\n",
    "\n",
    "train_x7 = np.array([feat[7] for feat in train_features])\n",
    "test_x7 = np.array([feat[7] for feat in test_features])\n",
    "\n",
    "# Compute graph properties\n",
    "# train_properties = compute_graph_properties(gnn.dataset[gnn.train_idx])\n",
    "# test_properties = compute_graph_properties(gnn.dataset[gnn.test_idx])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_x2 = torch.tensor(train_x2, dtype=torch.float32)\n",
    "train_x3 = torch.tensor(train_x3, dtype=torch.float32)\n",
    "train_x4 = torch.tensor(train_x4, dtype=torch.float32)\n",
    "train_x5 = torch.tensor(train_x5, dtype=torch.float32)\n",
    "train_x_global = torch.tensor(train_x_global, dtype=torch.float32)\n",
    "train_x6 = torch.tensor(train_x6, dtype=torch.float32)\n",
    "train_x7 = torch.tensor(train_x7, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x2 = torch.tensor(test_x2, dtype=torch.float32)\n",
    "test_x3 = torch.tensor(test_x3, dtype=torch.float32)\n",
    "test_x4 = torch.tensor(test_x4, dtype=torch.float32)\n",
    "test_x5 = torch.tensor(test_x5, dtype=torch.float32)\n",
    "test_x_global = torch.tensor(test_x_global, dtype=torch.float32)\n",
    "test_x6 = torch.tensor(test_x6, dtype=torch.float32)\n",
    "test_x7 = torch.tensor(test_x7, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_properties, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_properties, dtype=torch.float32)\n",
    "\n",
    "# Train and evaluate a model for each graph property and each embedding\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings = [(train_x, test_x), (train_x2, test_x2), (train_x3, test_x3), (train_x4, test_x4), (train_x5, test_x5), (train_x_global, test_x_global), (train_x6, test_x6), (train_x7, test_x7)]\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    input_size = train_embedding.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 1300000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding).squeeze()\n",
    "            target = train_y[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y[:, i].cpu().numpy()\n",
    "            test_target = test_y[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R: {train_r2:.4f}, Test R: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'tab:orange', 'tab:purple', 'tab:brown']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_name)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+\"_\"+MODEL+'_test_R2_plot_limited_cv.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming results, embeddings, and other necessary variables are defined as in your context\n",
    "property_names = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'num_cliques', 'num_triangles', 'num_squares', 'number_of_nodes_in_the_largest_fully_connected_component', 'assortativity', 'small_world']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'tab:orange', 'tab:purple', 'tab:brown']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_name in enumerate(property_names):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_name)][2]\n",
    "        if train_r2 < -0.05:  # Handle negative R values\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_name, color=colors[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+\"_\"+MODEL+'_train_R2_plot_limited_cv.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with more properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def calculate_avg_path_length(G):\n",
    "    if nx.is_connected(G):\n",
    "        return nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Use the average path length of the largest connected component for disconnected graphs\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "        largest_component = max(components, key=len)\n",
    "        return nx.average_shortest_path_length(largest_component)\n",
    "    \n",
    "   \n",
    "def compute_swi(graph):\n",
    "    # Calculate clustering coefficient and average path length for the given graph\n",
    "    clustering_coeff = nx.average_clustering(graph)\n",
    "    avg_path_len = calculate_avg_path_length(graph)\n",
    "    \n",
    "    # Generate a random graph with the same number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    \n",
    "    # Generate a lattice graph with the same number of nodes and edges\n",
    "    lattice_graph = nx.watts_strogatz_graph(num_nodes, k=4, p=0)  # Adjust k as needed\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the random graph\n",
    "    random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "    random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "    \n",
    "    # Calculate clustering coefficient and average path length for the lattice graph\n",
    "    lattice_clustering_coeff = nx.average_clustering(lattice_graph)\n",
    "    lattice_avg_path_len = calculate_avg_path_length(lattice_graph)\n",
    "    \n",
    "    # Compute the Small-World Index (SWI)\n",
    "    swi = ((avg_path_len - lattice_avg_path_len) / (random_avg_path_len - lattice_avg_path_len)) * \\\n",
    "          ((clustering_coeff - random_clustering_coeff) / (lattice_clustering_coeff - random_clustering_coeff))\n",
    "    \n",
    "    return swi\n",
    "\n",
    "\n",
    "def betweenness_centralization(G):\n",
    "    n = len(G)\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    max_betweenness = max(betweenness.values())\n",
    "    centralization = sum(max_betweenness - bet for bet in betweenness.values())\n",
    "    if n > 2:\n",
    "        centralization /= (n - 1) * (n - 2) / 2\n",
    "    return centralization\n",
    "\n",
    "def pagerank_centralization(G, alpha=0.85):\n",
    "    n = len(G)\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    max_pagerank = max(pagerank.values())\n",
    "    centralization = sum(max_pagerank - pr for pr in pagerank.values())\n",
    "    if n > 1:\n",
    "        centralization /= (n - 1)\n",
    "    return centralization\n",
    "\n",
    "def clustering_properties(G):\n",
    "    average_clustering = nx.average_clustering(G)\n",
    "    transitivity = nx.transitivity(G)\n",
    "    return average_clustering, transitivity\n",
    "\n",
    "def compute_graph_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        \n",
    "        # Number of nodes\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        \n",
    "        # Number of edges\n",
    "        num_edges = G.number_of_edges()\n",
    "        \n",
    "        # Density\n",
    "        density = nx.density(G)\n",
    "        \n",
    "        # Average Path Length\n",
    "        avg_path_len = calculate_avg_path_length(G)\n",
    "        \n",
    "        # Diameter\n",
    "        if nx.is_connected(G):\n",
    "            diameter = nx.diameter(G)\n",
    "        else:\n",
    "            # Use the diameter of the largest connected component for disconnected graphs\n",
    "            components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "            largest_component = max(components, key=len)\n",
    "            diameter = nx.diameter(largest_component)\n",
    "        \n",
    "        # Radius\n",
    "        if nx.is_connected(G):\n",
    "            radius = nx.radius(G)\n",
    "        else:\n",
    "            radius = nx.radius(largest_component)\n",
    "        \n",
    "        # Clustering Coefficient\n",
    "        clustering_coeff = nx.average_clustering(G)\n",
    "        \n",
    "        # Transitivity\n",
    "        transitivity = nx.transitivity(G)\n",
    "        \n",
    "        # Assortativity\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "        \n",
    "        # Number of Cliques\n",
    "        num_cliques = len(list(nx.find_cliques(G)))\n",
    "        \n",
    "        # Number of Triangles\n",
    "        num_triangles = sum(nx.triangles(G).values()) / 3\n",
    "        \n",
    "        # Number of Squares (4-cycles)\n",
    "        num_squares = sum(nx.square_clustering(G).values()) / 4\n",
    "        \n",
    "        # Size of the Largest Connected Component\n",
    "        largest_component_size = len(max(nx.connected_components(G), key=len))\n",
    "        \n",
    "        # Average Degree\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        avg_degree = np.mean(degrees)\n",
    "        \n",
    "        # Betweenness Centrality\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        avg_betweenness_centrality = np.mean(list(betweenness_centrality.values()))\n",
    "        \n",
    "        # Eigenvalues of the Adjacency Matrix (for spectral properties)\n",
    "        eigenvalues = np.linalg.eigvals(nx.adjacency_matrix(G).todense())\n",
    "        spectral_radius = max(eigenvalues)\n",
    "        algebraic_connectivity = sorted(eigenvalues)[1]  # second smallest eigenvalue\n",
    "        \n",
    "        # Graph Laplacian Eigenvalues\n",
    "        laplacian_eigenvalues = np.linalg.eigvals(nx.laplacian_matrix(G).todense())\n",
    "        graph_energy = sum(abs(laplacian_eigenvalues))\n",
    "        \n",
    "        # Small-World-ness\n",
    "        # Compare clustering coefficient and average path length with those of a random graph\n",
    "        random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "        random_clustering_coeff = nx.average_clustering(random_graph)\n",
    "        random_avg_path_len = calculate_avg_path_length(random_graph)\n",
    "        small_world_coefficient = (clustering_coeff / random_clustering_coeff) / (avg_path_len / random_avg_path_len)\n",
    "\n",
    "        small_world_index = compute_swi(G)\n",
    "\n",
    "        # Calculate Betweenness Centralization\n",
    "        betweenness_cent = betweenness_centralization(G)\n",
    "        print(f\"Betweenness Centralization: {betweenness_cent}\")\n",
    "\n",
    "        # Calculate PageRank Centralization\n",
    "        pagerank_cent = pagerank_centralization(G)\n",
    "        print(f\"PageRank Centralization: {pagerank_cent}\")\n",
    "\n",
    "        # Calculate Clustering properties\n",
    "        avg_clustering, transitivity = clustering_properties(G)\n",
    "        print(f\"Average Clustering Coefficient: {avg_clustering}\")\n",
    "        print(f\"Transitivity: {transitivity}\")\n",
    "        \n",
    "        properties.append((\n",
    "            num_nodes,\n",
    "            num_edges,\n",
    "            density,\n",
    "            avg_path_len,\n",
    "            diameter,\n",
    "            radius,\n",
    "            clustering_coeff,\n",
    "            transitivity,\n",
    "            assortativity,\n",
    "            num_cliques,\n",
    "            num_triangles,\n",
    "            num_squares,\n",
    "            largest_component_size,\n",
    "            avg_degree,\n",
    "            avg_betweenness_centrality,\n",
    "            spectral_radius,\n",
    "            algebraic_connectivity,\n",
    "            graph_energy,\n",
    "            small_world_coefficient, \n",
    "            betweenness_cent,\n",
    "            pagerank_cent,\n",
    "            avg_clustering,\n",
    "            small_world_index           \n",
    "\n",
    "        ))\n",
    "    return properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties_long = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties_long = compute_graph_properties(selected_dataset)\n",
    "\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties_long, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties_long, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"rb\") as f:\n",
    "    train_properties_long = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"rb\") as f:\n",
    "    test_properties_long = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the names of the properties\n",
    "# print(train_properties_long[0].keys())\n",
    "\n",
    "#print the first element of the properties\n",
    "# print(train_properties_long[0])\n",
    "\n",
    "#copare train_properties and train_properties_long\n",
    "print(train_properties[0])\n",
    "print(train_properties_long[0])\n",
    "print(len(train_properties_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "train_y_long = torch.tensor(train_properties_long, dtype=torch.float32)\n",
    "test_y_long = torch.tensor(test_properties_long, dtype=torch.float32)\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    input_size = train_embedding.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names_long):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 1300000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding).squeeze()\n",
    "            target = train_y_long[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y_long[:, i].cpu().numpy()\n",
    "            test_target = test_y_long[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R: {train_r2:.4f}, Test R: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plt\n",
    "import matplotlib.pyplot as plt\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']\n",
    "#make a color list for the properties names but with different colors\n",
    "colors_long = [\n",
    "    (0.0, 0.45, 0.70),  # Blue\n",
    "    (0.85, 0.37, 0.01),  # Orange\n",
    "    (0.8, 0.47, 0.74),   # Magenta\n",
    "    (0.0, 0.62, 0.45),   # Green\n",
    "    (0.95, 0.90, 0.25),  # Yellow\n",
    "    (0.9, 0.6, 0.0),     # Brown\n",
    "    (0.35, 0.7, 0.9),    # Sky Blue\n",
    "    (0.8, 0.6, 0.7),     # Light Pink\n",
    "    (0.3, 0.3, 0.3),     # Dark Gray\n",
    "    (0.5, 0.5, 0.0),     # Olive\n",
    "    (0.0, 0.75, 0.75),   # Cyan\n",
    "    (0.6, 0.6, 0.6),     # Light Gray\n",
    "    (0.7, 0.3, 0.1),     # Dark Orange\n",
    "    (0.6, 0.2, 0.5),     # Purple\n",
    "    (0.9, 0.4, 0.3),     # Salmon\n",
    "    (0.4, 0.4, 0.8),     # Light Blue\n",
    "    (0.2, 0.8, 0.2),     # Light Green\n",
    "    (0.6, 0.6, 0.3),     # Mustard\n",
    "    (0.3, 0.55, 0.55),    # Teal\n",
    "    (0.8, 0.5, 0.2),     # Dark Salmon\n",
    "    (0.5, 0.5, 0.5),     # Gray\n",
    "    (0.2, 0.2, 0.2),     # Black-Gray\n",
    "    (0.0, 0.0, 0.0)      # Black\n",
    "]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_names_long)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+\"_\"+MODEL+'_test_R2_plot_limited_cv_long.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        train_r2 = results[(name_of_embedding, property_names_long)][2]\n",
    "        if train_r2 < -0.05:  # Handle negative R values\n",
    "            train_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(train_r2)\n",
    "\n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+\"_\"+MODEL+'_train_R2_plot_limited_cv_long.png', dpi=300, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with a gnn train on random (the y are shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.FC.create_dataset import read_dataset\n",
    "dataset = read_dataset()\n",
    "MODEL = \"GCN_w_edge_weight\"\n",
    "DATASET = \"FC_suffled\"\n",
    "\n",
    "from models.models_FC import GCN_framework_wo_edge_weight as framework # import the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the y values of the dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "y = np.array([data.y for data in dataset])\n",
    "print(y)\n",
    "y = np.array(y, dtype=np.int64)  # Ensure y is a numeric array of type int64\n",
    "\n",
    "np.random.shuffle(y)\n",
    "\n",
    "#make y torch.int64, tensor([0]) instead of [0]\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i].y = y[i]\n",
    "\n",
    "check if the y values are shuffled\n",
    "y = np.array([data.y for data in dataset])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = framework(dataset,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataset\n",
    "import pickle as pkl\n",
    "with open(\"Datasets/FC/\"+DATASET+\"_\"+MODEL+\".pkl\", \"wb\") as f:\n",
    "    pkl.dump(dataset, f)\n",
    "\n",
    "#load the dataset\n",
    "import pickle as pkl\n",
    "with open(\"Datasets/FC/\"+DATASET+\"_\"+MODEL+\".pkl\", \"rb\") as f:\n",
    "    dataset = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "gnn.save_model(path=\"models/\"+DATASET+\"_\"+MODEL+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "gnn.load_model(path=\"models/\"+DATASET+\"_\"+MODEL+\"server.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_list = gnn.train_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in train_idx_list]\n",
    "train_properties_long = compute_graph_properties(selected_dataset)\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "selected_dataset = [gnn.dataset[i] for i in test_idx_list]\n",
    "test_properties_long = compute_graph_properties(selected_dataset)\n",
    "\n",
    "#save the properties in a file\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_properties_long, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(test_properties_long, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#save the properties in a file\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(train_properties, f)\n",
    "\n",
    "# with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(test_properties, f)\n",
    "\n",
    "#load the properties\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_train_properties_long.pkl\", \"rb\") as f:\n",
    "    train_properties = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_test_properties_long.pkl\", \"rb\") as f:\n",
    "    test_properties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embeddings of GCN are like this:\n",
    "return F.log_softmax(x7, dim=-1), (x1, x2, x3, x4, x5, x_global, x6, x7)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "output_size = 1  # Predicting one property at a time\n",
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Assume we have already evaluated to get features\n",
    "#train_features, test_features = gnn.evaluate_with_features2()\n",
    "\n",
    "# Extract x embeddings\n",
    "train_x = np.array([feat[0] for feat in train_features])\n",
    "test_x = np.array([feat[0] for feat in test_features])\n",
    "\n",
    "# Extract 2, 3, 4, global, 5, 6, 7 embeddings\n",
    "train_x2 = np.array([feat[1] for feat in train_features])\n",
    "test_x2 = np.array([feat[1] for feat in test_features])\n",
    "\n",
    "train_x3 = np.array([feat[2] for feat in train_features])\n",
    "test_x3 = np.array([feat[2] for feat in test_features])\n",
    "\n",
    "train_x4 = np.array([feat[3] for feat in train_features])\n",
    "test_x4 = np.array([feat[3] for feat in test_features])\n",
    "\n",
    "train_x5 = np.array([feat[4] for feat in train_features])\n",
    "test_x5 = np.array([feat[4] for feat in test_features])\n",
    "\n",
    "train_x_global = np.array([feat[5] for feat in train_features])\n",
    "test_x_global = np.array([feat[5] for feat in test_features])\n",
    "\n",
    "train_x6 = np.array([feat[6] for feat in train_features])\n",
    "test_x6 = np.array([feat[6] for feat in test_features])\n",
    "\n",
    "train_x7 = np.array([feat[7] for feat in train_features])\n",
    "test_x7 = np.array([feat[7] for feat in test_features])\n",
    "\n",
    "# Compute graph properties\n",
    "# train_properties = compute_graph_properties(gnn.dataset[gnn.train_idx])\n",
    "# test_properties = compute_graph_properties(gnn.dataset[gnn.test_idx])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_x2 = torch.tensor(train_x2, dtype=torch.float32)\n",
    "train_x3 = torch.tensor(train_x3, dtype=torch.float32)\n",
    "train_x4 = torch.tensor(train_x4, dtype=torch.float32)\n",
    "train_x5 = torch.tensor(train_x5, dtype=torch.float32)\n",
    "train_x_global = torch.tensor(train_x_global, dtype=torch.float32)\n",
    "train_x6 = torch.tensor(train_x6, dtype=torch.float32)\n",
    "train_x7 = torch.tensor(train_x7, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x2 = torch.tensor(test_x2, dtype=torch.float32)\n",
    "test_x3 = torch.tensor(test_x3, dtype=torch.float32)\n",
    "test_x4 = torch.tensor(test_x4, dtype=torch.float32)\n",
    "test_x5 = torch.tensor(test_x5, dtype=torch.float32)\n",
    "test_x_global = torch.tensor(test_x_global, dtype=torch.float32)\n",
    "test_x6 = torch.tensor(test_x6, dtype=torch.float32)\n",
    "test_x7 = torch.tensor(test_x7, dtype=torch.float32)\n",
    "\n",
    "# train_y = torch.tensor(train_properties, dtype=torch.float32)\n",
    "# test_y = torch.tensor(test_properties, dtype=torch.float32)\n",
    "\n",
    "# Train and evaluate a model for each graph property and each embedding\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "embeddings = [(train_x, test_x), (train_x2, test_x2), (train_x3, test_x3), (train_x4, test_x4), (train_x5, test_x5), (train_x_global, test_x_global), (train_x6, test_x6), (train_x7, test_x7)]\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "train_y_long = torch.tensor(train_properties_long, dtype=torch.float32)\n",
    "test_y_long = torch.tensor(test_properties_long, dtype=torch.float32)\n",
    "#create a dictionary where we will store the results for each embeddings, each property\n",
    "results = {}\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for train_embedding, test_embedding in embeddings:\n",
    "    input_size = train_embedding.shape[1]\n",
    "\n",
    "    for i, property_name in enumerate(property_names_long):\n",
    "        model = LinearModel(input_size, output_size)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 1300000  # Maximum number of epochs\n",
    "        min_epochs = 1000  # Minimum number of epochs\n",
    "        patience = 3000  # Number of epochs to wait for improvement\n",
    "        tolerance = 1e-6  # Tolerance for considering the loss as stable\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(train_embedding).squeeze()\n",
    "            target = train_y_long[:, i].squeeze()\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:  # Print every 1000 epochs\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Property: {property_name}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Check for early stopping, but only after minimum epochs\n",
    "            if epoch >= min_epochs:\n",
    "                if loss.item() < best_loss - tolerance:\n",
    "                    best_loss = loss.item()\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(train_embedding).squeeze().cpu().numpy()\n",
    "            test_pred = model(test_embedding).squeeze().cpu().numpy()\n",
    "\n",
    "            train_target = train_y_long[:, i].cpu().numpy()\n",
    "            test_target = test_y_long[:, i].cpu().numpy()\n",
    "\n",
    "            train_mse = mean_squared_error(train_target, train_pred)\n",
    "            test_mse = mean_squared_error(test_target, test_pred)\n",
    "\n",
    "            train_r2 = r2_score(train_target, train_pred)\n",
    "            test_r2 = r2_score(test_target, test_pred)\n",
    "\n",
    "            print(f'Embedding: {train_embedding.shape}')\n",
    "            print(f'Property: {property_name}')\n",
    "            print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "            print(f'  Train R: {train_r2:.4f}, Test R: {test_r2:.4f}')\n",
    "\n",
    "            #add the results to the dictionary\n",
    "            name_of_embedding = embeddings_names[ii]\n",
    "            results[(name_of_embedding, property_name)] = (train_mse, test_mse, train_r2, test_r2)\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "#save results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results_limited_cv_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results \n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_results.pkl\", \"rb\") as f:\n",
    "    results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors_long = [\n",
    "    (0.0, 0.45, 0.70),  # Blue\n",
    "    (0.85, 0.37, 0.01),  # Orange\n",
    "    (0.8, 0.47, 0.74),   # Magenta\n",
    "    (0.0, 0.62, 0.45),   # Green\n",
    "    (0.95, 0.90, 0.25),  # Yellow\n",
    "    (0.9, 0.6, 0.0),     # Brown\n",
    "    (0.35, 0.7, 0.9),    # Sky Blue\n",
    "    (0.8, 0.6, 0.7),     # Light Pink\n",
    "    (0.3, 0.3, 0.3),     # Dark Gray\n",
    "    (0.5, 0.5, 0.0),     # Olive\n",
    "    (0.0, 0.75, 0.75),   # Cyan\n",
    "    (0.6, 0.6, 0.6),     # Light Gray\n",
    "    (0.7, 0.3, 0.1),     # Dark Orange\n",
    "    (0.6, 0.2, 0.5),     # Purple\n",
    "    (0.9, 0.4, 0.3),     # Salmon\n",
    "    (0.4, 0.4, 0.8),     # Light Blue\n",
    "    (0.2, 0.8, 0.2),     # Light Green\n",
    "    (0.6, 0.6, 0.3),     # Mustard\n",
    "    (0.3, 0.55, 0.55),    # Teal\n",
    "    (0.8, 0.5, 0.2),     # Dark Salmon\n",
    "    (0.5, 0.5, 0.5),     # Gray\n",
    "    (0.2, 0.2, 0.2),      # Black-Gray\n",
    "    (0.0, 0.0, 0.0)      # Black\n",
    "]\n",
    "\n",
    "property_names_long = ['num_nodes', 'num_edges', 'density', 'avg_path_len', 'diameter', 'radius', 'clustering_coeff', 'transitivity', 'assortativity', 'num_cliques', 'num_triangles', 'num_squares', 'largest_component_size', 'avg_degree', 'avg_betweenness_centrality', 'spectral_radius', 'algebraic_connectivity', 'graph_energy', 'small_world_coefficient', 'betweenness_cent', 'pagerank_cent', 'avg_clustering', 'small_world_index']\n",
    "embeddings_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_global', 'x6', 'x7']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, property_names_long in enumerate(property_names_long):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for j, embedding in enumerate(embeddings):\n",
    "        name_of_embedding = embeddings_names[j]\n",
    "        test_r2 = results[(name_of_embedding, property_names_long)][3]\n",
    "        if test_r2 < -0.05:  # Handle negative R values\n",
    "            test_r2 = -0.05\n",
    "        x_points.append(j)\n",
    "        y_points.append(test_r2)\n",
    "    \n",
    "    # Plotting the line for the current property\n",
    "    plt.plot(x_points, y_points, label=property_names_long, color=colors_long[i], marker='x')\n",
    "\n",
    "plt.xticks(range(len(embeddings)), embeddings_names)\n",
    "plt.xlabel('Embedding')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "plt.title('FC matrice - GCN - R for different embeddings and properties')\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig('results/'+DATASET+'_'+MODEL+'test_R2_plot_long_random.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node embedding probing\n",
    "\n",
    "/!\\ Try to not forget that we need to change the batch_size to 1 if we want to probe for node properties as we need the forward pass to be made 1 graph by 1 graph at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Node properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Define function to compute node-level properties\n",
    "def compute_node_properties(data):\n",
    "    properties = []\n",
    "    for graph_data in data:\n",
    "        G = nx.from_edgelist(graph_data.edge_index.t().tolist())\n",
    "        node_degrees = list(dict(G.degree()).values())\n",
    "        clustering_coeffs = list(nx.clustering(G).values())\n",
    "        betweenness_centralities = list(nx.betweenness_centrality(G).values())\n",
    "        eigenvector_centralities = list(nx.eigenvector_centrality(G, max_iter=10000).values())\n",
    "        Local_clustering_coefficients = list(nx.clustering(G).values())\n",
    "\n",
    "        properties.append((node_degrees, clustering_coeffs, betweenness_centralities, eigenvector_centralities, Local_clustering_coefficients))\n",
    "    return properties\n",
    "\n",
    "# Compute node-level properties for train and test sets\n",
    "# Ensure gnn.train_idx and gnn.test_idx are lists of integers\n",
    "train_idx = gnn.train_idx.tolist() if isinstance(gnn.train_idx, torch.Tensor) else gnn.train_idx\n",
    "test_idx = gnn.test_idx.tolist() if isinstance(gnn.test_idx, torch.Tensor) else gnn.test_idx\n",
    "\n",
    "# Compute node-level properties for train and test sets\n",
    "train_node_properties = compute_node_properties([gnn.dataset[i] for i in train_idx])\n",
    "test_node_properties = compute_node_properties([gnn.dataset[i] for i in test_idx])\n",
    "\n",
    "#train_node_properties is a list of tuples, where each tuple contains 5 lists, each list contains the node-level property for each node in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_node_properties), len(test_node_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the first three betweenness centralities of the three first graphs in the train set\n",
    "[len(train_node_properties[i][2]) for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Ensure gnn.test_idx is a list of integers\n",
    "test_idx = gnn.test_idx.tolist() if isinstance(gnn.test_idx, torch.Tensor) else gnn.test_idx\n",
    "\n",
    "# Visualize the first graph of the test set to see if the properties are correct\n",
    "G = nx.from_edgelist(gnn.dataset[test_idx[0]].edge_index.t().tolist())\n",
    "nx.draw(G, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#represent the node degrees of the first graph in the test set\n",
    "node_degrees = test_node_properties[0][0]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_size=[v * 100 for v in node_degrees], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#represent the properties of betweenness centrality on the graph by chaging the size of the nodes\n",
    "betweenness_centrality = test_node_properties[0][2]\n",
    "node_degrees = test_node_properties[0][0]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_size=[v * 10000 for v in betweenness_centrality], node_color=node_degrees, cmap='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for eigenvector centrality\n",
    "eigenvector_centralities = test_node_properties[0][3]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_size=[v * 10000 for v in eigenvector_centralities], node_color=node_degrees, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for local clustering coefficients\n",
    "Local_clustering_coefficients = test_node_properties[0][4]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_size=[v * 1000 for v in Local_clustering_coefficients], node_color=node_degrees, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each graph, the length of the betweenness centralities, and in general the length of properties, are equal to the number of nodes in the graph and thus\n",
    "is equal to the length of the x matrix in the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = gnn.evaluate_with_features2(return_node_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the first graph's features\n",
    "first_graph_features = train_features[1]\n",
    "for i, feature in enumerate(first_graph_features):\n",
    "    print(f\"Feature {i+1} shape:\", feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[1][0][115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probing for the top n nodes on train_features only and averaging the results of the different diagnostic classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Function to get the top 3 nodes based on a specific property\n",
    "def get_top_nodes(property_list, top_n=37):\n",
    "    sorted_indices = sorted(range(len(property_list)), key=lambda k: property_list[k], reverse=True)\n",
    "    return sorted_indices[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the top 3 nodes for local clustering coefficient and eigenvector centrality\n",
    "top_nodes_degrees = [get_top_nodes(graph_props[0], 37) for graph_props in train_node_properties]  # Assuming 0th index is for node degrees\n",
    "top_nodes_clustering = [get_top_nodes(graph_props[1], 37) for graph_props in train_node_properties]  # Assuming 1st index is for clustering coefficient\n",
    "top_nodes_betweenness = [get_top_nodes(graph_props[2], 37) for graph_props in train_node_properties]  # Assuming 2nd index is for betweenness centrality\n",
    "top_nodes_local_clustering = [get_top_nodes(graph_props[3], 37) for graph_props in train_node_properties]  # Assuming 3rd index is for local clustering coefficient\n",
    "top_nodes_eigenvector = [get_top_nodes(graph_props[4], 37) for graph_props in train_node_properties]  # Assuming 4th index is for eigenvector centrality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for linear regression model training\n",
    "def prepare_regression_data(features, properties, top_nodes_indices):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, graph_features in enumerate(features):\n",
    "        for layer in range(len(graph_features)):\n",
    "            for node_index in top_nodes_indices[i]:\n",
    "                X.append(graph_features[layer][node_index])\n",
    "                y.append(properties[i][node_index])\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "# Training and evaluating linear regression models\n",
    "def train_and_evaluate_regression(X, y):\n",
    "    model = LinearModel(X.shape[1], 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X.float())\n",
    "        loss = criterion(outputs, y.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X.float()).view(-1)\n",
    "        r2 = r2_score(y.float(), predictions)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare data for node degree regression\n",
    "X_node_degree, y_node_degree = prepare_regression_data(train_features, [props[0] for props in train_node_properties], top_nodes_degrees)\n",
    "\n",
    "# Train and evaluate model for node degree\n",
    "r2_node_degree = train_and_evaluate_regression(X_node_degree, y_node_degree)\n",
    "print(f'R for node degree prediction: {r2_node_degree}')\n",
    "\n",
    "# Prepare data for betweenness centrality regression\n",
    "X_betweenness, y_betweenness = prepare_regression_data(train_features, [props[2] for props in train_node_properties], top_nodes_betweenness)\n",
    "\n",
    "# Train and evaluate model for betweenness centrality\n",
    "r2_betweenness = train_and_evaluate_regression(X_betweenness, y_betweenness)\n",
    "print(f'R for betweenness centrality prediction: {r2_betweenness}')\n",
    "\n",
    "# Prepare data for local clustering coefficient regression\n",
    "X_local_clustering, y_local_clustering = prepare_regression_data(train_features, [props[3] for props in train_node_properties], top_nodes_local_clustering)\n",
    "\n",
    "# Train and evaluate model for local clustering coefficient\n",
    "r2_local_clustering = train_and_evaluate_regression(X_local_clustering, y_local_clustering)\n",
    "print(f'R for local clustering coefficient prediction: {r2_local_clustering}')\n",
    "\n",
    "# Prepare data for eigenvector centrality regression\n",
    "X_eigenvector, y_eigenvector = prepare_regression_data(train_features, [props[4] for props in train_node_properties], top_nodes_eigenvector)\n",
    "\n",
    "# Train and evaluate model for eigenvector centrality\n",
    "r2_eigenvector = train_and_evaluate_regression(X_eigenvector, y_eigenvector)\n",
    "print(f'R for eigenvector centrality prediction: {r2_eigenvector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probing for the top n nodes with diagnostic classifier trained on the train set and test on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define Linear Model for probing (diagnostic classifier)\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Prepare the data for probing classifier\n",
    "def prepare_regression_data(features, properties, property_index, top_n_nodes=37):\n",
    "    X_layers = [[] for _ in range(len(features[0]))]\n",
    "    y_layers = [[] for _ in range(len(features[0]))]\n",
    "\n",
    "    for i, graph_features in enumerate(features):\n",
    "        top_nodes_indices = get_top_nodes(properties[i][property_index], top_n=top_n_nodes)\n",
    "        for layer in range(len(graph_features)):\n",
    "            for node_index in top_nodes_indices:\n",
    "                X_layers[layer].append(graph_features[layer][node_index])\n",
    "                y_layers[layer].append(properties[i][property_index][node_index])\n",
    "\n",
    "    X_layers = [torch.tensor(X) for X in X_layers]\n",
    "    y_layers = [torch.tensor(y) for y in y_layers]\n",
    "    \n",
    "    return X_layers, y_layers\n",
    "\n",
    "# Train and evaluate the probing classifier for each layer\n",
    "def train_and_evaluate_regression(X_train_layers, y_train_layers, X_test_layers, y_test_layers):\n",
    "    r2_scores_train = []\n",
    "    r2_scores_test = []\n",
    "    \n",
    "    for layer in range(len(X_train_layers)):\n",
    "        X_train = X_train_layers[layer]\n",
    "        y_train = y_train_layers[layer]\n",
    "        X_test = X_test_layers[layer]\n",
    "        y_test = y_test_layers[layer]\n",
    "        \n",
    "        model = LinearModel(X_train.shape[1], 1)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(10000):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train.float())\n",
    "            loss = criterion(outputs, y_train.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Layer {layer+1}, Epoch {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions_train = model(X_train.float()).view(-1)\n",
    "            predictions_test = model(X_test.float()).view(-1)\n",
    "            r2_train = r2_score(y_train.float(), predictions_train)\n",
    "            r2_test = r2_score(y_test.float(), predictions_test)\n",
    "        r2_scores_train.append(r2_train)\n",
    "        r2_scores_test.append(r2_test)\n",
    "    \n",
    "    return r2_scores_train, r2_scores_test\n",
    "\n",
    "# Aggregate R scores across all graphs\n",
    "def aggregate_r2_scores(features_train, properties_train, features_test, properties_test, property_index):\n",
    "    X_train_layers, y_train_layers = prepare_regression_data(features_train, properties_train, property_index)\n",
    "    X_test_layers, y_test_layers = prepare_regression_data(features_test, properties_test, property_index)\n",
    "\n",
    "    #save the results in a file\n",
    "    with open(\"results/\"+DATASET+\"_\"+MODEL+\"_X_train_layers.pkl\", \"wb\") as f:\n",
    "        pkl.dump(X_train_layers, f)\n",
    "\n",
    "    with open(\"results/\"+DATASET+\"_\"+MODEL+\"_y_train_layers.pkl\", \"wb\") as f:\n",
    "        pkl.dump(y_train_layers, f)\n",
    "    \n",
    "    r2_scores_train, r2_scores_test = train_and_evaluate_regression(X_train_layers, y_train_layers, X_test_layers, y_test_layers)\n",
    "    \n",
    "    return r2_scores_train, r2_scores_test\n",
    "\n",
    "# Properties indices: 0 - node_degrees, 1 - clustering_coeffs, 2 - betweenness_centralities, 3 - eigenvector_centralities, 4 - Local_clustering_coefficients\n",
    "properties_indices = [0, 1, 2, 3, 4]\n",
    "property_names = ['Node Degrees', 'Clustering Coefficients', 'Betweenness Centralities', 'Eigenvector Centralities', 'Local Clustering Coefficients']\n",
    "\n",
    "# Initialize dictionaries to store average R scores across all layers\n",
    "avg_r2_train_dict = {name: [] for name in property_names}\n",
    "avg_r2_test_dict = {name: [] for name in property_names}\n",
    "\n",
    "# Train and evaluate the probing classifier for each property\n",
    "for prop_idx, prop_name in zip(properties_indices, property_names):\n",
    "    print(f\"Processing property: {prop_name}\")\n",
    "    avg_r2_train_dict[prop_name], avg_r2_test_dict[prop_name] = aggregate_r2_scores(train_features, train_node_properties, test_features, test_node_properties, prop_idx)\n",
    "\n",
    "# Plotting the average R scores across layers for each property\n",
    "layers = np.arange(len(avg_r2_train_dict[property_names[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save all the variables necessary for the plot\n",
    "import pickle as pkl\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_train_dict_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(avg_r2_train_dict, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_test_dict_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(avg_r2_test_dict, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_layers_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(layers, f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_property_names_long.pkl\", \"wb\") as f:\n",
    "    pkl.dump(property_names, f)\n",
    "\n",
    "#load all the variables necessary for the plot\n",
    "import pickle as pkl\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_train_dict_long.pkl\", \"rb\") as f:\n",
    "    avg_r2_train_dict = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_avg_r2_test_dict_long.pkl\", \"rb\") as f:\n",
    "    avg_r2_test_dict = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_layers_long.pkl\", \"rb\") as f:\n",
    "    layers = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_property_names_long.pkl\", \"rb\") as f:\n",
    "    property_names = pkl.load(f)\n",
    "\n",
    "    #load the layer results\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_X_train_layers.pkl\", \"rb\") as f:\n",
    "    X_train_layers = pkl.load(f)\n",
    "\n",
    "with open(\"results/\"+DATASET+\"_\"+MODEL+\"_y_train_layers.pkl\", \"rb\") as f:\n",
    "    y_train_layers = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for average R scores\n",
    "for prop_name in property_names:\n",
    "    plt.plot(layers, avg_r2_train_dict[prop_name], label=f'{prop_name} (Train)', marker='o')\n",
    "    plt.plot(layers, avg_r2_test_dict[prop_name], label=f'{prop_name} (Test)', linestyle='--', marker='x')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average R Score')\n",
    "plt.title('Average R Score for Node Properties Prediction Across Layers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot only test results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for average R scores\n",
    "for prop_name in property_names:\n",
    "    plt.plot(layers, avg_r2_test_dict[prop_name], label=f'{prop_name} (Test)', linestyle='--', marker='x')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average R Score')\n",
    "plt.title('Average R Score for Node Properties Prediction Across Layers')\n",
    "plt.legend()\n",
    "#x axis called layer 1, layer 2, etc\n",
    "plt.xticks(range(len(layers)), [f'Layer {i+1}' for i in layers])           \n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y=1 and y=0 as two different plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels from the dataset using train_idx_list and test_idx_list\n",
    "train_idx_list = gnn.train_idx.tolist()\n",
    "test_idx_list = gnn.test_idx.tolist()\n",
    "\n",
    "train_labels = [gnn.dataset[i].y.item() for i in train_idx_list]\n",
    "test_labels = [gnn.dataset[i].y.item() for i in test_idx_list]\n",
    "\n",
    "# Split the dataset by label y=0 and y=1\n",
    "def split_by_label(features, properties, labels):\n",
    "    features_0, properties_0, features_1, properties_1 = [], [], [], []\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 0:\n",
    "            features_0.append(features[i])\n",
    "            properties_0.append(properties[i])\n",
    "        else:\n",
    "            features_1.append(features[i])\n",
    "            properties_1.append(properties[i])\n",
    "    \n",
    "    return features_0, properties_0, features_1, properties_1\n",
    "\n",
    "# Assuming you have train_features, train_node_properties, test_features, test_node_properties from your GNN\n",
    "train_features_0, train_node_properties_0, train_features_1, train_node_properties_1 = split_by_label(train_features, train_node_properties, train_labels)\n",
    "test_features_0, test_node_properties_0, test_features_1, test_node_properties_1 = split_by_label(test_features, test_node_properties, test_labels)\n",
    "\n",
    "# Properties indices: 0 - node_degrees, 1 - clustering_coeffs, 2 - betweenness_centralities, 3 - eigenvector_centralities, 4 - Local_clustering_coefficients\n",
    "properties_indices = [0, 1, 2, 3, 4]\n",
    "property_names = ['Node Degrees', 'Clustering Coefficients', 'Betweenness Centralities', 'Eigenvector Centralities', 'Local Clustering Coefficients']\n",
    "\n",
    "# Initialize dictionaries to store average R scores across all layers for y=0 and y=1\n",
    "avg_r2_train_dict_0 = {name: [] for name in property_names}\n",
    "avg_r2_test_dict_0 = {name: [] for name in property_names}\n",
    "avg_r2_train_dict_1 = {name: [] for name in property_names}\n",
    "avg_r2_test_dict_1 = {name: [] for name in property_names}\n",
    "\n",
    "# Train and evaluate the probing classifier for each property for y=0\n",
    "for prop_idx, prop_name in zip(properties_indices, property_names):\n",
    "    print(f\"Processing property for y=0: {prop_name}\")\n",
    "    avg_r2_train_dict_0[prop_name], avg_r2_test_dict_0[prop_name] = aggregate_r2_scores(train_features_0, train_node_properties_0, test_features_0, test_node_properties_0, prop_idx)\n",
    "\n",
    "# Train and evaluate the probing classifier for each property for y=1\n",
    "for prop_idx, prop_name in zip(properties_indices, property_names):\n",
    "    print(f\"Processing property for y=1: {prop_name}\")\n",
    "    avg_r2_train_dict_1[prop_name], avg_r2_test_dict_1[prop_name] = aggregate_r2_scores(train_features_1, train_node_properties_1, test_features_1, test_node_properties_1, prop_idx)\n",
    "\n",
    "# Plotting the average R scores across layers for each property, separately for y=0 and y=1\n",
    "layers = np.arange(len(avg_r2_train_dict_0[property_names[0]]))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Plot for y=0\n",
    "plt.subplot(2, 1, 1)\n",
    "for prop_name in property_names:\n",
    "    plt.plot(layers, avg_r2_train_dict_0[prop_name], label=f'{prop_name} (Train)', marker='o')\n",
    "    plt.plot(layers, avg_r2_test_dict_0[prop_name], label=f'{prop_name} (Test)', linestyle='--', marker='x')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average R Score')\n",
    "plt.title('Average R Score for Node Properties Prediction Across Layers (y=0)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for y=1\n",
    "plt.subplot(2, 1, 2)\n",
    "for prop_name in property_names:\n",
    "    plt.plot(layers, avg_r2_train_dict_1[prop_name], label=f'{prop_name} (Train)', marker='o')\n",
    "    plt.plot(layers, avg_r2_test_dict_1[prop_name], label=f'{prop_name} (Test)', linestyle='--', marker='x')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average R Score')\n",
    "plt.title('Average R Score for Node Properties Prediction Across Layers (y=1)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
